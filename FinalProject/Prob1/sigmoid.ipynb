{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH08: Regularized Neural Net\n",
    "\n",
    "TODO: Ghi họ tên và MSSV của bạn (vd, Nguyễn Văn A - 1234567)\n",
    "\n",
    "---\n",
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex08-RegularizedNeuralNet.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Neural Net trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh, và tập kiểm tra gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm lan truyền tiến qua Neural Net\n",
    "\n",
    "Trong bài này, ta sẽ sử dụng nơ-ron sigmoid ở các tẩng ẩn, và tầng softmax là tầng xuất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = sigmoid(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm huấn luyện Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    \n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            #Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = sigmoid(Z);\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "                    \n",
    "                #Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            \n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                #print delta.shape, Ws[-it+1].shape, it\n",
    "                \n",
    "                delta = delta.dot(Ws[-it+1].T)*As[-it]*(1-As[-it])\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "                #grad = grad[:,1:] + 1.0*l2_reg_level*Ws[-it];\n",
    "                #if it == 2 and epoch == 0:\n",
    "                    #print (grad[:,1:]).shape, (Ws[-2]).shape\n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thí nghiệm\n",
    "\n",
    "Để thấy được ảnh hưởng của hệ số `l2_reg_level`, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu.\n",
    "2. Huấn luyện Neural Net trên tập huấn luyện với `layer_sizes = [784, 30, 10]`, `learning_rate = 0.1`, `mnb_size = 10`, `max_patience = 20`, và `l2_reg_level = 0, 0.0001, 0.001`. Để dễ nhìn khi chương trình `print` ra, bạn nên dùng 3 code cell cho 3 lần gọi hàm huấn luyện (ứng với 3 giá trị của `l2_reg_level`).\n",
    "3. Ở cell kế tiếp, bạn sẽ vẽ ra đồ trị có trục hoàng là số lượng epoch và trục tung là độ lỗi. Với mỗi giá trị của `l2_reg_level`, bạn sẽ vẽ ra 2 đường ứng với độ lỗi MBE trên tập huấn luyện và tập validation; như vậy, trên đồ thị sẽ có tất cả 6 đường.\n",
    "4. Cho nhận xét dựa vào đồ thị kết quả.\n",
    "5. Cuối cùng, bạn sẽ tính và in ra độ lỗi trên tập kiểm tra của mô hình có độ lỗi nhỏ nhất trên tập validation (trong số 3 mô hình ứng với 3 giá trị của `l2_reg_level`).\n",
    "\n",
    "(Kết quả chạy của mình: với `l2_reg_level = 0`, độ lỗi trên tập huấn luyện và tập validation lần lượt là 1.076% và 3.480%; với `l2_reg_level = 0.0001`, độ lỗi là 2.114% và 2.910%; với `l2_reg_level = 0.001`, độ lỗi là 6.940% và 6.130%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  7.336 %, val err  6.93 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  5.778 %, val err  5.65 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  4.926 %, val err  5.12 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  4.294 %, val err  4.66 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  3.95 %, val err  4.55 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  3.362 %, val err  4.06 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  3.248 %, val err  4.15 %, patience  19 \n",
      "\n",
      "Epoch  7 , training err  3.016 %, val err  3.88 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  2.862 %, val err  3.9 %, patience  19 \n",
      "\n",
      "Epoch  9 , training err  2.706 %, val err  3.64 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  2.6 %, val err  3.91 %, patience  19 \n",
      "\n",
      "Epoch  11 , training err  2.342 %, val err  3.64 %, patience  18 \n",
      "\n",
      "Epoch  12 , training err  2.224 %, val err  3.74 %, patience  17 \n",
      "\n",
      "Epoch  13 , training err  2.062 %, val err  3.53 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  2.068 %, val err  3.64 %, patience  19 \n",
      "\n",
      "Epoch  15 , training err  1.95 %, val err  3.65 %, patience  18 \n",
      "\n",
      "Epoch  16 , training err  1.816 %, val err  3.71 %, patience  17 \n",
      "\n",
      "Epoch  17 , training err  1.69 %, val err  3.67 %, patience  16 \n",
      "\n",
      "Epoch  18 , training err  1.698 %, val err  3.56 %, patience  15 \n",
      "\n",
      "Epoch  19 , training err  1.608 %, val err  3.65 %, patience  14 \n",
      "\n",
      "Epoch  20 , training err  1.656 %, val err  3.74 %, patience  13 \n",
      "\n",
      "Epoch  21 , training err  1.48 %, val err  3.75 %, patience  12 \n",
      "\n",
      "Epoch  22 , training err  1.502 %, val err  3.64 %, patience  11 \n",
      "\n",
      "Epoch  23 , training err  1.386 %, val err  3.5 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  1.406 %, val err  3.81 %, patience  19 \n",
      "\n",
      "Epoch  25 , training err  1.286 %, val err  3.69 %, patience  18 \n",
      "\n",
      "Epoch  26 , training err  1.15 %, val err  3.63 %, patience  17 \n",
      "\n",
      "Epoch  27 , training err  1.158 %, val err  3.49 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  1.136 %, val err  3.66 %, patience  19 \n",
      "\n",
      "Epoch  29 , training err  1.076 %, val err  3.48 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  1.168 %, val err  3.79 %, patience  19 \n",
      "\n",
      "Epoch  31 , training err  0.912 %, val err  3.59 %, patience  18 \n",
      "\n",
      "Epoch  32 , training err  1.14 %, val err  3.88 %, patience  17 \n",
      "\n",
      "Epoch  33 , training err  1.034 %, val err  3.66 %, patience  16 \n",
      "\n",
      "Epoch  34 , training err  1.006 %, val err  3.68 %, patience  15 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c3bc50f13fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbWs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbestWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5d5993dc8b33>\u001b[0m in \u001b[0;36mtrain_neural_net\u001b[1;34m(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,30,10], 0.1, 10, 20, 0.0);\n",
    "bestWs.append(bWs);\n",
    "train_errs.append(tE);\n",
    "val_errs.append(vE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,30,10], 0.1, 10, 20, 0.0001);\n",
    "bestWs.append(bWs);\n",
    "train_errs.append(tE);\n",
    "val_errs.append(vE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,30,10], 0.1, 10, 20, 0.001);\n",
    "bestWs.append(bWs);\n",
    "train_errs.append(tE);\n",
    "val_errs.append(vE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "legend = [];\n",
    "for i in range(len(l2_reg)):\n",
    "    plt.plot(range(len(train_errs[i])),train_errs[i]);\n",
    "    plt.plot(range(len(val_errs[i])),val_errs[i]);\n",
    "    legend.append('MBE - training set   - ' + str(l2_reg[i]));\n",
    "    legend.append('MBE - validation set - ' + str(l2_reg[i]));\n",
    "plt.legend(legend, loc='upper left', bbox_to_anchor=(1, 1));\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ta nhận thấy độ lỗi trên tập train và validation ứng với l2_reg_level = 0.001 đều cao, và cao hơn so với các giá trị l2 khác. Điều này cho thấy mô hình này bị underfiting do giá trị l2 khá cao dẫn đến việc các giá trị Ws tương ứng không được \"dao động thoải mái\" và có xu hướng gần giá trị 0 để tối ưu hàm chi phí J do đó khó đạt được tới mô hình tối ưu.\n",
    "- Ngược lại với giá trị l2 = 0, thì mô hình này có độ lỗi trên tập train xuống rất thấp nhưng đồng thời độ lỗi trên tập validation ban đầu có xu hướng xuống nhưng sau lại tăng lên. Điều này cho thấy mô hình này bị overfiting do giá trị l2 quá thấp dẫn tới việc các giá trị Ws 'thoải mái di chuyển' và cố gắng fit tập train (gồm cả nhiễu) dẫn tới việc mô hình không được tổng quát.\n",
    "- Còn với giá trị l2 = 0.001 thì ta thấy độ lỗi trên tập train và tập validation đều có xu hướng đi xuống và có giá trị thấp. Mô hình này tương đối tốt nếu so với 2 mô hình còn lại do giá trị l2 được chọn khá cân bằng (không quá cao hay thấp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosenOne = None;\n",
    "bestValErr = 111;\n",
    "for Ws in bestWs:\n",
    "    tmp = meanBinaryError(X_val, Ws, Y_val)\n",
    "    if tmp < bestValErr:\n",
    "        chosenOne = Ws;\n",
    "        bestValErr = tmp;\n",
    "print 'Val error ', bestValErr*100, '%. Test error ', meanBinaryError(X_test, chosenOne, Y_test)*100, '%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
