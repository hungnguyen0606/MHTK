{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = sigmoid(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    \n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            #Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = sigmoid(Z);\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "                    \n",
    "                #Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            \n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                #print delta.shape, Ws[-it+1].shape, it\n",
    "                \n",
    "                delta = delta.dot(Ws[-it+1].T)*As[-it]*(1-As[-it])\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "                #grad = grad[:,1:] + 1.0*l2_reg_level*Ws[-it];\n",
    "                #if it == 2 and epoch == 0:\n",
    "                    #print (grad[:,1:]).shape, (Ws[-2]).shape\n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  9.992 %, val err  9.19 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  6.772 %, val err  6.06 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  5.06 %, val err  4.87 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  3.922 %, val err  3.83 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  3.748 %, val err  3.88 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  5.258 %, val err  5.34 %, patience  18 \n",
      "\n",
      "Epoch  6 , training err  2.784 %, val err  3.13 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  2.668 %, val err  3.09 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  2.566 %, val err  3.02 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  2.73 %, val err  3.17 %, patience  19 \n",
      "\n",
      "Epoch  10 , training err  2.038 %, val err  2.69 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  2.02 %, val err  2.61 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  2.61 %, val err  3.27 %, patience  19 \n",
      "\n",
      "Epoch  13 , training err  2.446 %, val err  3.12 %, patience  18 \n",
      "\n",
      "Epoch  14 , training err  2.038 %, val err  2.95 %, patience  17 \n",
      "\n",
      "Epoch  15 , training err  2.006 %, val err  2.57 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  1.76 %, val err  2.53 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  2.082 %, val err  2.61 %, patience  19 \n",
      "\n",
      "Epoch  18 , training err  1.788 %, val err  2.58 %, patience  18 \n",
      "\n",
      "Epoch  19 , training err  1.704 %, val err  2.47 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  1.536 %, val err  2.51 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  1.558 %, val err  2.52 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  1.826 %, val err  2.7 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  1.686 %, val err  2.54 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  1.762 %, val err  2.63 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  1.66 %, val err  2.66 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  1.492 %, val err  2.41 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  1.652 %, val err  2.53 %, patience  19 \n",
      "\n",
      "Epoch  28 , training err  1.422 %, val err  2.35 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  1.73 %, val err  2.62 %, patience  19 \n",
      "\n",
      "Epoch  30 , training err  2.53 %, val err  3.17 %, patience  18 \n",
      "\n",
      "Epoch  31 , training err  1.446 %, val err  2.41 %, patience  17 \n",
      "\n",
      "Epoch  32 , training err  1.468 %, val err  2.39 %, patience  16 \n",
      "\n",
      "Epoch  33 , training err  1.622 %, val err  2.54 %, patience  15 \n",
      "\n",
      "Epoch  34 , training err  1.698 %, val err  2.66 %, patience  14 \n",
      "\n",
      "Epoch  35 , training err  1.662 %, val err  2.58 %, patience  13 \n",
      "\n",
      "Epoch  36 , training err  1.632 %, val err  2.51 %, patience  12 \n",
      "\n",
      "Epoch  37 , training err  1.638 %, val err  2.37 %, patience  11 \n",
      "\n",
      "Epoch  38 , training err  1.566 %, val err  2.59 %, patience  10 \n",
      "\n",
      "Epoch  39 , training err  1.428 %, val err  2.36 %, patience  9 \n",
      "\n",
      "Epoch  40 , training err  1.828 %, val err  2.7 %, patience  8 \n",
      "\n",
      "Epoch  41 , training err  1.764 %, val err  2.7 %, patience  7 \n",
      "\n",
      "Epoch  42 , training err  1.536 %, val err  2.42 %, patience  6 \n",
      "\n",
      "Epoch  43 , training err  1.398 %, val err  2.41 %, patience  5 \n",
      "\n",
      "Epoch  44 , training err  1.438 %, val err  2.43 %, patience  4 \n",
      "\n",
      "Epoch  45 , training err  1.59 %, val err  2.46 %, patience  3 \n",
      "\n",
      "Epoch  46 , training err  1.402 %, val err  2.51 %, patience  2 \n",
      "\n",
      "Epoch  47 , training err  1.43 %, val err  2.28 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  1.272 %, val err  2.37 %, patience  19 \n",
      "\n",
      "Epoch  49 , training err  1.344 %, val err  2.44 %, patience  18 \n",
      "\n",
      "Epoch  50 , training err  1.414 %, val err  2.35 %, patience  17 \n",
      "\n",
      "Epoch  51 , training err  1.304 %, val err  2.27 %, patience  20 \n",
      "\n",
      "Epoch  52 , training err  1.396 %, val err  2.43 %, patience  19 \n",
      "\n",
      "Epoch  53 , training err  1.686 %, val err  2.56 %, patience  18 \n",
      "\n",
      "Epoch  54 , training err  1.658 %, val err  2.52 %, patience  17 \n",
      "\n",
      "Epoch  55 , training err  1.316 %, val err  2.26 %, patience  20 \n",
      "\n",
      "Epoch  56 , training err  1.34 %, val err  2.41 %, patience  19 \n",
      "\n",
      "Epoch  57 , training err  1.368 %, val err  2.41 %, patience  18 \n",
      "\n",
      "Epoch  58 , training err  1.684 %, val err  2.62 %, patience  17 \n",
      "\n",
      "Epoch  59 , training err  1.29 %, val err  2.35 %, patience  16 \n",
      "\n",
      "Epoch  60 , training err  1.562 %, val err  2.44 %, patience  15 \n",
      "\n",
      "Epoch  61 , training err  1.582 %, val err  2.63 %, patience  14 \n",
      "\n",
      "Epoch  62 , training err  1.39 %, val err  2.41 %, patience  13 \n",
      "\n",
      "Epoch  63 , training err  1.324 %, val err  2.34 %, patience  12 \n",
      "\n",
      "Epoch  64 , training err  1.6 %, val err  2.57 %, patience  11 \n",
      "\n",
      "Epoch  65 , training err  1.634 %, val err  2.4 %, patience  10 \n",
      "\n",
      "Epoch  66 , training err  1.55 %, val err  2.39 %, patience  9 \n",
      "\n",
      "Epoch  67 , training err  1.23 %, val err  2.24 %, patience  20 \n",
      "\n",
      "Epoch  68 , training err  1.646 %, val err  2.6 %, patience  19 \n",
      "\n",
      "Epoch  69 , training err  1.366 %, val err  2.29 %, patience  18 \n",
      "\n",
      "Epoch  70 , training err  1.698 %, val err  2.52 %, patience  17 \n",
      "\n",
      "Epoch  71 , training err  1.552 %, val err  2.47 %, patience  16 \n",
      "\n",
      "Epoch  72 , training err  1.252 %, val err  2.16 %, patience  20 \n",
      "\n",
      "Epoch  73 , training err  1.364 %, val err  2.32 %, patience  19 \n",
      "\n",
      "Epoch  74 , training err  1.508 %, val err  2.4 %, patience  18 \n",
      "\n",
      "Epoch  75 , training err  1.378 %, val err  2.31 %, patience  17 \n",
      "\n",
      "Epoch  76 , training err  1.186 %, val err  2.23 %, patience  16 \n",
      "\n",
      "Epoch  77 , training err  1.43 %, val err  2.25 %, patience  15 \n",
      "\n",
      "Epoch  78 , training err  1.37 %, val err  2.3 %, patience  14 \n",
      "\n",
      "Epoch  79 , training err  1.316 %, val err  2.42 %, patience  13 \n",
      "\n",
      "Epoch  80 , training err  2.198 %, val err  3.16 %, patience  12 \n",
      "\n",
      "Epoch  81 , training err  1.39 %, val err  2.33 %, patience  11 \n",
      "\n",
      "Epoch  82 , training err  1.208 %, val err  2.15 %, patience  20 \n",
      "\n",
      "Epoch  83 , training err  1.382 %, val err  2.24 %, patience  19 \n",
      "\n",
      "Epoch  84 , training err  1.442 %, val err  2.42 %, patience  18 \n",
      "\n",
      "Epoch  85 , training err  1.34 %, val err  2.33 %, patience  17 \n",
      "\n",
      "Epoch  86 , training err  1.29 %, val err  2.33 %, patience  16 \n",
      "\n",
      "Epoch  87 , training err  1.472 %, val err  2.43 %, patience  15 \n",
      "\n",
      "Epoch  88 , training err  1.206 %, val err  2.29 %, patience  14 \n",
      "\n",
      "Epoch  89 , training err  1.526 %, val err  2.41 %, patience  13 \n",
      "\n",
      "Epoch  90 , training err  1.404 %, val err  2.3 %, patience  12 \n",
      "\n",
      "Epoch  91 , training err  1.394 %, val err  2.27 %, patience  11 \n",
      "\n",
      "Epoch  92 , training err  1.814 %, val err  2.93 %, patience  10 \n",
      "\n",
      "Epoch  93 , training err  1.422 %, val err  2.46 %, patience  9 \n",
      "\n",
      "Epoch  94 , training err  1.806 %, val err  2.62 %, patience  8 \n",
      "\n",
      "Epoch  95 , training err  1.456 %, val err  2.41 %, patience  7 \n",
      "\n",
      "Epoch  96 , training err  1.74 %, val err  2.67 %, patience  6 \n",
      "\n",
      "Epoch  97 , training err  2.144 %, val err  2.92 %, patience  5 \n",
      "\n",
      "Epoch  98 , training err  1.73 %, val err  2.6 %, patience  4 \n",
      "\n",
      "Epoch  99 , training err  1.434 %, val err  2.66 %, patience  3 \n",
      "\n",
      "Epoch  100 , training err  1.292 %, val err  2.23 %, patience  2 \n",
      "\n",
      "Epoch  101 , training err  1.284 %, val err  2.27 %, patience  1 \n",
      "\n",
      "Epoch  102 , training err  1.408 %, val err  2.25 %, patience  0 \n",
      "\n",
      "Best val err  2.15 % at epoch  82  corresponding train err  1.208 %\n",
      "Epoch  0 , training err  8.524 %, val err  8.02 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  6.382 %, val err  5.9 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  4.852 %, val err  4.58 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  4.266 %, val err  3.97 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  4.536 %, val err  4.54 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  3.284 %, val err  3.46 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  4.018 %, val err  3.97 %, patience  19 \n",
      "\n",
      "Epoch  7 , training err  2.768 %, val err  3.18 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  2.854 %, val err  3.22 %, patience  19 \n",
      "\n",
      "Epoch  9 , training err  2.24 %, val err  2.73 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  2.466 %, val err  2.93 %, patience  19 \n",
      "\n",
      "Epoch  11 , training err  2.402 %, val err  2.97 %, patience  18 \n",
      "\n",
      "Epoch  12 , training err  2.586 %, val err  3.09 %, patience  17 \n",
      "\n",
      "Epoch  13 , training err  2.074 %, val err  2.67 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  2.122 %, val err  2.72 %, patience  19 \n",
      "\n",
      "Epoch  15 , training err  2.186 %, val err  2.88 %, patience  18 \n",
      "\n",
      "Epoch  16 , training err  2.508 %, val err  2.95 %, patience  17 \n",
      "\n",
      "Epoch  17 , training err  2.162 %, val err  2.79 %, patience  16 \n",
      "\n",
      "Epoch  18 , training err  2.0 %, val err  2.78 %, patience  15 \n",
      "\n",
      "Epoch  19 , training err  1.832 %, val err  2.53 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  2.36 %, val err  3.16 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  1.802 %, val err  2.54 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  1.728 %, val err  2.55 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  1.846 %, val err  2.65 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  2.078 %, val err  3.09 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  1.922 %, val err  2.75 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  1.586 %, val err  2.33 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  2.462 %, val err  3.13 %, patience  19 \n",
      "\n",
      "Epoch  28 , training err  1.982 %, val err  2.95 %, patience  18 \n",
      "\n",
      "Epoch  29 , training err  1.64 %, val err  2.33 %, patience  17 \n",
      "\n",
      "Epoch  30 , training err  1.74 %, val err  2.51 %, patience  16 \n",
      "\n",
      "Epoch  31 , training err  1.628 %, val err  2.46 %, patience  15 \n",
      "\n",
      "Epoch  32 , training err  1.99 %, val err  2.79 %, patience  14 \n",
      "\n",
      "Epoch  33 , training err  1.406 %, val err  2.35 %, patience  13 \n",
      "\n",
      "Epoch  34 , training err  1.434 %, val err  2.28 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  2.078 %, val err  2.84 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  1.63 %, val err  2.39 %, patience  18 \n",
      "\n",
      "Epoch  37 , training err  1.742 %, val err  2.5 %, patience  17 \n",
      "\n",
      "Epoch  38 , training err  1.498 %, val err  2.36 %, patience  16 \n",
      "\n",
      "Epoch  39 , training err  1.422 %, val err  2.25 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  3.03 %, val err  3.63 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  1.95 %, val err  2.84 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  1.768 %, val err  2.67 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  1.776 %, val err  2.65 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  1.512 %, val err  2.6 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  1.348 %, val err  2.14 %, patience  20 \n",
      "\n",
      "Epoch  46 , training err  1.418 %, val err  2.24 %, patience  19 \n",
      "\n",
      "Epoch  47 , training err  1.938 %, val err  2.73 %, patience  18 \n",
      "\n",
      "Epoch  48 , training err  1.578 %, val err  2.54 %, patience  17 \n",
      "\n",
      "Epoch  49 , training err  1.43 %, val err  2.6 %, patience  16 \n",
      "\n",
      "Epoch  50 , training err  1.258 %, val err  2.29 %, patience  15 \n",
      "\n",
      "Epoch  51 , training err  1.612 %, val err  2.6 %, patience  14 \n",
      "\n",
      "Epoch  52 , training err  1.498 %, val err  2.45 %, patience  13 \n",
      "\n",
      "Epoch  53 , training err  1.83 %, val err  2.59 %, patience  12 \n",
      "\n",
      "Epoch  54 , training err  1.286 %, val err  2.04 %, patience  20 \n",
      "\n",
      "Epoch  55 , training err  1.372 %, val err  2.29 %, patience  19 \n",
      "\n",
      "Epoch  56 , training err  1.428 %, val err  2.22 %, patience  18 \n",
      "\n",
      "Epoch  57 , training err  2.008 %, val err  2.8 %, patience  17 \n",
      "\n",
      "Epoch  58 , training err  1.348 %, val err  2.18 %, patience  16 \n",
      "\n",
      "Epoch  59 , training err  1.448 %, val err  2.42 %, patience  15 \n",
      "\n",
      "Epoch  60 , training err  2.098 %, val err  2.94 %, patience  14 \n",
      "\n",
      "Epoch  61 , training err  1.26 %, val err  2.19 %, patience  13 \n",
      "\n",
      "Epoch  62 , training err  1.276 %, val err  2.19 %, patience  12 \n",
      "\n",
      "Epoch  63 , training err  1.356 %, val err  2.3 %, patience  11 \n",
      "\n",
      "Epoch  64 , training err  1.35 %, val err  2.34 %, patience  10 \n",
      "\n",
      "Epoch  65 , training err  1.364 %, val err  2.34 %, patience  9 \n",
      "\n",
      "Epoch  66 , training err  1.51 %, val err  2.4 %, patience  8 \n",
      "\n",
      "Epoch  67 , training err  1.41 %, val err  2.21 %, patience  7 \n",
      "\n",
      "Epoch  68 , training err  1.678 %, val err  2.52 %, patience  6 \n",
      "\n",
      "Epoch  69 , training err  1.522 %, val err  2.43 %, patience  5 \n",
      "\n",
      "Epoch  70 , training err  1.434 %, val err  2.57 %, patience  4 \n",
      "\n",
      "Epoch  71 , training err  1.682 %, val err  2.49 %, patience  3 \n",
      "\n",
      "Epoch  72 , training err  1.442 %, val err  2.26 %, patience  2 \n",
      "\n",
      "Epoch  73 , training err  1.322 %, val err  2.35 %, patience  1 \n",
      "\n",
      "Epoch  74 , training err  1.792 %, val err  2.73 %, patience  0 \n",
      "\n",
      "Best val err  2.04 % at epoch  54  corresponding train err  1.286 %\n",
      "Epoch  0 , training err  12.308 %, val err  11.44 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  7.258 %, val err  7.0 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  5.978 %, val err  5.6 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  6.086 %, val err  5.73 %, patience  19 \n",
      "\n",
      "Epoch  4 , training err  3.674 %, val err  3.73 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  3.186 %, val err  3.32 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  3.064 %, val err  3.28 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  3.212 %, val err  3.51 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  2.678 %, val err  2.94 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  3.352 %, val err  3.78 %, patience  19 \n",
      "\n",
      "Epoch  10 , training err  2.482 %, val err  3.08 %, patience  18 \n",
      "\n",
      "Epoch  11 , training err  2.4 %, val err  3.02 %, patience  17 \n",
      "\n",
      "Epoch  12 , training err  2.284 %, val err  2.8 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  2.622 %, val err  3.41 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  2.35 %, val err  2.87 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  2.106 %, val err  3.01 %, patience  17 \n",
      "\n",
      "Epoch  16 , training err  2.17 %, val err  2.89 %, patience  16 \n",
      "\n",
      "Epoch  17 , training err  1.928 %, val err  2.66 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  2.882 %, val err  3.62 %, patience  19 \n",
      "\n",
      "Epoch  19 , training err  1.792 %, val err  2.49 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  2.302 %, val err  3.26 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  3.266 %, val err  3.89 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  1.844 %, val err  2.46 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  1.762 %, val err  2.58 %, patience  19 \n",
      "\n",
      "Epoch  24 , training err  2.086 %, val err  2.79 %, patience  18 \n",
      "\n",
      "Epoch  25 , training err  1.686 %, val err  2.55 %, patience  17 \n",
      "\n",
      "Epoch  26 , training err  2.632 %, val err  3.19 %, patience  16 \n",
      "\n",
      "Epoch  27 , training err  1.552 %, val err  2.58 %, patience  15 \n",
      "\n",
      "Epoch  28 , training err  1.7 %, val err  2.54 %, patience  14 \n",
      "\n",
      "Epoch  29 , training err  1.728 %, val err  2.63 %, patience  13 \n",
      "\n",
      "Epoch  30 , training err  1.862 %, val err  2.62 %, patience  12 \n",
      "\n",
      "Epoch  31 , training err  1.602 %, val err  2.45 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  2.214 %, val err  3.1 %, patience  19 \n",
      "\n",
      "Epoch  33 , training err  1.908 %, val err  2.65 %, patience  18 \n",
      "\n",
      "Epoch  34 , training err  1.674 %, val err  2.62 %, patience  17 \n",
      "\n",
      "Epoch  35 , training err  1.862 %, val err  2.56 %, patience  16 \n",
      "\n",
      "Epoch  36 , training err  1.756 %, val err  2.71 %, patience  15 \n",
      "\n",
      "Epoch  37 , training err  1.996 %, val err  2.65 %, patience  14 \n",
      "\n",
      "Epoch  38 , training err  2.164 %, val err  2.91 %, patience  13 \n",
      "\n",
      "Epoch  39 , training err  1.602 %, val err  2.27 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  1.512 %, val err  2.48 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  2.654 %, val err  3.31 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  1.442 %, val err  2.48 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  1.85 %, val err  2.7 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  1.868 %, val err  2.81 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  1.574 %, val err  2.5 %, patience  14 \n",
      "\n",
      "Epoch  46 , training err  1.676 %, val err  2.6 %, patience  13 \n",
      "\n",
      "Epoch  47 , training err  1.752 %, val err  2.72 %, patience  12 \n",
      "\n",
      "Epoch  48 , training err  2.21 %, val err  2.82 %, patience  11 \n",
      "\n",
      "Epoch  49 , training err  1.978 %, val err  2.8 %, patience  10 \n",
      "\n",
      "Epoch  50 , training err  1.438 %, val err  2.32 %, patience  9 \n",
      "\n",
      "Epoch  51 , training err  1.958 %, val err  2.73 %, patience  8 \n",
      "\n",
      "Epoch  52 , training err  1.538 %, val err  2.38 %, patience  7 \n",
      "\n",
      "Epoch  53 , training err  1.496 %, val err  2.41 %, patience  6 \n",
      "\n",
      "Epoch  54 , training err  1.628 %, val err  2.46 %, patience  5 \n",
      "\n",
      "Epoch  55 , training err  2.07 %, val err  2.77 %, patience  4 \n",
      "\n",
      "Epoch  56 , training err  3.21 %, val err  4.02 %, patience  3 \n",
      "\n",
      "Epoch  57 , training err  1.518 %, val err  2.54 %, patience  2 \n",
      "\n",
      "Epoch  58 , training err  1.518 %, val err  2.54 %, patience  1 \n",
      "\n",
      "Epoch  59 , training err  2.438 %, val err  3.13 %, patience  0 \n",
      "\n",
      "Best val err  2.27 % at epoch  39  corresponding train err  1.602 %\n",
      "Hidden layer  150 . Test error  2.22 %\n",
      "Hidden layer  200 . Test error  2.34 %\n",
      "Hidden layer  300 . Test error  2.63 %\n"
     ]
    }
   ],
   "source": [
    "middle = [150, 200, 300]\n",
    "for mid in middle:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,mid,10], 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, mid));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, mid in bestWs:\n",
    "    print  'Hidden layer ', mid,  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  15.17 %, val err  14.14 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  7.856 %, val err  7.53 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  5.972 %, val err  5.66 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  5.752 %, val err  5.87 %, patience  19 \n",
      "\n",
      "Epoch  4 , training err  3.686 %, val err  3.92 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  3.162 %, val err  3.85 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  2.738 %, val err  3.35 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  3.072 %, val err  3.77 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  2.42 %, val err  3.29 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  2.256 %, val err  3.32 %, patience  19 \n",
      "\n",
      "Epoch  10 , training err  2.222 %, val err  3.33 %, patience  18 \n",
      "\n",
      "Epoch  11 , training err  2.002 %, val err  3.08 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  2.57 %, val err  3.57 %, patience  19 \n",
      "\n",
      "Epoch  13 , training err  2.226 %, val err  3.3 %, patience  18 \n",
      "\n",
      "Epoch  14 , training err  2.138 %, val err  3.36 %, patience  17 \n",
      "\n",
      "Epoch  15 , training err  2.388 %, val err  3.46 %, patience  16 \n",
      "\n",
      "Epoch  16 , training err  1.916 %, val err  3.36 %, patience  15 \n",
      "\n",
      "Epoch  17 , training err  1.442 %, val err  2.74 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  1.568 %, val err  2.92 %, patience  19 \n",
      "\n",
      "Epoch  19 , training err  1.636 %, val err  2.92 %, patience  18 \n",
      "\n",
      "Epoch  20 , training err  2.156 %, val err  3.56 %, patience  17 \n",
      "\n",
      "Epoch  21 , training err  1.544 %, val err  2.88 %, patience  16 \n",
      "\n",
      "Epoch  22 , training err  1.574 %, val err  2.88 %, patience  15 \n",
      "\n",
      "Epoch  23 , training err  1.746 %, val err  3.02 %, patience  14 \n",
      "\n",
      "Epoch  24 , training err  1.416 %, val err  2.8 %, patience  13 \n",
      "\n",
      "Epoch  25 , training err  1.68 %, val err  2.96 %, patience  12 \n",
      "\n",
      "Epoch  26 , training err  1.388 %, val err  2.93 %, patience  11 \n",
      "\n",
      "Epoch  27 , training err  1.426 %, val err  2.96 %, patience  10 \n",
      "\n",
      "Epoch  28 , training err  1.78 %, val err  3.0 %, patience  9 \n",
      "\n",
      "Epoch  29 , training err  1.324 %, val err  2.66 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  1.7 %, val err  3.15 %, patience  19 \n",
      "\n",
      "Epoch  31 , training err  1.44 %, val err  2.87 %, patience  18 \n",
      "\n",
      "Epoch  32 , training err  1.244 %, val err  2.63 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  2.594 %, val err  3.74 %, patience  19 \n",
      "\n",
      "Epoch  34 , training err  1.266 %, val err  2.77 %, patience  18 \n",
      "\n",
      "Epoch  35 , training err  1.358 %, val err  2.88 %, patience  17 \n",
      "\n",
      "Epoch  36 , training err  1.81 %, val err  3.24 %, patience  16 \n",
      "\n",
      "Epoch  37 , training err  1.25 %, val err  2.62 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  1.456 %, val err  3.08 %, patience  19 \n",
      "\n",
      "Epoch  39 , training err  1.394 %, val err  2.98 %, patience  18 \n",
      "\n",
      "Epoch  40 , training err  1.268 %, val err  2.72 %, patience  17 \n",
      "\n",
      "Epoch  41 , training err  1.784 %, val err  3.26 %, patience  16 \n",
      "\n",
      "Epoch  42 , training err  1.22 %, val err  2.75 %, patience  15 \n",
      "\n",
      "Epoch  43 , training err  1.248 %, val err  3.0 %, patience  14 \n",
      "\n",
      "Epoch  44 , training err  1.372 %, val err  2.78 %, patience  13 \n",
      "\n",
      "Epoch  45 , training err  1.548 %, val err  3.05 %, patience  12 \n",
      "\n",
      "Epoch  46 , training err  2.004 %, val err  3.42 %, patience  11 \n",
      "\n",
      "Epoch  47 , training err  1.248 %, val err  2.67 %, patience  10 \n",
      "\n",
      "Epoch  48 , training err  2.714 %, val err  4.01 %, patience  9 \n",
      "\n",
      "Epoch  49 , training err  1.276 %, val err  2.8 %, patience  8 \n",
      "\n",
      "Epoch  50 , training err  1.744 %, val err  3.2 %, patience  7 \n",
      "\n",
      "Epoch  51 , training err  1.128 %, val err  2.77 %, patience  6 \n",
      "\n",
      "Epoch  52 , training err  1.356 %, val err  2.73 %, patience  5 \n",
      "\n",
      "Epoch  53 , training err  1.292 %, val err  2.75 %, patience  4 \n",
      "\n",
      "Epoch  54 , training err  1.468 %, val err  2.87 %, patience  3 \n",
      "\n",
      "Epoch  55 , training err  1.124 %, val err  2.65 %, patience  2 \n",
      "\n",
      "Epoch  56 , training err  1.308 %, val err  2.68 %, patience  1 \n",
      "\n",
      "Epoch  57 , training err  1.692 %, val err  3.1 %, patience  0 \n",
      "\n",
      "Best val err  2.62 % at epoch  37  corresponding train err  1.25 %\n",
      "Epoch  0 , training err  89.65 %, val err  89.1 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  90.316 %, val err  89.91 %, patience  19 \n",
      "\n",
      "Epoch  2 , training err  88.644 %, val err  89.36 %, patience  18 \n",
      "\n",
      "Epoch  3 , training err  88.644 %, val err  89.36 %, patience  17 \n",
      "\n",
      "Epoch  4 , training err  90.024 %, val err  90.39 %, patience  16 \n",
      "\n",
      "Epoch  5 , training err  90.282 %, val err  90.17 %, patience  15 \n",
      "\n",
      "Epoch  6 , training err  88.644 %, val err  89.36 %, patience  14 \n",
      "\n",
      "Epoch  7 , training err  90.064 %, val err  90.1 %, patience  13 \n",
      "\n",
      "Epoch  8 , training err  90.136 %, val err  90.09 %, patience  12 \n",
      "\n",
      "Epoch  9 , training err  89.798 %, val err  89.7 %, patience  11 \n",
      "\n",
      "Epoch  10 , training err  89.65 %, val err  89.1 %, patience  10 \n",
      "\n",
      "Epoch  11 , training err  90.064 %, val err  90.1 %, patience  9 \n",
      "\n",
      "Epoch  12 , training err  90.282 %, val err  90.17 %, patience  8 \n",
      "\n",
      "Epoch  13 , training err  89.65 %, val err  89.1 %, patience  7 \n",
      "\n",
      "Epoch  14 , training err  90.316 %, val err  89.91 %, patience  6 \n",
      "\n",
      "Epoch  15 , training err  89.65 %, val err  89.1 %, patience  5 \n",
      "\n",
      "Epoch  16 , training err  90.136 %, val err  90.09 %, patience  4 \n",
      "\n",
      "Epoch  17 , training err  90.988 %, val err  90.85 %, patience  3 \n",
      "\n",
      "Epoch  18 , training err  90.098 %, val err  90.33 %, patience  2 \n",
      "\n",
      "Epoch  19 , training err  90.064 %, val err  90.1 %, patience  1 \n",
      "\n",
      "Epoch  20 , training err  90.282 %, val err  90.17 %, patience  0 \n",
      "\n",
      "Best val err  89.1 % at epoch  0  corresponding train err  89.65 %\n",
      "150\n",
      ". Test error  2.22 %\n",
      "200\n",
      ". Test error  2.34 %\n",
      "300\n",
      ". Test error  2.63 %\n",
      "[784, 50, 30, 30, 10]\n",
      ". Test error  2.96 %\n",
      "[784, 50, 30, 30, 30, 10]\n",
      ". Test error  89.72 %\n"
     ]
    }
   ],
   "source": [
    "layer = [[784, 50, 30, 20, 10], [784, 50, 30, 20, 20, 10]]\n",
    "for ll in layer:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, ll, 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, ll));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, ll in bestWs:\n",
    "    print ll \n",
    "    print  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
