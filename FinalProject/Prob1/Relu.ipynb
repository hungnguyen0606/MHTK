{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH08: Regularized Neural Net\n",
    "\n",
    "TODO: Ghi họ tên và MSSV của bạn (vd, Nguyễn Văn A - 1234567)\n",
    "\n",
    "---\n",
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex08-RegularizedNeuralNet.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Neural Net trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh, và tập kiểm tra gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm lan truyền tiến qua Neural Net\n",
    "\n",
    "Trong bài này, ta sẽ sử dụng nơ-ron sigmoid ở các tẩng ẩn, và tầng softmax là tầng xuất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Computes relu function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    ret = np.array(Z)\n",
    "    ret[ret<0] = 0\n",
    "    return ret\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "   \n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = relu(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm huấn luyện Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    #Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1])/np.sqrt(X_train.shape[0]) for l in range(len(layer_sizes)-1)]\n",
    "    #print Ws\n",
    "    #return\n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = relu(Z)\n",
    "\n",
    "                    A = np.hstack((np.ones((A.shape[0],1))/10,A));\n",
    "                    \n",
    "                Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            #print np.sum(As[-1])\n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                #print delta.shape, Ws[-it+1].shape, it\n",
    "                temp = np.array(Zs[-it])\n",
    "                temp[temp > 0] = 1 \n",
    "                temp = np.hstack((np.zeros((temp.shape[0],1)),temp));\n",
    "                #print temp.shape, As[-it].shape\n",
    "                #print (delta.dot(Ws[-it+1].T)).shape, temp.shape\n",
    "                delta = delta.dot(Ws[-it+1].T)*temp\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "                #grad = grad[:,1:] + 1.0*l2_reg_level*Ws[-it];\n",
    "                #if it == 2 and epoch == 0:\n",
    "                    #print (grad[:,1:]).shape, (Ws[-2]).shape\n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thí nghiệm\n",
    "\n",
    "Để thấy được ảnh hưởng của hệ số `l2_reg_level`, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu.\n",
    "2. Huấn luyện Neural Net trên tập huấn luyện với `layer_sizes = [784, 30, 10]`, `learning_rate = 0.1`, `mnb_size = 10`, `max_patience = 20`, và `l2_reg_level = 0, 0.0001, 0.001`. Để dễ nhìn khi chương trình `print` ra, bạn nên dùng 3 code cell cho 3 lần gọi hàm huấn luyện (ứng với 3 giá trị của `l2_reg_level`).\n",
    "3. Ở cell kế tiếp, bạn sẽ vẽ ra đồ trị có trục hoàng là số lượng epoch và trục tung là độ lỗi. Với mỗi giá trị của `l2_reg_level`, bạn sẽ vẽ ra 2 đường ứng với độ lỗi MBE trên tập huấn luyện và tập validation; như vậy, trên đồ thị sẽ có tất cả 6 đường.\n",
    "4. Cho nhận xét dựa vào đồ thị kết quả.\n",
    "5. Cuối cùng, bạn sẽ tính và in ra độ lỗi trên tập kiểm tra của mô hình có độ lỗi nhỏ nhất trên tập validation (trong số 3 mô hình ứng với 3 giá trị của `l2_reg_level`).\n",
    "\n",
    "(Kết quả chạy của mình: với `l2_reg_level = 0`, độ lỗi trên tập huấn luyện và tập validation lần lượt là 1.076% và 3.480%; với `l2_reg_level = 0.0001`, độ lỗi là 2.114% và 2.910%; với `l2_reg_level = 0.001`, độ lỗi là 6.940% và 6.130%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  88.644 %, val err  89.36 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  88.644 %, val err  89.36 %, patience  19 \n",
      "\n",
      "Epoch  2 , training err  88.644 %, val err  89.36 %, patience  18 \n",
      "\n",
      "Epoch  3 , training err  88.644 %, val err  89.36 %, patience  17 \n",
      "\n",
      "Epoch  4 , training err  87.102 %, val err  87.92 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  89.606 %, val err  89.06 %, patience  19 \n",
      "\n",
      "Epoch  6 , training err  82.404 %, val err  81.66 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  82.39 %, val err  81.58 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  81.87 %, val err  81.25 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  81.45 %, val err  80.81 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  81.008 %, val err  80.24 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  81.034 %, val err  80.42 %, patience  19 \n",
      "\n",
      "Epoch  12 , training err  80.148 %, val err  79.66 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  79.774 %, val err  79.32 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  79.738 %, val err  79.01 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  78.798 %, val err  78.19 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  78.138 %, val err  77.49 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  77.676 %, val err  76.97 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  77.762 %, val err  77.19 %, patience  19 \n",
      "\n",
      "Epoch  19 , training err  76.776 %, val err  75.89 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  76.246 %, val err  75.47 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  75.394 %, val err  74.84 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  73.632 %, val err  73.06 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  72.512 %, val err  71.84 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  71.852 %, val err  71.64 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  68.594 %, val err  68.49 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  68.69 %, val err  68.2 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  67.498 %, val err  68.01 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  67.3 %, val err  67.73 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  67.47 %, val err  67.7 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  68.218 %, val err  68.65 %, patience  19 \n",
      "\n",
      "Epoch  31 , training err  68.598 %, val err  69.01 %, patience  18 \n",
      "\n",
      "Epoch  32 , training err  69.526 %, val err  69.93 %, patience  17 \n",
      "\n",
      "Epoch  33 , training err  70.23 %, val err  70.45 %, patience  16 \n",
      "\n",
      "Epoch  34 , training err  70.8 %, val err  70.89 %, patience  15 \n",
      "\n",
      "Epoch  35 , training err  71.05 %, val err  71.12 %, patience  14 \n",
      "\n",
      "Epoch  36 , training err  71.84 %, val err  72.03 %, patience  13 \n",
      "\n",
      "Epoch  37 , training err  72.602 %, val err  72.65 %, patience  12 \n",
      "\n",
      "Epoch  38 , training err  73.632 %, val err  73.56 %, patience  11 \n",
      "\n",
      "Epoch  39 , training err  72.88 %, val err  72.71 %, patience  10 \n",
      "\n",
      "Epoch  40 , training err  72.76 %, val err  72.69 %, patience  9 \n",
      "\n",
      "Epoch  41 , training err  73.388 %, val err  73.22 %, patience  8 \n",
      "\n",
      "Epoch  42 , training err  73.566 %, val err  73.5 %, patience  7 \n",
      "\n",
      "Epoch  43 , training err  73.81 %, val err  73.63 %, patience  6 \n",
      "\n",
      "Epoch  44 , training err  73.714 %, val err  73.52 %, patience  5 \n",
      "\n",
      "Epoch  45 , training err  73.732 %, val err  73.53 %, patience  4 \n",
      "\n",
      "Epoch  46 , training err  75.204 %, val err  74.96 %, patience  3 \n",
      "\n",
      "Epoch  47 , training err  74.77 %, val err  74.67 %, patience  2 \n",
      "\n",
      "Epoch  48 , training err  75.416 %, val err  75.2 %, patience  1 \n",
      "\n",
      "Epoch  49 , training err  75.626 %, val err  75.3 %, patience  0 \n",
      "\n",
      "Best val err  67.7 % at epoch  29  corresponding train err  67.47 %\n",
      "Epoch  0 , training err  89.53 %, val err  88.92 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  85.968 %, val err  84.86 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  79.07 %, val err  79.33 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  75.874 %, val err  75.79 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  72.718 %, val err  72.3 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  69.466 %, val err  68.55 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  67.998 %, val err  68.02 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  70.036 %, val err  70.1 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  80.1 %, val err  80.87 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  79.642 %, val err  80.26 %, patience  17 \n",
      "\n",
      "Epoch  10 , training err  79.128 %, val err  79.75 %, patience  16 \n",
      "\n",
      "Epoch  11 , training err  78.966 %, val err  79.6 %, patience  15 \n",
      "\n",
      "Epoch  12 , training err  78.496 %, val err  79.29 %, patience  14 \n",
      "\n",
      "Epoch  13 , training err  78.218 %, val err  79.03 %, patience  13 \n",
      "\n",
      "Epoch  14 , training err  77.356 %, val err  78.21 %, patience  12 \n",
      "\n",
      "Epoch  15 , training err  76.502 %, val err  77.5 %, patience  11 \n",
      "\n",
      "Epoch  16 , training err  76.476 %, val err  77.46 %, patience  10 \n",
      "\n",
      "Epoch  17 , training err  75.644 %, val err  76.56 %, patience  9 \n",
      "\n",
      "Epoch  18 , training err  75.016 %, val err  75.99 %, patience  8 \n",
      "\n",
      "Epoch  19 , training err  74.598 %, val err  75.59 %, patience  7 \n",
      "\n",
      "Epoch  20 , training err  74.28 %, val err  75.45 %, patience  6 \n",
      "\n",
      "Epoch  21 , training err  73.694 %, val err  74.88 %, patience  5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:121: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 , training err  100.0 %, val err  100.0 %, patience  4 \n",
      "\n",
      "Epoch  23 , training err  100.0 %, val err  100.0 %, patience  3 \n",
      "\n",
      "Epoch  24 , training err  100.0 %, val err  100.0 %, patience  2 \n",
      "\n",
      "Epoch  25 , training err  100.0 %, val err  100.0 %, patience  1 \n",
      "\n",
      "Epoch  26 , training err  100.0 %, val err  100.0 %, patience  0 \n",
      "\n",
      "Best val err  68.02 % at epoch  6  corresponding train err  67.998 %\n",
      "Epoch  0 , training err  88.64 %, val err  89.34 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  78.806 %, val err  78.61 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  62.652 %, val err  62.23 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  32.358 %, val err  30.77 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  22.776 %, val err  20.71 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  19.26 %, val err  17.67 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  17.376 %, val err  15.76 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  15.86 %, val err  14.59 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  15.17 %, val err  13.88 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  14.608 %, val err  13.37 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  14.24 %, val err  13.06 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  13.842 %, val err  12.64 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  13.584 %, val err  12.49 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  13.474 %, val err  12.31 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  13.242 %, val err  12.04 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  13.19 %, val err  12.11 %, patience  19 \n",
      "\n",
      "Epoch  16 , training err  13.038 %, val err  12.1 %, patience  18 \n",
      "\n",
      "Epoch  17 , training err  12.824 %, val err  11.86 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  12.722 %, val err  11.77 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  12.734 %, val err  11.65 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  12.632 %, val err  11.64 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  12.5 %, val err  11.55 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  12.4 %, val err  11.45 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  12.47 %, val err  11.51 %, patience  19 \n",
      "\n",
      "Epoch  24 , training err  12.464 %, val err  11.34 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  12.446 %, val err  11.34 %, patience  19 \n",
      "\n",
      "Epoch  26 , training err  12.52 %, val err  11.34 %, patience  18 \n",
      "\n",
      "Epoch  27 , training err  12.598 %, val err  11.26 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  12.946 %, val err  11.43 %, patience  19 \n",
      "\n",
      "Epoch  29 , training err  15.074 %, val err  13.34 %, patience  18 \n",
      "\n",
      "Epoch  30 , training err  21.026 %, val err  18.84 %, patience  17 \n",
      "\n",
      "Epoch  31 , training err  19.576 %, val err  17.87 %, patience  16 \n",
      "\n",
      "Epoch  32 , training err  18.736 %, val err  17.08 %, patience  15 \n",
      "\n",
      "Epoch  33 , training err  18.228 %, val err  16.6 %, patience  14 \n",
      "\n",
      "Epoch  34 , training err  17.6 %, val err  16.03 %, patience  13 \n",
      "\n",
      "Epoch  35 , training err  17.082 %, val err  15.66 %, patience  12 \n",
      "\n",
      "Epoch  36 , training err  16.846 %, val err  15.42 %, patience  11 \n",
      "\n",
      "Epoch  37 , training err  16.6 %, val err  15.25 %, patience  10 \n",
      "\n",
      "Epoch  38 , training err  16.45 %, val err  15.26 %, patience  9 \n",
      "\n",
      "Epoch  39 , training err  16.052 %, val err  15.07 %, patience  8 \n",
      "\n",
      "Epoch  40 , training err  15.956 %, val err  14.73 %, patience  7 \n",
      "\n",
      "Epoch  41 , training err  15.762 %, val err  14.68 %, patience  6 \n",
      "\n",
      "Epoch  42 , training err  15.764 %, val err  14.65 %, patience  5 \n",
      "\n",
      "Epoch  43 , training err  15.634 %, val err  14.62 %, patience  4 \n",
      "\n",
      "Epoch  44 , training err  15.546 %, val err  14.52 %, patience  3 \n",
      "\n",
      "Epoch  45 , training err  15.404 %, val err  14.36 %, patience  2 \n",
      "\n",
      "Epoch  46 , training err  15.35 %, val err  14.36 %, patience  1 \n",
      "\n",
      "Epoch  47 , training err  15.238 %, val err  14.27 %, patience  0 \n",
      "\n",
      "Best val err  11.26 % at epoch  27  corresponding train err  12.598 %\n",
      "Epoch  0 , training err  71.462 %, val err  71.07 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  54.02 %, val err  53.66 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  29.02 %, val err  27.09 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  20.284 %, val err  18.28 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  16.678 %, val err  14.99 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  14.352 %, val err  12.89 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  12.862 %, val err  11.77 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  12.112 %, val err  11.09 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  11.496 %, val err  10.49 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  11.076 %, val err  10.04 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  10.644 %, val err  9.57 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  10.362 %, val err  9.41 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  10.05 %, val err  9.04 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  9.842 %, val err  8.89 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  9.686 %, val err  8.71 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  9.486 %, val err  8.57 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  9.31 %, val err  8.54 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  9.132 %, val err  8.45 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  8.988 %, val err  8.28 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  8.792 %, val err  7.94 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  8.772 %, val err  7.94 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  8.682 %, val err  7.92 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  8.69 %, val err  7.98 %, patience  19 \n",
      "\n",
      "Epoch  23 , training err  8.656 %, val err  7.9 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  8.738 %, val err  8.3 %, patience  19 \n",
      "\n",
      "Epoch  25 , training err  9.09 %, val err  8.56 %, patience  18 \n",
      "\n",
      "Epoch  26 , training err  9.658 %, val err  8.89 %, patience  17 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27 , training err  100.0 %, val err  9.05 %, patience  16 \n",
      "\n",
      "Epoch  28 , training err  100.0 %, val err  100.0 %, patience  15 \n",
      "\n",
      "Epoch  29 , training err  100.0 %, val err  100.0 %, patience  14 \n",
      "\n",
      "Epoch  30 , training err  100.0 %, val err  100.0 %, patience  13 \n",
      "\n",
      "Epoch  31 , training err  100.0 %, val err  100.0 %, patience  12 \n",
      "\n",
      "Epoch  32 , training err  100.0 %, val err  100.0 %, patience  11 \n",
      "\n",
      "Epoch  33 , training err  100.0 %, val err  100.0 %, patience  10 \n",
      "\n",
      "Epoch  34 , training err  100.0 %, val err  100.0 %, patience  9 \n",
      "\n",
      "Epoch  35 , training err  100.0 %, val err  100.0 %, patience  8 \n",
      "\n",
      "Epoch  36 , training err  100.0 %, val err  100.0 %, patience  7 \n",
      "\n",
      "Epoch  37 , training err  100.0 %, val err  100.0 %, patience  6 \n",
      "\n",
      "Epoch  38 , training err  100.0 %, val err  100.0 %, patience  5 \n",
      "\n",
      "Epoch  39 , training err  100.0 %, val err  100.0 %, patience  4 \n",
      "\n",
      "Epoch  40 , training err  100.0 %, val err  100.0 %, patience  3 \n",
      "\n",
      "Epoch  41 , training err  100.0 %, val err  100.0 %, patience  2 \n",
      "\n",
      "Epoch  42 , training err  100.0 %, val err  100.0 %, patience  1 \n",
      "\n",
      "Epoch  43 , training err  100.0 %, val err  100.0 %, patience  0 \n",
      "\n",
      "Best val err  7.9 % at epoch  23  corresponding train err  8.656 %\n",
      "Epoch  0 , training err  72.042 %, val err  72.25 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  51.71 %, val err  50.61 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  26.784 %, val err  24.28 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  19.672 %, val err  17.56 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  15.766 %, val err  14.28 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  13.866 %, val err  12.46 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  12.57 %, val err  11.42 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  11.75 %, val err  10.68 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  11.22 %, val err  10.26 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  10.728 %, val err  9.95 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  10.476 %, val err  9.61 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  10.078 %, val err  9.29 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.804 %, val err  9.06 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  9.602 %, val err  8.88 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  9.454 %, val err  8.85 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  9.242 %, val err  8.53 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  9.19 %, val err  8.4 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  9.046 %, val err  8.29 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  8.842 %, val err  8.0 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  8.692 %, val err  7.93 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  8.506 %, val err  7.81 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  8.316 %, val err  7.65 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  8.248 %, val err  7.67 %, patience  19 \n",
      "\n",
      "Epoch  23 , training err  8.108 %, val err  7.44 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  7.934 %, val err  7.2 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  7.76 %, val err  7.07 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  7.59 %, val err  6.98 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  7.528 %, val err  6.99 %, patience  19 \n",
      "\n",
      "Epoch  28 , training err  7.406 %, val err  6.86 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  7.28 %, val err  6.77 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  7.206 %, val err  6.47 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  7.114 %, val err  6.55 %, patience  19 \n",
      "\n",
      "Epoch  32 , training err  7.054 %, val err  6.48 %, patience  18 \n",
      "\n",
      "Epoch  33 , training err  6.956 %, val err  6.44 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  6.972 %, val err  6.3 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  6.898 %, val err  6.35 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  6.832 %, val err  6.36 %, patience  18 \n",
      "\n",
      "Epoch  37 , training err  6.684 %, val err  6.22 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  6.698 %, val err  6.28 %, patience  19 \n",
      "\n",
      "Epoch  39 , training err  6.548 %, val err  6.19 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  6.504 %, val err  6.17 %, patience  20 \n",
      "\n",
      "Epoch  41 , training err  6.48 %, val err  6.04 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  6.526 %, val err  6.0 %, patience  20 \n",
      "\n",
      "Epoch  43 , training err  6.406 %, val err  6.07 %, patience  19 \n",
      "\n",
      "Epoch  44 , training err  6.408 %, val err  6.03 %, patience  18 \n",
      "\n",
      "Epoch  45 , training err  6.314 %, val err  5.98 %, patience  20 \n",
      "\n",
      "Epoch  46 , training err  6.342 %, val err  6.12 %, patience  19 \n",
      "\n",
      "Epoch  47 , training err  6.368 %, val err  6.1 %, patience  18 \n",
      "\n",
      "Epoch  48 , training err  6.29 %, val err  5.89 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  6.354 %, val err  6.0 %, patience  19 \n",
      "\n",
      "Epoch  50 , training err  6.334 %, val err  6.01 %, patience  18 \n",
      "\n",
      "Epoch  51 , training err  6.238 %, val err  5.88 %, patience  20 \n",
      "\n",
      "Epoch  52 , training err  6.318 %, val err  5.94 %, patience  19 \n",
      "\n",
      "Epoch  53 , training err  6.242 %, val err  5.88 %, patience  18 \n",
      "\n",
      "Epoch  54 , training err  6.288 %, val err  6.02 %, patience  17 \n",
      "\n",
      "Epoch  55 , training err  6.25 %, val err  5.9 %, patience  16 \n",
      "\n",
      "Epoch  56 , training err  6.238 %, val err  5.86 %, patience  20 \n",
      "\n",
      "Epoch  57 , training err  6.268 %, val err  5.95 %, patience  19 \n",
      "\n",
      "Epoch  58 , training err  6.262 %, val err  5.88 %, patience  18 \n",
      "\n",
      "Epoch  59 , training err  6.216 %, val err  5.85 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  6.27 %, val err  5.93 %, patience  19 \n",
      "\n",
      "Epoch  61 , training err  6.22 %, val err  6.09 %, patience  18 \n",
      "\n",
      "Epoch  62 , training err  6.272 %, val err  6.15 %, patience  17 \n",
      "\n",
      "Epoch  63 , training err  6.208 %, val err  5.88 %, patience  16 \n",
      "\n",
      "Epoch  64 , training err  6.336 %, val err  6.16 %, patience  15 \n",
      "\n",
      "Epoch  65 , training err  6.256 %, val err  6.0 %, patience  14 \n",
      "\n",
      "Epoch  66 , training err  6.41 %, val err  6.21 %, patience  13 \n",
      "\n",
      "Epoch  67 , training err  6.354 %, val err  6.2 %, patience  12 \n",
      "\n",
      "Epoch  68 , training err  6.374 %, val err  6.16 %, patience  11 \n",
      "\n",
      "Epoch  69 , training err  6.46 %, val err  6.18 %, patience  10 \n",
      "\n",
      "Epoch  70 , training err  6.536 %, val err  6.12 %, patience  9 \n",
      "\n",
      "Epoch  71 , training err  6.494 %, val err  6.15 %, patience  8 \n",
      "\n",
      "Epoch  72 , training err  6.47 %, val err  6.23 %, patience  7 \n",
      "\n",
      "Epoch  73 , training err  6.468 %, val err  6.32 %, patience  6 \n",
      "\n",
      "Epoch  74 , training err  6.516 %, val err  6.22 %, patience  5 \n",
      "\n",
      "Epoch  75 , training err  6.73 %, val err  6.53 %, patience  4 \n",
      "\n",
      "Epoch  76 , training err  6.628 %, val err  6.43 %, patience  3 \n",
      "\n",
      "Epoch  77 , training err  6.662 %, val err  6.42 %, patience  2 \n",
      "\n",
      "Epoch  78 , training err  6.794 %, val err  6.7 %, patience  1 \n",
      "\n",
      "Epoch  79 , training err  6.804 %, val err  6.74 %, patience  0 \n",
      "\n",
      "Best val err  5.85 % at epoch  59  corresponding train err  6.216 %\n",
      "Epoch  0 , training err  76.424 %, val err  76.46 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  37.822 %, val err  36.4 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  24.368 %, val err  22.27 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  17.666 %, val err  16.05 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  14.932 %, val err  13.53 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  13.354 %, val err  11.96 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  12.318 %, val err  11.25 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  11.62 %, val err  10.51 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  11.196 %, val err  10.18 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  10.818 %, val err  9.78 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  10.438 %, val err  9.46 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  10.16 %, val err  9.24 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.91 %, val err  8.9 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  9.666 %, val err  8.8 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  9.446 %, val err  8.57 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  9.24 %, val err  8.3 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  9.112 %, val err  8.2 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  8.882 %, val err  8.03 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  8.78 %, val err  7.91 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  8.678 %, val err  7.99 %, patience  19 \n",
      "\n",
      "Epoch  20 , training err  8.544 %, val err  7.82 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  8.492 %, val err  7.85 %, patience  19 \n",
      "\n",
      "Epoch  22 , training err  8.312 %, val err  7.82 %, patience  18 \n",
      "\n",
      "Epoch  23 , training err  8.13 %, val err  7.71 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  7.96 %, val err  7.62 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  7.896 %, val err  7.49 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  7.744 %, val err  7.5 %, patience  19 \n",
      "\n",
      "Epoch  27 , training err  7.604 %, val err  7.26 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  7.436 %, val err  6.96 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  7.316 %, val err  6.93 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  7.244 %, val err  6.76 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  7.19 %, val err  6.69 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  7.132 %, val err  6.63 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  7.074 %, val err  6.68 %, patience  19 \n",
      "\n",
      "Epoch  34 , training err  7.018 %, val err  6.64 %, patience  18 \n",
      "\n",
      "Epoch  35 , training err  7.004 %, val err  6.63 %, patience  17 \n",
      "\n",
      "Epoch  36 , training err  6.93 %, val err  6.6 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  6.898 %, val err  6.52 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  6.876 %, val err  6.45 %, patience  20 \n",
      "\n",
      "Epoch  39 , training err  6.836 %, val err  6.48 %, patience  19 \n",
      "\n",
      "Epoch  40 , training err  6.712 %, val err  6.4 %, patience  20 \n",
      "\n",
      "Epoch  41 , training err  6.724 %, val err  6.37 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  6.684 %, val err  6.26 %, patience  20 \n",
      "\n",
      "Epoch  43 , training err  6.652 %, val err  6.24 %, patience  20 \n",
      "\n",
      "Epoch  44 , training err  6.6 %, val err  6.13 %, patience  20 \n",
      "\n",
      "Epoch  45 , training err  6.544 %, val err  6.15 %, patience  19 \n",
      "\n",
      "Epoch  46 , training err  6.484 %, val err  6.06 %, patience  20 \n",
      "\n",
      "Epoch  47 , training err  6.414 %, val err  5.95 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  6.368 %, val err  5.79 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  6.302 %, val err  5.83 %, patience  19 \n",
      "\n",
      "Epoch  50 , training err  6.2 %, val err  5.81 %, patience  18 \n",
      "\n",
      "Epoch  51 , training err  6.192 %, val err  5.8 %, patience  17 \n",
      "\n",
      "Epoch  52 , training err  6.144 %, val err  5.68 %, patience  20 \n",
      "\n",
      "Epoch  53 , training err  6.074 %, val err  5.73 %, patience  19 \n",
      "\n",
      "Epoch  54 , training err  6.03 %, val err  5.58 %, patience  20 \n",
      "\n",
      "Epoch  55 , training err  6.002 %, val err  5.46 %, patience  20 \n",
      "\n",
      "Epoch  56 , training err  6.03 %, val err  5.74 %, patience  19 \n",
      "\n",
      "Epoch  57 , training err  6.004 %, val err  5.65 %, patience  18 \n",
      "\n",
      "Epoch  58 , training err  5.97 %, val err  5.66 %, patience  17 \n",
      "\n",
      "Epoch  59 , training err  6.018 %, val err  5.67 %, patience  16 \n",
      "\n",
      "Epoch  60 , training err  5.91 %, val err  5.58 %, patience  15 \n",
      "\n",
      "Epoch  61 , training err  5.956 %, val err  5.63 %, patience  14 \n",
      "\n",
      "Epoch  62 , training err  5.89 %, val err  5.66 %, patience  13 \n",
      "\n",
      "Epoch  63 , training err  5.996 %, val err  5.56 %, patience  12 \n",
      "\n",
      "Epoch  64 , training err  5.858 %, val err  5.47 %, patience  11 \n",
      "\n",
      "Epoch  65 , training err  5.898 %, val err  5.71 %, patience  10 \n",
      "\n",
      "Epoch  66 , training err  5.88 %, val err  5.52 %, patience  9 \n",
      "\n",
      "Epoch  67 , training err  5.886 %, val err  5.54 %, patience  8 \n",
      "\n",
      "Epoch  68 , training err  6.02 %, val err  5.68 %, patience  7 \n",
      "\n",
      "Epoch  69 , training err  5.89 %, val err  5.68 %, patience  6 \n",
      "\n",
      "Epoch  70 , training err  5.872 %, val err  5.59 %, patience  5 \n",
      "\n",
      "Epoch  71 , training err  5.838 %, val err  5.63 %, patience  4 \n",
      "\n",
      "Epoch  72 , training err  5.892 %, val err  5.67 %, patience  3 \n",
      "\n",
      "Epoch  73 , training err  5.934 %, val err  5.66 %, patience  2 \n",
      "\n",
      "Epoch  74 , training err  5.902 %, val err  5.7 %, patience  1 \n",
      "\n",
      "Epoch  75 , training err  5.946 %, val err  5.74 %, patience  0 \n",
      "\n",
      "Best val err  5.46 % at epoch  55  corresponding train err  6.002 %\n",
      "Epoch  0 , training err  72.63 %, val err  72.98 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  41.366 %, val err  40.46 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  23.454 %, val err  21.18 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  17.402 %, val err  15.37 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  14.714 %, val err  13.09 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  13.082 %, val err  11.8 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  11.97 %, val err  10.8 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  11.418 %, val err  10.38 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.926 %, val err  9.84 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  10.418 %, val err  9.42 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  10.104 %, val err  9.17 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.884 %, val err  8.98 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.574 %, val err  8.79 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  9.362 %, val err  8.48 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  9.168 %, val err  8.37 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  8.894 %, val err  8.12 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  8.772 %, val err  7.93 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  8.532 %, val err  7.79 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  8.292 %, val err  7.48 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  8.034 %, val err  7.34 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.83 %, val err  7.26 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  7.662 %, val err  7.08 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  7.404 %, val err  6.93 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  7.262 %, val err  6.78 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  7.146 %, val err  6.54 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  7.0 %, val err  6.51 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.818 %, val err  6.28 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  6.712 %, val err  6.22 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  6.64 %, val err  6.03 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  6.546 %, val err  5.96 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  6.462 %, val err  5.87 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  6.382 %, val err  5.79 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  6.312 %, val err  5.7 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  6.232 %, val err  5.64 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  6.198 %, val err  5.51 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  6.154 %, val err  5.53 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  5.98 %, val err  5.36 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  6.01 %, val err  5.51 %, patience  19 \n",
      "\n",
      "Epoch  38 , training err  6.006 %, val err  5.4 %, patience  18 \n",
      "\n",
      "Epoch  39 , training err  5.902 %, val err  5.44 %, patience  17 \n",
      "\n",
      "Epoch  40 , training err  5.958 %, val err  5.56 %, patience  16 \n",
      "\n",
      "Epoch  41 , training err  5.844 %, val err  5.48 %, patience  15 \n",
      "\n",
      "Epoch  42 , training err  5.866 %, val err  5.49 %, patience  14 \n",
      "\n",
      "Epoch  43 , training err  5.93 %, val err  5.59 %, patience  13 \n",
      "\n",
      "Epoch  44 , training err  5.83 %, val err  5.45 %, patience  12 \n",
      "\n",
      "Epoch  45 , training err  5.882 %, val err  5.42 %, patience  11 \n",
      "\n",
      "Epoch  46 , training err  5.856 %, val err  5.42 %, patience  10 \n",
      "\n",
      "Epoch  47 , training err  5.906 %, val err  5.42 %, patience  9 \n",
      "\n",
      "Epoch  48 , training err  5.844 %, val err  5.32 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  5.908 %, val err  5.44 %, patience  19 \n",
      "\n",
      "Epoch  50 , training err  5.836 %, val err  5.4 %, patience  18 \n",
      "\n",
      "Epoch  51 , training err  5.936 %, val err  5.58 %, patience  17 \n",
      "\n",
      "Epoch  52 , training err  5.854 %, val err  5.45 %, patience  16 \n",
      "\n",
      "Epoch  53 , training err  5.852 %, val err  5.44 %, patience  15 \n",
      "\n",
      "Epoch  54 , training err  5.922 %, val err  5.46 %, patience  14 \n",
      "\n",
      "Epoch  55 , training err  5.786 %, val err  5.43 %, patience  13 \n",
      "\n",
      "Epoch  56 , training err  5.766 %, val err  5.36 %, patience  12 \n",
      "\n",
      "Epoch  57 , training err  5.664 %, val err  5.29 %, patience  20 \n",
      "\n",
      "Epoch  58 , training err  5.664 %, val err  5.28 %, patience  20 \n",
      "\n",
      "Epoch  59 , training err  5.728 %, val err  5.24 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  5.514 %, val err  5.07 %, patience  20 \n",
      "\n",
      "Epoch  61 , training err  5.39 %, val err  5.03 %, patience  20 \n",
      "\n",
      "Epoch  62 , training err  5.436 %, val err  5.08 %, patience  19 \n",
      "\n",
      "Epoch  63 , training err  5.316 %, val err  4.99 %, patience  20 \n",
      "\n",
      "Epoch  64 , training err  5.342 %, val err  4.99 %, patience  19 \n",
      "\n",
      "Epoch  65 , training err  5.352 %, val err  4.92 %, patience  20 \n",
      "\n",
      "Epoch  66 , training err  5.27 %, val err  4.76 %, patience  20 \n",
      "\n",
      "Epoch  67 , training err  5.256 %, val err  4.75 %, patience  20 \n",
      "\n",
      "Epoch  68 , training err  5.328 %, val err  4.82 %, patience  19 \n",
      "\n",
      "Epoch  69 , training err  5.3 %, val err  4.82 %, patience  18 \n",
      "\n",
      "Epoch  70 , training err  5.294 %, val err  4.87 %, patience  17 \n",
      "\n",
      "Epoch  71 , training err  5.314 %, val err  5.09 %, patience  16 \n",
      "\n",
      "Epoch  72 , training err  5.254 %, val err  4.99 %, patience  15 \n",
      "\n",
      "Epoch  73 , training err  5.26 %, val err  4.88 %, patience  14 \n",
      "\n",
      "Epoch  74 , training err  5.282 %, val err  5.02 %, patience  13 \n",
      "\n",
      "Epoch  75 , training err  5.22 %, val err  5.03 %, patience  12 \n",
      "\n",
      "Epoch  76 , training err  5.256 %, val err  5.15 %, patience  11 \n",
      "\n",
      "Epoch  77 , training err  5.25 %, val err  5.06 %, patience  10 \n",
      "\n",
      "Epoch  78 , training err  5.254 %, val err  5.11 %, patience  9 \n",
      "\n",
      "Epoch  79 , training err  5.18 %, val err  5.14 %, patience  8 \n",
      "\n",
      "Epoch  80 , training err  5.242 %, val err  5.15 %, patience  7 \n",
      "\n",
      "Epoch  81 , training err  5.172 %, val err  5.06 %, patience  6 \n",
      "\n",
      "Epoch  82 , training err  5.162 %, val err  5.04 %, patience  5 \n",
      "\n",
      "Epoch  83 , training err  5.178 %, val err  5.08 %, patience  4 \n",
      "\n",
      "Epoch  84 , training err  5.12 %, val err  5.01 %, patience  3 \n",
      "\n",
      "Epoch  85 , training err  5.17 %, val err  5.14 %, patience  2 \n",
      "\n",
      "Epoch  86 , training err  5.178 %, val err  5.04 %, patience  1 \n",
      "\n",
      "Epoch  87 , training err  5.182 %, val err  5.16 %, patience  0 \n",
      "\n",
      "Best val err  4.75 % at epoch  67  corresponding train err  5.256 %\n",
      "Epoch  0 , training err  62.796 %, val err  62.7 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  36.31 %, val err  35.0 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  21.534 %, val err  19.51 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  17.082 %, val err  15.22 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  14.638 %, val err  13.25 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  12.966 %, val err  11.57 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  11.888 %, val err  10.66 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  11.216 %, val err  10.15 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.778 %, val err  9.81 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  10.298 %, val err  9.29 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  9.874 %, val err  9.03 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.664 %, val err  8.76 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.312 %, val err  8.54 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  9.048 %, val err  8.3 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  8.734 %, val err  8.02 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  8.486 %, val err  7.8 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  8.226 %, val err  7.66 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  8.0 %, val err  7.33 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  7.79 %, val err  7.25 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  7.658 %, val err  7.0 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.572 %, val err  6.88 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  7.358 %, val err  6.84 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  7.266 %, val err  6.7 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  7.102 %, val err  6.46 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  6.96 %, val err  6.34 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  6.868 %, val err  6.18 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.778 %, val err  6.1 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  6.678 %, val err  5.97 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  6.578 %, val err  5.93 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  6.412 %, val err  5.77 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  6.376 %, val err  5.72 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  6.216 %, val err  5.68 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  6.194 %, val err  5.63 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  6.078 %, val err  5.56 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  5.994 %, val err  5.45 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  5.992 %, val err  5.49 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  5.954 %, val err  5.36 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  5.87 %, val err  5.31 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  5.842 %, val err  5.47 %, patience  19 \n",
      "\n",
      "Epoch  39 , training err  5.744 %, val err  5.34 %, patience  18 \n",
      "\n",
      "Epoch  40 , training err  5.724 %, val err  5.31 %, patience  17 \n",
      "\n",
      "Epoch  41 , training err  5.704 %, val err  5.25 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  5.606 %, val err  5.35 %, patience  19 \n",
      "\n",
      "Epoch  43 , training err  5.584 %, val err  5.46 %, patience  18 \n",
      "\n",
      "Epoch  44 , training err  5.474 %, val err  5.28 %, patience  17 \n",
      "\n",
      "Epoch  45 , training err  5.482 %, val err  5.25 %, patience  16 \n",
      "\n",
      "Epoch  46 , training err  5.468 %, val err  5.11 %, patience  20 \n",
      "\n",
      "Epoch  47 , training err  5.418 %, val err  5.1 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  5.326 %, val err  5.2 %, patience  19 \n",
      "\n",
      "Epoch  49 , training err  5.324 %, val err  5.01 %, patience  20 \n",
      "\n",
      "Epoch  50 , training err  5.312 %, val err  4.94 %, patience  20 \n",
      "\n",
      "Epoch  51 , training err  5.288 %, val err  4.99 %, patience  19 \n",
      "\n",
      "Epoch  52 , training err  5.266 %, val err  5.03 %, patience  18 \n",
      "\n",
      "Epoch  53 , training err  5.286 %, val err  5.12 %, patience  17 \n",
      "\n",
      "Epoch  54 , training err  5.282 %, val err  5.14 %, patience  16 \n",
      "\n",
      "Epoch  55 , training err  5.2 %, val err  5.04 %, patience  15 \n",
      "\n",
      "Epoch  56 , training err  5.174 %, val err  5.02 %, patience  14 \n",
      "\n",
      "Epoch  57 , training err  5.172 %, val err  5.07 %, patience  13 \n",
      "\n",
      "Epoch  58 , training err  5.07 %, val err  5.01 %, patience  12 \n",
      "\n",
      "Epoch  59 , training err  5.056 %, val err  4.95 %, patience  11 \n",
      "\n",
      "Epoch  60 , training err  5.068 %, val err  4.94 %, patience  10 \n",
      "\n",
      "Epoch  61 , training err  5.064 %, val err  5.06 %, patience  9 \n",
      "\n",
      "Epoch  62 , training err  5.03 %, val err  4.88 %, patience  20 \n",
      "\n",
      "Epoch  63 , training err  5.118 %, val err  4.98 %, patience  19 \n",
      "\n",
      "Epoch  64 , training err  5.03 %, val err  4.82 %, patience  20 \n",
      "\n",
      "Epoch  65 , training err  4.994 %, val err  4.72 %, patience  20 \n",
      "\n",
      "Epoch  66 , training err  4.956 %, val err  4.65 %, patience  20 \n",
      "\n",
      "Epoch  67 , training err  4.966 %, val err  4.72 %, patience  19 \n",
      "\n",
      "Epoch  68 , training err  4.96 %, val err  4.79 %, patience  18 \n",
      "\n",
      "Epoch  69 , training err  4.938 %, val err  4.73 %, patience  17 \n",
      "\n",
      "Epoch  70 , training err  4.904 %, val err  4.65 %, patience  16 \n",
      "\n",
      "Epoch  71 , training err  4.868 %, val err  4.71 %, patience  15 \n",
      "\n",
      "Epoch  72 , training err  4.902 %, val err  4.89 %, patience  14 \n",
      "\n",
      "Epoch  73 , training err  4.866 %, val err  4.72 %, patience  13 \n",
      "\n",
      "Epoch  74 , training err  4.906 %, val err  4.79 %, patience  12 \n",
      "\n",
      "Epoch  75 , training err  4.868 %, val err  4.78 %, patience  11 \n",
      "\n",
      "Epoch  76 , training err  4.862 %, val err  4.63 %, patience  20 \n",
      "\n",
      "Epoch  77 , training err  4.86 %, val err  4.72 %, patience  19 \n",
      "\n",
      "Epoch  78 , training err  4.812 %, val err  4.66 %, patience  18 \n",
      "\n",
      "Epoch  79 , training err  4.744 %, val err  4.78 %, patience  17 \n",
      "\n",
      "Epoch  80 , training err  4.88 %, val err  4.79 %, patience  16 \n",
      "\n",
      "Epoch  81 , training err  4.828 %, val err  4.85 %, patience  15 \n",
      "\n",
      "Epoch  82 , training err  4.83 %, val err  4.81 %, patience  14 \n",
      "\n",
      "Epoch  83 , training err  4.784 %, val err  4.68 %, patience  13 \n",
      "\n",
      "Epoch  84 , training err  4.758 %, val err  4.7 %, patience  12 \n",
      "\n",
      "Epoch  85 , training err  100.0 %, val err  100.0 %, patience  11 \n",
      "\n",
      "Epoch  86 , training err  100.0 %, val err  100.0 %, patience  10 \n",
      "\n",
      "Epoch  87 , training err  100.0 %, val err  100.0 %, patience  9 \n",
      "\n",
      "Epoch  88 , training err  100.0 %, val err  100.0 %, patience  8 \n",
      "\n",
      "Epoch  89 , training err  100.0 %, val err  100.0 %, patience  7 \n",
      "\n",
      "Epoch  90 , training err  100.0 %, val err  100.0 %, patience  6 \n",
      "\n",
      "Epoch  91 , training err  100.0 %, val err  100.0 %, patience  5 \n",
      "\n",
      "Epoch  92 , training err  100.0 %, val err  100.0 %, patience  4 \n",
      "\n",
      "Epoch  93 , training err  100.0 %, val err  100.0 %, patience  3 \n",
      "\n",
      "Epoch  94 , training err  100.0 %, val err  100.0 %, patience  2 \n",
      "\n",
      "Epoch  95 , training err  100.0 %, val err  100.0 %, patience  1 \n",
      "\n",
      "Epoch  96 , training err  100.0 %, val err  100.0 %, patience  0 \n",
      "\n",
      "Best val err  4.63 % at epoch  76  corresponding train err  4.862 %\n",
      "Epoch  0 , training err  73.8 %, val err  72.63 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  30.386 %, val err  28.67 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  19.342 %, val err  17.4 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  15.534 %, val err  13.97 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  13.472 %, val err  12.13 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  12.308 %, val err  11.09 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  11.472 %, val err  10.25 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  10.912 %, val err  10.04 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.482 %, val err  9.45 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  10.062 %, val err  9.12 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  9.746 %, val err  8.96 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.5 %, val err  8.58 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.192 %, val err  8.4 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  8.842 %, val err  8.17 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  8.64 %, val err  7.91 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  8.274 %, val err  7.7 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  8.106 %, val err  7.48 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  7.874 %, val err  7.29 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  7.584 %, val err  6.92 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  7.432 %, val err  6.85 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.23 %, val err  6.69 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  7.01 %, val err  6.56 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  6.91 %, val err  6.29 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  6.722 %, val err  6.17 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  6.596 %, val err  6.01 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  6.446 %, val err  5.97 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.358 %, val err  5.84 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  6.284 %, val err  5.66 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  6.214 %, val err  5.6 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  6.154 %, val err  5.47 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  6.02 %, val err  5.42 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  6.014 %, val err  5.26 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  5.9 %, val err  5.32 %, patience  19 \n",
      "\n",
      "Epoch  33 , training err  5.886 %, val err  5.15 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  5.8 %, val err  5.07 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  5.802 %, val err  5.12 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  5.692 %, val err  5.01 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  5.65 %, val err  5.09 %, patience  19 \n",
      "\n",
      "Epoch  38 , training err  5.666 %, val err  4.91 %, patience  20 \n",
      "\n",
      "Epoch  39 , training err  5.572 %, val err  4.91 %, patience  19 \n",
      "\n",
      "Epoch  40 , training err  5.438 %, val err  4.91 %, patience  18 \n",
      "\n",
      "Epoch  41 , training err  5.436 %, val err  4.82 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  5.382 %, val err  4.95 %, patience  19 \n",
      "\n",
      "Epoch  43 , training err  5.4 %, val err  4.87 %, patience  18 \n",
      "\n",
      "Epoch  44 , training err  5.314 %, val err  4.82 %, patience  17 \n",
      "\n",
      "Epoch  45 , training err  5.326 %, val err  4.9 %, patience  16 \n",
      "\n",
      "Epoch  46 , training err  5.186 %, val err  4.89 %, patience  15 \n",
      "\n",
      "Epoch  47 , training err  5.248 %, val err  5.04 %, patience  14 \n",
      "\n",
      "Epoch  48 , training err  5.154 %, val err  4.82 %, patience  13 \n",
      "\n",
      "Epoch  49 , training err  5.19 %, val err  4.75 %, patience  20 \n",
      "\n",
      "Epoch  50 , training err  5.056 %, val err  4.74 %, patience  20 \n",
      "\n",
      "Epoch  51 , training err  4.998 %, val err  4.73 %, patience  20 \n",
      "\n",
      "Epoch  52 , training err  5.03 %, val err  4.81 %, patience  19 \n",
      "\n",
      "Epoch  53 , training err  4.942 %, val err  4.66 %, patience  20 \n",
      "\n",
      "Epoch  54 , training err  4.912 %, val err  4.72 %, patience  19 \n",
      "\n",
      "Epoch  55 , training err  4.846 %, val err  4.68 %, patience  18 \n",
      "\n",
      "Epoch  56 , training err  4.834 %, val err  4.68 %, patience  17 \n",
      "\n",
      "Epoch  57 , training err  4.744 %, val err  4.59 %, patience  20 \n",
      "\n",
      "Epoch  58 , training err  4.722 %, val err  4.55 %, patience  20 \n",
      "\n",
      "Epoch  59 , training err  4.692 %, val err  4.51 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  4.648 %, val err  4.46 %, patience  20 \n",
      "\n",
      "Epoch  61 , training err  4.614 %, val err  4.37 %, patience  20 \n",
      "\n",
      "Epoch  62 , training err  4.572 %, val err  4.35 %, patience  20 \n",
      "\n",
      "Epoch  63 , training err  4.486 %, val err  4.31 %, patience  20 \n",
      "\n",
      "Epoch  64 , training err  4.484 %, val err  4.21 %, patience  20 \n",
      "\n",
      "Epoch  65 , training err  4.522 %, val err  4.32 %, patience  19 \n",
      "\n",
      "Epoch  66 , training err  4.382 %, val err  4.36 %, patience  18 \n",
      "\n",
      "Epoch  67 , training err  4.362 %, val err  4.19 %, patience  20 \n",
      "\n",
      "Epoch  68 , training err  4.382 %, val err  4.25 %, patience  19 \n",
      "\n",
      "Epoch  69 , training err  4.344 %, val err  4.27 %, patience  18 \n",
      "\n",
      "Epoch  70 , training err  4.308 %, val err  4.14 %, patience  20 \n",
      "\n",
      "Epoch  71 , training err  4.282 %, val err  4.18 %, patience  19 \n",
      "\n",
      "Epoch  72 , training err  4.272 %, val err  4.13 %, patience  20 \n",
      "\n",
      "Epoch  73 , training err  4.244 %, val err  4.15 %, patience  19 \n",
      "\n",
      "Epoch  74 , training err  4.25 %, val err  4.19 %, patience  18 \n",
      "\n",
      "Epoch  75 , training err  4.248 %, val err  4.2 %, patience  17 \n",
      "\n",
      "Epoch  76 , training err  4.162 %, val err  4.11 %, patience  20 \n",
      "\n",
      "Epoch  77 , training err  4.154 %, val err  4.05 %, patience  20 \n",
      "\n",
      "Epoch  78 , training err  4.198 %, val err  4.11 %, patience  19 \n",
      "\n",
      "Epoch  79 , training err  4.162 %, val err  4.02 %, patience  20 \n",
      "\n",
      "Epoch  80 , training err  4.148 %, val err  4.03 %, patience  19 \n",
      "\n",
      "Epoch  81 , training err  4.114 %, val err  4.02 %, patience  18 \n",
      "\n",
      "Epoch  82 , training err  4.146 %, val err  3.98 %, patience  20 \n",
      "\n",
      "Epoch  83 , training err  4.154 %, val err  4.2 %, patience  19 \n",
      "\n",
      "Epoch  84 , training err  4.1 %, val err  4.06 %, patience  18 \n",
      "\n",
      "Epoch  85 , training err  4.15 %, val err  4.2 %, patience  17 \n",
      "\n",
      "Epoch  86 , training err  4.122 %, val err  4.18 %, patience  16 \n",
      "\n",
      "Epoch  87 , training err  4.062 %, val err  4.16 %, patience  15 \n",
      "\n",
      "Epoch  88 , training err  4.102 %, val err  4.05 %, patience  14 \n",
      "\n",
      "Epoch  89 , training err  4.148 %, val err  4.21 %, patience  13 \n",
      "\n",
      "Epoch  90 , training err  4.128 %, val err  4.16 %, patience  12 \n",
      "\n",
      "Epoch  91 , training err  4.072 %, val err  4.12 %, patience  11 \n",
      "\n",
      "Epoch  92 , training err  4.062 %, val err  4.19 %, patience  10 \n",
      "\n",
      "Epoch  93 , training err  4.082 %, val err  4.17 %, patience  9 \n",
      "\n",
      "Epoch  94 , training err  4.056 %, val err  4.1 %, patience  8 \n",
      "\n",
      "Epoch  95 , training err  4.062 %, val err  4.08 %, patience  7 \n",
      "\n",
      "Epoch  96 , training err  4.088 %, val err  4.2 %, patience  6 \n",
      "\n",
      "Epoch  97 , training err  4.008 %, val err  4.21 %, patience  5 \n",
      "\n",
      "Epoch  98 , training err  3.976 %, val err  4.17 %, patience  4 \n",
      "\n",
      "Epoch  99 , training err  4.006 %, val err  4.16 %, patience  3 \n",
      "\n",
      "Epoch  100 , training err  3.984 %, val err  4.22 %, patience  2 \n",
      "\n",
      "Epoch  101 , training err  4.052 %, val err  4.28 %, patience  1 \n",
      "\n",
      "Epoch  102 , training err  3.966 %, val err  4.21 %, patience  0 \n",
      "\n",
      "Best val err  3.98 % at epoch  82  corresponding train err  4.146 %\n",
      "Hidden layer  1 . Test error  67.31 %\n",
      "Hidden layer  2 . Test error  67.78 %\n",
      "Hidden layer  5 . Test error  12.03 %\n",
      "Hidden layer  15 . Test error  8.37 %\n",
      "Hidden layer  20 . Test error  6.21 %\n",
      "Hidden layer  30 . Test error  6.07 %\n",
      "Hidden layer  40 . Test error  5.55 %\n",
      "Hidden layer  50 . Test error  4.82 %\n",
      "Hidden layer  100 . Test error  4.59 %\n"
     ]
    }
   ],
   "source": [
    "middle = [1, 2, 5, 15, 20, 30, 40, 50, 100]\n",
    "for mid in middle:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,mid,10], 0.001, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, mid));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, mid in bestWs:\n",
    "    print  'Hidden layer ', mid,  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ta nhận thấy độ lỗi trên tập train và validation ứng với l2_reg_level = 0.001 đều cao, và cao hơn so với các giá trị l2 khác. Điều này cho thấy mô hình này bị underfiting do giá trị l2 khá cao dẫn đến việc các giá trị Ws tương ứng không được \"dao động thoải mái\" và có xu hướng gần giá trị 0 để tối ưu hàm chi phí J do đó khó đạt được tới mô hình tối ưu.\n",
    "- Ngược lại với giá trị l2 = 0, thì mô hình này có độ lỗi trên tập train xuống rất thấp nhưng đồng thời độ lỗi trên tập validation ban đầu có xu hướng xuống nhưng sau lại tăng lên. Điều này cho thấy mô hình này bị overfiting do giá trị l2 quá thấp dẫn tới việc các giá trị Ws 'thoải mái di chuyển' và cố gắng fit tập train (gồm cả nhiễu) dẫn tới việc mô hình không được tổng quát.\n",
    "- Còn với giá trị l2 = 0.001 thì ta thấy độ lỗi trên tập train và tập validation đều có xu hướng đi xuống và có giá trị thấp. Mô hình này tương đối tốt nếu so với 2 mô hình còn lại do giá trị l2 được chọn khá cân bằng (không quá cao hay thấp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  90.282 %, val err  90.17 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  90.282 %, val err  90.17 %, patience  19 \n",
      "\n",
      "Epoch  2 , training err  90.282 %, val err  90.17 %, patience  18 \n",
      "\n",
      "Epoch  3 , training err  90.282 %, val err  90.17 %, patience  17 \n",
      "\n",
      "Epoch  4 , training err  88.644 %, val err  89.36 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  88.644 %, val err  89.36 %, patience  19 \n",
      "\n",
      "Epoch  6 , training err  88.644 %, val err  89.36 %, patience  18 \n",
      "\n",
      "Epoch  7 , training err  88.644 %, val err  89.36 %, patience  17 \n",
      "\n",
      "Epoch  8 , training err  88.644 %, val err  89.36 %, patience  16 \n",
      "\n",
      "Epoch  9 , training err  88.644 %, val err  89.36 %, patience  15 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-50ba84d6b4d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mll\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbWs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mbestWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-84190e7d6ecd>\u001b[0m in \u001b[0;36mtrain_neural_net\u001b[1;34m(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0meTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0meVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mtrain_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-84190e7d6ecd>\u001b[0m in \u001b[0;36mmeanBinaryError\u001b[1;34m(X, W, Y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#for debugging: def forward_prop(X, Ws):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmyY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyY\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a64aa235bf09>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(X, Ws)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer = [[784, 50, 30, 20, 10], [784, 50, 30, 20, 20, 10]]\n",
    "for ll in layer:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, ll, 0.0001, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, ll));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, ll in bestWs:\n",
    "    print ll \n",
    "    print  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print len(bestWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
