{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Computes relu function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    ret = np.array(Z)\n",
    "    ret[ret<0] = 0\n",
    "    return ret\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "   \n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = relu(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    #Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1])/np.sqrt(X_train.shape[0]) for l in range(len(layer_sizes)-1)]\n",
    "    #print Ws\n",
    "    #return\n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = relu(Z)\n",
    "\n",
    "                    A = np.hstack((np.ones((A.shape[0],1))/10,A));\n",
    "                    \n",
    "                Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            #print np.sum(As[-1])\n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                #print delta.shape, Ws[-it+1].shape, it\n",
    "                temp = np.array(Zs[-it])\n",
    "                temp[temp > 0] = 1 \n",
    "                temp = np.hstack((np.zeros((temp.shape[0],1)),temp));\n",
    "                #print temp.shape, As[-it].shape\n",
    "                #print (delta.dot(Ws[-it+1].T)).shape, temp.shape\n",
    "                delta = delta.dot(Ws[-it+1].T)*temp\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "                #grad = grad[:,1:] + 1.0*l2_reg_level*Ws[-it];\n",
    "                #if it == 2 and epoch == 0:\n",
    "                    #print (grad[:,1:]).shape, (Ws[-2]).shape\n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực nghiệm mạng nơ-ron 1 lớp ẩn\n",
    "Thêm các kích thước lớp ẩn vào middle.\n",
    "\n",
    "Ví dụ middle = [1,2,3]\n",
    "- Huấn luyện 3 mạng có số nút của lớp ẩn lần lượt là 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  60.718 %, val err  59.29 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  28.964 %, val err  27.23 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  18.58 %, val err  16.58 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  15.098 %, val err  13.5 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  13.138 %, val err  11.81 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  12.12 %, val err  10.9 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  11.324 %, val err  10.24 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  10.762 %, val err  9.89 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.42 %, val err  9.34 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  9.982 %, val err  8.99 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  9.676 %, val err  8.79 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.356 %, val err  8.48 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.056 %, val err  8.22 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  8.732 %, val err  7.85 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  8.514 %, val err  7.87 %, patience  19 \n",
      "\n",
      "Epoch  15 , training err  8.254 %, val err  7.66 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  8.024 %, val err  7.53 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  7.82 %, val err  7.38 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  7.612 %, val err  7.24 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  7.454 %, val err  6.98 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.276 %, val err  6.79 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  7.078 %, val err  6.65 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  6.918 %, val err  6.46 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  6.8 %, val err  6.32 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  6.656 %, val err  6.1 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  6.562 %, val err  6.01 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.384 %, val err  5.91 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  6.316 %, val err  5.79 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  6.182 %, val err  5.66 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  6.056 %, val err  5.55 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  5.952 %, val err  5.45 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  5.864 %, val err  5.3 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  5.752 %, val err  5.25 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  5.654 %, val err  5.04 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  5.576 %, val err  4.97 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  5.49 %, val err  5.0 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  5.386 %, val err  4.86 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  5.32 %, val err  4.67 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  5.242 %, val err  4.68 %, patience  19 \n",
      "\n",
      "Epoch  39 , training err  5.192 %, val err  4.62 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  5.128 %, val err  4.5 %, patience  20 \n",
      "\n",
      "Epoch  41 , training err  5.046 %, val err  4.46 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  5.024 %, val err  4.43 %, patience  20 \n",
      "\n",
      "Epoch  43 , training err  4.96 %, val err  4.5 %, patience  19 \n",
      "\n",
      "Epoch  44 , training err  4.896 %, val err  4.39 %, patience  20 \n",
      "\n",
      "Epoch  45 , training err  4.844 %, val err  4.42 %, patience  19 \n",
      "\n",
      "Epoch  46 , training err  4.82 %, val err  4.28 %, patience  20 \n",
      "\n",
      "Epoch  47 , training err  4.762 %, val err  4.32 %, patience  19 \n",
      "\n",
      "Epoch  48 , training err  4.758 %, val err  4.29 %, patience  18 \n",
      "\n",
      "Epoch  49 , training err  4.698 %, val err  4.27 %, patience  20 \n",
      "\n",
      "Epoch  50 , training err  4.63 %, val err  4.25 %, patience  20 \n",
      "\n",
      "Epoch  51 , training err  4.564 %, val err  4.19 %, patience  20 \n",
      "\n",
      "Epoch  52 , training err  4.526 %, val err  4.15 %, patience  20 \n",
      "\n",
      "Epoch  53 , training err  4.516 %, val err  4.22 %, patience  19 \n",
      "\n",
      "Epoch  54 , training err  4.476 %, val err  4.23 %, patience  18 \n",
      "\n",
      "Epoch  55 , training err  4.454 %, val err  4.2 %, patience  17 \n",
      "\n",
      "Epoch  56 , training err  4.418 %, val err  4.17 %, patience  16 \n",
      "\n",
      "Epoch  57 , training err  4.372 %, val err  4.15 %, patience  15 \n",
      "\n",
      "Epoch  58 , training err  4.344 %, val err  4.14 %, patience  20 \n",
      "\n",
      "Epoch  59 , training err  4.352 %, val err  4.1 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  4.306 %, val err  4.08 %, patience  20 \n",
      "\n",
      "Epoch  61 , training err  4.282 %, val err  4.02 %, patience  20 \n",
      "\n",
      "Epoch  62 , training err  4.298 %, val err  4.05 %, patience  19 \n",
      "\n",
      "Epoch  63 , training err  4.252 %, val err  4.07 %, patience  18 \n",
      "\n",
      "Epoch  64 , training err  4.234 %, val err  4.08 %, patience  17 \n",
      "\n",
      "Epoch  65 , training err  4.164 %, val err  3.99 %, patience  20 \n",
      "\n",
      "Epoch  66 , training err  4.186 %, val err  4.13 %, patience  19 \n",
      "\n",
      "Epoch  67 , training err  4.132 %, val err  4.03 %, patience  18 \n",
      "\n",
      "Epoch  68 , training err  4.082 %, val err  4.01 %, patience  17 \n",
      "\n",
      "Epoch  69 , training err  4.058 %, val err  3.95 %, patience  20 \n",
      "\n",
      "Epoch  70 , training err  4.018 %, val err  4.11 %, patience  19 \n",
      "\n",
      "Epoch  71 , training err  3.966 %, val err  3.91 %, patience  20 \n",
      "\n",
      "Epoch  72 , training err  3.992 %, val err  3.92 %, patience  19 \n",
      "\n",
      "Epoch  73 , training err  3.906 %, val err  3.93 %, patience  18 \n",
      "\n",
      "Epoch  74 , training err  3.926 %, val err  3.92 %, patience  17 \n",
      "\n",
      "Epoch  75 , training err  3.906 %, val err  3.96 %, patience  16 \n",
      "\n",
      "Epoch  76 , training err  3.862 %, val err  3.85 %, patience  20 \n",
      "\n",
      "Epoch  77 , training err  3.872 %, val err  3.83 %, patience  20 \n",
      "\n",
      "Epoch  78 , training err  3.816 %, val err  3.88 %, patience  19 \n",
      "\n",
      "Epoch  79 , training err  3.76 %, val err  3.83 %, patience  18 \n",
      "\n",
      "Epoch  80 , training err  3.736 %, val err  3.89 %, patience  17 \n",
      "\n",
      "Epoch  81 , training err  3.712 %, val err  3.84 %, patience  16 \n",
      "\n",
      "Epoch  82 , training err  3.732 %, val err  3.86 %, patience  15 \n",
      "\n",
      "Epoch  83 , training err  3.718 %, val err  3.88 %, patience  14 \n",
      "\n",
      "Epoch  84 , training err  3.66 %, val err  3.78 %, patience  20 \n",
      "\n",
      "Epoch  85 , training err  3.69 %, val err  3.92 %, patience  19 \n",
      "\n",
      "Epoch  86 , training err  3.64 %, val err  3.84 %, patience  18 \n",
      "\n",
      "Epoch  87 , training err  3.716 %, val err  3.92 %, patience  17 \n",
      "\n",
      "Epoch  88 , training err  3.636 %, val err  3.89 %, patience  16 \n",
      "\n",
      "Epoch  89 , training err  3.61 %, val err  3.91 %, patience  15 \n",
      "\n",
      "Epoch  90 , training err  3.64 %, val err  3.8 %, patience  14 \n",
      "\n",
      "Epoch  91 , training err  3.578 %, val err  3.79 %, patience  13 \n",
      "\n",
      "Epoch  92 , training err  3.582 %, val err  3.83 %, patience  12 \n",
      "\n",
      "Epoch  93 , training err  3.536 %, val err  3.84 %, patience  11 \n",
      "\n",
      "Epoch  94 , training err  3.596 %, val err  3.79 %, patience  10 \n",
      "\n",
      "Epoch  95 , training err  3.446 %, val err  3.68 %, patience  20 \n",
      "\n",
      "Epoch  96 , training err  3.492 %, val err  3.62 %, patience  20 \n",
      "\n",
      "Epoch  97 , training err  3.454 %, val err  3.64 %, patience  19 \n",
      "\n",
      "Epoch  98 , training err  3.406 %, val err  3.62 %, patience  18 \n",
      "\n",
      "Epoch  99 , training err  3.404 %, val err  3.71 %, patience  17 \n",
      "\n",
      "Epoch  100 , training err  3.342 %, val err  3.61 %, patience  20 \n",
      "\n",
      "Epoch  101 , training err  3.378 %, val err  3.65 %, patience  19 \n",
      "\n",
      "Epoch  102 , training err  3.338 %, val err  3.69 %, patience  18 \n",
      "\n",
      "Epoch  103 , training err  3.32 %, val err  3.62 %, patience  17 \n",
      "\n",
      "Epoch  104 , training err  3.314 %, val err  3.63 %, patience  16 \n",
      "\n",
      "Epoch  105 , training err  3.32 %, val err  3.52 %, patience  20 \n",
      "\n",
      "Epoch  106 , training err  3.296 %, val err  3.68 %, patience  19 \n",
      "\n",
      "Epoch  107 , training err  3.232 %, val err  3.58 %, patience  18 \n",
      "\n",
      "Epoch  108 , training err  3.22 %, val err  3.52 %, patience  17 \n",
      "\n",
      "Epoch  109 , training err  3.306 %, val err  3.66 %, patience  16 \n",
      "\n",
      "Epoch  110 , training err  3.268 %, val err  3.51 %, patience  20 \n",
      "\n",
      "Epoch  111 , training err  3.266 %, val err  3.5 %, patience  20 \n",
      "\n",
      "Epoch  112 , training err  3.24 %, val err  3.52 %, patience  19 \n",
      "\n",
      "Epoch  113 , training err  3.216 %, val err  3.48 %, patience  20 \n",
      "\n",
      "Epoch  114 , training err  3.184 %, val err  3.46 %, patience  20 \n",
      "\n",
      "Epoch  115 , training err  3.226 %, val err  3.47 %, patience  19 \n",
      "\n",
      "Epoch  116 , training err  3.182 %, val err  3.53 %, patience  18 \n",
      "\n",
      "Epoch  117 , training err  3.204 %, val err  3.6 %, patience  17 \n",
      "\n",
      "Epoch  118 , training err  3.176 %, val err  3.6 %, patience  16 \n",
      "\n",
      "Epoch  119 , training err  3.182 %, val err  3.54 %, patience  15 \n",
      "\n",
      "Epoch  120 , training err  3.138 %, val err  3.58 %, patience  14 \n",
      "\n",
      "Epoch  121 , training err  3.174 %, val err  3.52 %, patience  13 \n",
      "\n",
      "Epoch  122 , training err  3.15 %, val err  3.55 %, patience  12 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  123 , training err  100.0 %, val err  3.48 %, patience  11 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:121: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  124 , training err  100.0 %, val err  100.0 %, patience  10 \n",
      "\n",
      "Epoch  125 , training err  100.0 %, val err  100.0 %, patience  9 \n",
      "\n",
      "Epoch  126 , training err  100.0 %, val err  100.0 %, patience  8 \n",
      "\n",
      "Epoch  127 , training err  100.0 %, val err  100.0 %, patience  7 \n",
      "\n",
      "Epoch  128 , training err  100.0 %, val err  100.0 %, patience  6 \n",
      "\n",
      "Epoch  129 , training err  100.0 %, val err  100.0 %, patience  5 \n",
      "\n",
      "Epoch  130 , training err  100.0 %, val err  100.0 %, patience  4 \n",
      "\n",
      "Epoch  131 , training err  100.0 %, val err  100.0 %, patience  3 \n",
      "\n",
      "Epoch  132 , training err  100.0 %, val err  100.0 %, patience  2 \n",
      "\n",
      "Epoch  133 , training err  100.0 %, val err  100.0 %, patience  1 \n",
      "\n",
      "Epoch  134 , training err  100.0 %, val err  100.0 %, patience  0 \n",
      "\n",
      "Best val err  3.46 % at epoch  114  corresponding train err  3.184 %\n",
      "Epoch  0 , training err  56.606 %, val err  55.12 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  28.202 %, val err  26.18 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  18.076 %, val err  15.91 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  14.616 %, val err  12.99 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  12.904 %, val err  11.56 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  11.742 %, val err  10.63 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  11.112 %, val err  9.97 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  10.64 %, val err  9.61 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.264 %, val err  9.27 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  9.856 %, val err  8.82 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  9.56 %, val err  8.56 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.254 %, val err  8.5 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  9.024 %, val err  8.22 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  8.722 %, val err  8.05 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  8.466 %, val err  7.77 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  8.224 %, val err  7.59 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  8.004 %, val err  7.48 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  7.75 %, val err  7.18 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  7.564 %, val err  7.14 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  7.294 %, val err  6.88 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.05 %, val err  6.64 %, patience  20 \n",
      "\n",
      "Epoch  21 , training err  6.93 %, val err  6.57 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  6.73 %, val err  6.32 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  6.596 %, val err  6.08 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  6.422 %, val err  5.96 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  6.27 %, val err  5.86 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.148 %, val err  5.74 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  5.96 %, val err  5.61 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  5.906 %, val err  5.49 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  5.8 %, val err  5.3 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  5.692 %, val err  5.28 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  5.584 %, val err  5.18 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  5.528 %, val err  5.03 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  5.438 %, val err  4.93 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  5.36 %, val err  4.89 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  5.256 %, val err  4.84 %, patience  20 \n",
      "\n",
      "Epoch  36 , training err  5.206 %, val err  4.82 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  5.132 %, val err  4.7 %, patience  20 \n",
      "\n",
      "Epoch  38 , training err  5.108 %, val err  4.57 %, patience  20 \n",
      "\n",
      "Epoch  39 , training err  5.032 %, val err  4.65 %, patience  19 \n",
      "\n",
      "Epoch  40 , training err  4.96 %, val err  4.53 %, patience  20 \n",
      "\n",
      "Epoch  41 , training err  4.916 %, val err  4.51 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  4.866 %, val err  4.46 %, patience  20 \n",
      "\n",
      "Epoch  43 , training err  4.788 %, val err  4.35 %, patience  20 \n",
      "\n",
      "Epoch  44 , training err  4.72 %, val err  4.31 %, patience  20 \n",
      "\n",
      "Epoch  45 , training err  4.652 %, val err  4.25 %, patience  20 \n",
      "\n",
      "Epoch  46 , training err  4.614 %, val err  4.23 %, patience  20 \n",
      "\n",
      "Epoch  47 , training err  4.53 %, val err  4.14 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  4.528 %, val err  4.11 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  4.456 %, val err  4.08 %, patience  20 \n",
      "\n",
      "Epoch  50 , training err  4.436 %, val err  4.02 %, patience  20 \n",
      "\n",
      "Epoch  51 , training err  4.366 %, val err  4.02 %, patience  19 \n",
      "\n",
      "Epoch  52 , training err  4.352 %, val err  4.01 %, patience  20 \n",
      "\n",
      "Epoch  53 , training err  4.3 %, val err  3.98 %, patience  20 \n",
      "\n",
      "Epoch  54 , training err  4.264 %, val err  3.97 %, patience  20 \n",
      "\n",
      "Epoch  55 , training err  4.208 %, val err  3.9 %, patience  20 \n",
      "\n",
      "Epoch  56 , training err  4.202 %, val err  3.89 %, patience  20 \n",
      "\n",
      "Epoch  57 , training err  4.108 %, val err  3.92 %, patience  19 \n",
      "\n",
      "Epoch  58 , training err  4.074 %, val err  3.92 %, patience  18 \n",
      "\n",
      "Epoch  59 , training err  4.07 %, val err  3.88 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  3.994 %, val err  3.85 %, patience  20 \n",
      "\n",
      "Epoch  61 , training err  3.956 %, val err  3.79 %, patience  20 \n",
      "\n",
      "Epoch  62 , training err  3.938 %, val err  3.77 %, patience  20 \n",
      "\n",
      "Epoch  63 , training err  3.968 %, val err  3.73 %, patience  20 \n",
      "\n",
      "Epoch  64 , training err  3.916 %, val err  3.7 %, patience  20 \n",
      "\n",
      "Epoch  65 , training err  3.846 %, val err  3.71 %, patience  19 \n",
      "\n",
      "Epoch  66 , training err  3.812 %, val err  3.68 %, patience  20 \n",
      "\n",
      "Epoch  67 , training err  3.762 %, val err  3.69 %, patience  19 \n",
      "\n",
      "Epoch  68 , training err  3.742 %, val err  3.66 %, patience  20 \n",
      "\n",
      "Epoch  69 , training err  3.712 %, val err  3.61 %, patience  20 \n",
      "\n",
      "Epoch  70 , training err  3.68 %, val err  3.65 %, patience  19 \n",
      "\n",
      "Epoch  71 , training err  3.662 %, val err  3.62 %, patience  18 \n",
      "\n",
      "Epoch  72 , training err  3.61 %, val err  3.59 %, patience  20 \n",
      "\n",
      "Epoch  73 , training err  3.578 %, val err  3.53 %, patience  20 \n",
      "\n",
      "Epoch  74 , training err  3.558 %, val err  3.49 %, patience  20 \n",
      "\n",
      "Epoch  75 , training err  3.552 %, val err  3.53 %, patience  19 \n",
      "\n",
      "Epoch  76 , training err  3.498 %, val err  3.52 %, patience  18 \n",
      "\n",
      "Epoch  77 , training err  3.498 %, val err  3.59 %, patience  17 \n",
      "\n",
      "Epoch  78 , training err  3.44 %, val err  3.53 %, patience  16 \n",
      "\n",
      "Epoch  79 , training err  3.426 %, val err  3.55 %, patience  15 \n",
      "\n",
      "Epoch  80 , training err  3.368 %, val err  3.47 %, patience  20 \n",
      "\n",
      "Epoch  81 , training err  3.328 %, val err  3.45 %, patience  20 \n",
      "\n",
      "Epoch  82 , training err  3.368 %, val err  3.48 %, patience  19 \n",
      "\n",
      "Epoch  83 , training err  3.296 %, val err  3.46 %, patience  18 \n",
      "\n",
      "Epoch  84 , training err  3.266 %, val err  3.42 %, patience  20 \n",
      "\n",
      "Epoch  85 , training err  3.24 %, val err  3.38 %, patience  20 \n",
      "\n",
      "Epoch  86 , training err  3.252 %, val err  3.38 %, patience  19 \n",
      "\n",
      "Epoch  87 , training err  3.238 %, val err  3.46 %, patience  18 \n",
      "\n",
      "Epoch  88 , training err  3.202 %, val err  3.4 %, patience  17 \n",
      "\n",
      "Epoch  89 , training err  3.188 %, val err  3.36 %, patience  20 \n",
      "\n",
      "Epoch  90 , training err  3.182 %, val err  3.4 %, patience  19 \n",
      "\n",
      "Epoch  91 , training err  3.212 %, val err  3.36 %, patience  18 \n",
      "\n",
      "Epoch  92 , training err  3.166 %, val err  3.44 %, patience  17 \n",
      "\n",
      "Epoch  93 , training err  3.13 %, val err  3.46 %, patience  16 \n",
      "\n",
      "Epoch  94 , training err  3.158 %, val err  3.39 %, patience  15 \n",
      "\n",
      "Epoch  95 , training err  3.216 %, val err  3.35 %, patience  20 \n",
      "\n",
      "Epoch  96 , training err  3.154 %, val err  3.42 %, patience  19 \n",
      "\n",
      "Epoch  97 , training err  3.158 %, val err  3.42 %, patience  18 \n",
      "\n",
      "Epoch  98 , training err  3.15 %, val err  3.4 %, patience  17 \n",
      "\n",
      "Epoch  99 , training err  3.118 %, val err  3.35 %, patience  16 \n",
      "\n",
      "Epoch  100 , training err  3.146 %, val err  3.36 %, patience  15 \n",
      "\n",
      "Epoch  101 , training err  3.11 %, val err  3.4 %, patience  14 \n",
      "\n",
      "Epoch  102 , training err  3.116 %, val err  3.31 %, patience  20 \n",
      "\n",
      "Epoch  103 , training err  3.116 %, val err  3.45 %, patience  19 \n",
      "\n",
      "Epoch  104 , training err  3.138 %, val err  3.41 %, patience  18 \n",
      "\n",
      "Epoch  105 , training err  3.102 %, val err  3.37 %, patience  17 \n",
      "\n",
      "Epoch  106 , training err  3.126 %, val err  3.39 %, patience  16 \n",
      "\n",
      "Epoch  107 , training err  3.106 %, val err  3.39 %, patience  15 \n",
      "\n",
      "Epoch  108 , training err  3.094 %, val err  3.36 %, patience  14 \n",
      "\n",
      "Epoch  109 , training err  3.09 %, val err  3.31 %, patience  13 \n",
      "\n",
      "Epoch  110 , training err  3.094 %, val err  3.38 %, patience  12 \n",
      "\n",
      "Epoch  111 , training err  3.084 %, val err  3.26 %, patience  20 \n",
      "\n",
      "Epoch  112 , training err  3.102 %, val err  3.25 %, patience  20 \n",
      "\n",
      "Epoch  113 , training err  3.088 %, val err  3.28 %, patience  19 \n",
      "\n",
      "Epoch  114 , training err  3.084 %, val err  3.25 %, patience  18 \n",
      "\n",
      "Epoch  115 , training err  3.078 %, val err  3.31 %, patience  17 \n",
      "\n",
      "Epoch  116 , training err  3.064 %, val err  3.27 %, patience  16 \n",
      "\n",
      "Epoch  117 , training err  3.058 %, val err  3.29 %, patience  15 \n",
      "\n",
      "Epoch  118 , training err  3.084 %, val err  3.34 %, patience  14 \n",
      "\n",
      "Epoch  119 , training err  3.028 %, val err  3.3 %, patience  13 \n",
      "\n",
      "Epoch  120 , training err  3.008 %, val err  3.4 %, patience  12 \n",
      "\n",
      "Epoch  121 , training err  3.04 %, val err  3.33 %, patience  11 \n",
      "\n",
      "Epoch  122 , training err  3.056 %, val err  3.31 %, patience  10 \n",
      "\n",
      "Epoch  123 , training err  3.022 %, val err  3.31 %, patience  9 \n",
      "\n",
      "Epoch  124 , training err  3.004 %, val err  3.41 %, patience  8 \n",
      "\n",
      "Epoch  125 , training err  3.01 %, val err  3.29 %, patience  7 \n",
      "\n",
      "Epoch  126 , training err  3.016 %, val err  3.37 %, patience  6 \n",
      "\n",
      "Epoch  127 , training err  2.962 %, val err  3.35 %, patience  5 \n",
      "\n",
      "Epoch  128 , training err  2.976 %, val err  3.33 %, patience  4 \n",
      "\n",
      "Epoch  129 , training err  2.974 %, val err  3.37 %, patience  3 \n",
      "\n",
      "Epoch  130 , training err  2.946 %, val err  3.29 %, patience  2 \n",
      "\n",
      "Epoch  131 , training err  2.93 %, val err  3.28 %, patience  1 \n",
      "\n",
      "Epoch  132 , training err  2.93 %, val err  3.28 %, patience  0 \n",
      "\n",
      "Best val err  3.25 % at epoch  112  corresponding train err  3.102 %\n",
      "Epoch  0 , training err  43.124 %, val err  41.24 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  25.408 %, val err  23.47 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  17.448 %, val err  15.46 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  14.114 %, val err  12.74 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  12.604 %, val err  11.33 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  11.532 %, val err  10.59 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  10.992 %, val err  9.96 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  10.502 %, val err  9.49 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  10.07 %, val err  9.17 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  9.734 %, val err  8.85 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  9.406 %, val err  8.56 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  9.152 %, val err  8.34 %, patience  20 \n",
      "\n",
      "Epoch  12 , training err  8.884 %, val err  8.11 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  8.606 %, val err  7.86 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  8.294 %, val err  7.69 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  8.012 %, val err  7.56 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  7.766 %, val err  7.23 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  7.62 %, val err  7.11 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  7.398 %, val err  6.92 %, patience  20 \n",
      "\n",
      "Epoch  19 , training err  7.242 %, val err  6.63 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  7.112 %, val err  6.64 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  6.928 %, val err  6.4 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  6.814 %, val err  6.31 %, patience  20 \n",
      "\n",
      "Epoch  23 , training err  6.704 %, val err  6.29 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  6.59 %, val err  6.19 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  6.46 %, val err  6.13 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  6.308 %, val err  6.01 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  6.202 %, val err  5.94 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  6.116 %, val err  5.84 %, patience  20 \n",
      "\n",
      "Epoch  29 , training err  6.022 %, val err  5.67 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  5.884 %, val err  5.7 %, patience  19 \n",
      "\n",
      "Epoch  31 , training err  5.78 %, val err  5.46 %, patience  20 \n",
      "\n",
      "Epoch  32 , training err  5.654 %, val err  5.36 %, patience  20 \n",
      "\n",
      "Epoch  33 , training err  5.612 %, val err  5.23 %, patience  20 \n",
      "\n",
      "Epoch  34 , training err  5.536 %, val err  5.23 %, patience  19 \n",
      "\n",
      "Epoch  35 , training err  5.45 %, val err  5.06 %, patience  20 \n",
      "\n",
      "Epoch  36 , training err  5.376 %, val err  4.84 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  5.288 %, val err  4.86 %, patience  19 \n",
      "\n",
      "Epoch  38 , training err  5.15 %, val err  4.76 %, patience  20 \n",
      "\n",
      "Epoch  39 , training err  5.1 %, val err  4.78 %, patience  19 \n",
      "\n",
      "Epoch  40 , training err  5.036 %, val err  4.75 %, patience  20 \n",
      "\n",
      "Epoch  41 , training err  4.998 %, val err  4.57 %, patience  20 \n",
      "\n",
      "Epoch  42 , training err  4.904 %, val err  4.59 %, patience  19 \n",
      "\n",
      "Epoch  43 , training err  4.828 %, val err  4.51 %, patience  20 \n",
      "\n",
      "Epoch  44 , training err  4.774 %, val err  4.5 %, patience  20 \n",
      "\n",
      "Epoch  45 , training err  4.706 %, val err  4.51 %, patience  19 \n",
      "\n",
      "Epoch  46 , training err  4.624 %, val err  4.43 %, patience  20 \n",
      "\n",
      "Epoch  47 , training err  4.622 %, val err  4.38 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  4.574 %, val err  4.32 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  4.51 %, val err  4.33 %, patience  19 \n",
      "\n",
      "Epoch  50 , training err  4.454 %, val err  4.21 %, patience  20 \n",
      "\n",
      "Epoch  51 , training err  4.398 %, val err  4.15 %, patience  20 \n",
      "\n",
      "Epoch  52 , training err  4.364 %, val err  4.06 %, patience  20 \n",
      "\n",
      "Epoch  53 , training err  4.304 %, val err  4.02 %, patience  20 \n",
      "\n",
      "Epoch  54 , training err  4.222 %, val err  4.02 %, patience  19 \n",
      "\n",
      "Epoch  55 , training err  4.192 %, val err  3.96 %, patience  20 \n",
      "\n",
      "Epoch  56 , training err  4.172 %, val err  3.99 %, patience  19 \n",
      "\n",
      "Epoch  57 , training err  4.136 %, val err  3.9 %, patience  20 \n",
      "\n",
      "Epoch  58 , training err  4.056 %, val err  3.89 %, patience  20 \n",
      "\n",
      "Epoch  59 , training err  4.038 %, val err  3.83 %, patience  20 \n",
      "\n",
      "Epoch  60 , training err  3.98 %, val err  3.87 %, patience  19 \n",
      "\n",
      "Epoch  61 , training err  3.954 %, val err  3.81 %, patience  20 \n",
      "\n",
      "Epoch  62 , training err  3.92 %, val err  3.76 %, patience  20 \n",
      "\n",
      "Epoch  63 , training err  3.838 %, val err  3.73 %, patience  20 \n",
      "\n",
      "Epoch  64 , training err  3.83 %, val err  3.72 %, patience  20 \n",
      "\n",
      "Epoch  65 , training err  3.778 %, val err  3.73 %, patience  19 \n",
      "\n",
      "Epoch  66 , training err  3.794 %, val err  3.65 %, patience  20 \n",
      "\n",
      "Epoch  67 , training err  3.74 %, val err  3.68 %, patience  19 \n",
      "\n",
      "Epoch  68 , training err  3.708 %, val err  3.58 %, patience  20 \n",
      "\n",
      "Epoch  69 , training err  3.72 %, val err  3.68 %, patience  19 \n",
      "\n",
      "Epoch  70 , training err  3.65 %, val err  3.57 %, patience  20 \n",
      "\n",
      "Epoch  71 , training err  3.594 %, val err  3.54 %, patience  20 \n",
      "\n",
      "Epoch  72 , training err  3.604 %, val err  3.58 %, patience  19 \n",
      "\n",
      "Epoch  73 , training err  3.542 %, val err  3.46 %, patience  20 \n",
      "\n",
      "Epoch  74 , training err  3.508 %, val err  3.54 %, patience  19 \n",
      "\n",
      "Epoch  75 , training err  3.474 %, val err  3.5 %, patience  18 \n",
      "\n",
      "Epoch  76 , training err  3.468 %, val err  3.44 %, patience  20 \n",
      "\n",
      "Epoch  77 , training err  3.454 %, val err  3.52 %, patience  19 \n",
      "\n",
      "Epoch  78 , training err  3.396 %, val err  3.45 %, patience  18 \n",
      "\n",
      "Epoch  79 , training err  3.382 %, val err  3.43 %, patience  20 \n",
      "\n",
      "Epoch  80 , training err  3.346 %, val err  3.38 %, patience  20 \n",
      "\n",
      "Epoch  81 , training err  3.308 %, val err  3.35 %, patience  20 \n",
      "\n",
      "Epoch  82 , training err  3.272 %, val err  3.32 %, patience  20 \n",
      "\n",
      "Epoch  83 , training err  3.304 %, val err  3.39 %, patience  19 \n",
      "\n",
      "Epoch  84 , training err  3.264 %, val err  3.36 %, patience  18 \n",
      "\n",
      "Epoch  85 , training err  3.256 %, val err  3.34 %, patience  17 \n",
      "\n",
      "Epoch  86 , training err  3.21 %, val err  3.32 %, patience  16 \n",
      "\n",
      "Epoch  87 , training err  3.202 %, val err  3.36 %, patience  15 \n",
      "\n",
      "Epoch  88 , training err  3.164 %, val err  3.36 %, patience  14 \n",
      "\n",
      "Epoch  89 , training err  3.136 %, val err  3.29 %, patience  20 \n",
      "\n",
      "Epoch  90 , training err  3.146 %, val err  3.25 %, patience  20 \n",
      "\n",
      "Epoch  91 , training err  3.11 %, val err  3.29 %, patience  19 \n",
      "\n",
      "Epoch  92 , training err  3.122 %, val err  3.28 %, patience  18 \n",
      "\n",
      "Epoch  93 , training err  3.09 %, val err  3.3 %, patience  17 \n",
      "\n",
      "Epoch  94 , training err  3.052 %, val err  3.27 %, patience  16 \n",
      "\n",
      "Epoch  95 , training err  3.028 %, val err  3.22 %, patience  20 \n",
      "\n",
      "Epoch  96 , training err  3.03 %, val err  3.16 %, patience  20 \n",
      "\n",
      "Epoch  97 , training err  2.99 %, val err  3.14 %, patience  20 \n",
      "\n",
      "Epoch  98 , training err  2.988 %, val err  3.18 %, patience  19 \n",
      "\n",
      "Epoch  99 , training err  2.984 %, val err  3.14 %, patience  18 \n",
      "\n",
      "Epoch  100 , training err  2.956 %, val err  3.08 %, patience  20 \n",
      "\n",
      "Epoch  101 , training err  2.978 %, val err  3.16 %, patience  19 \n",
      "\n",
      "Epoch  102 , training err  2.892 %, val err  3.13 %, patience  18 \n",
      "\n",
      "Epoch  103 , training err  2.91 %, val err  3.15 %, patience  17 \n",
      "\n",
      "Epoch  104 , training err  2.896 %, val err  3.13 %, patience  16 \n",
      "\n",
      "Epoch  105 , training err  2.894 %, val err  3.14 %, patience  15 \n",
      "\n",
      "Epoch  106 , training err  2.866 %, val err  3.18 %, patience  14 \n",
      "\n",
      "Epoch  107 , training err  2.83 %, val err  3.14 %, patience  13 \n",
      "\n",
      "Epoch  108 , training err  2.832 %, val err  3.15 %, patience  12 \n",
      "\n",
      "Epoch  109 , training err  2.812 %, val err  3.15 %, patience  11 \n",
      "\n",
      "Epoch  110 , training err  2.804 %, val err  3.04 %, patience  20 \n",
      "\n",
      "Epoch  111 , training err  2.818 %, val err  3.1 %, patience  19 \n",
      "\n",
      "Epoch  112 , training err  2.762 %, val err  3.14 %, patience  18 \n",
      "\n",
      "Epoch  113 , training err  2.748 %, val err  3.03 %, patience  20 \n",
      "\n",
      "Epoch  114 , training err  2.768 %, val err  3.09 %, patience  19 \n",
      "\n",
      "Epoch  115 , training err  2.74 %, val err  3.01 %, patience  20 \n",
      "\n",
      "Epoch  116 , training err  2.728 %, val err  3.02 %, patience  19 \n",
      "\n",
      "Epoch  117 , training err  2.746 %, val err  3.14 %, patience  18 \n",
      "\n",
      "Epoch  118 , training err  2.746 %, val err  3.04 %, patience  17 \n",
      "\n",
      "Epoch  119 , training err  2.69 %, val err  3.07 %, patience  16 \n",
      "\n",
      "Epoch  120 , training err  2.688 %, val err  3.13 %, patience  15 \n",
      "\n",
      "Epoch  121 , training err  2.636 %, val err  2.96 %, patience  20 \n",
      "\n",
      "Epoch  122 , training err  2.634 %, val err  3.02 %, patience  19 \n",
      "\n",
      "Epoch  123 , training err  2.624 %, val err  3.01 %, patience  18 \n",
      "\n",
      "Epoch  124 , training err  2.612 %, val err  2.92 %, patience  20 \n",
      "\n",
      "Epoch  125 , training err  2.634 %, val err  2.9 %, patience  20 \n",
      "\n",
      "Epoch  126 , training err  2.63 %, val err  3.01 %, patience  19 \n",
      "\n",
      "Epoch  127 , training err  2.57 %, val err  2.87 %, patience  20 \n",
      "\n",
      "Epoch  128 , training err  2.546 %, val err  2.97 %, patience  19 \n",
      "\n",
      "Epoch  129 , training err  2.538 %, val err  2.85 %, patience  20 \n",
      "\n",
      "Epoch  130 , training err  2.576 %, val err  2.93 %, patience  19 \n",
      "\n",
      "Epoch  131 , training err  2.564 %, val err  2.99 %, patience  18 \n",
      "\n",
      "Epoch  132 , training err  2.55 %, val err  2.97 %, patience  17 \n",
      "\n",
      "Epoch  133 , training err  2.514 %, val err  3.01 %, patience  16 \n",
      "\n",
      "Epoch  134 , training err  2.466 %, val err  2.9 %, patience  15 \n",
      "\n",
      "Epoch  135 , training err  2.47 %, val err  2.91 %, patience  14 \n",
      "\n",
      "Epoch  136 , training err  2.432 %, val err  2.83 %, patience  20 \n",
      "\n",
      "Epoch  137 , training err  2.468 %, val err  2.84 %, patience  19 \n",
      "\n",
      "Epoch  138 , training err  2.474 %, val err  2.93 %, patience  18 \n",
      "\n",
      "Epoch  139 , training err  2.424 %, val err  2.89 %, patience  17 \n",
      "\n",
      "Epoch  140 , training err  2.484 %, val err  2.95 %, patience  16 \n",
      "\n",
      "Epoch  141 , training err  2.412 %, val err  2.91 %, patience  15 \n",
      "\n",
      "Epoch  142 , training err  2.438 %, val err  2.92 %, patience  14 \n",
      "\n",
      "Epoch  143 , training err  2.426 %, val err  2.94 %, patience  13 \n",
      "\n",
      "Epoch  144 , training err  2.402 %, val err  2.88 %, patience  12 \n",
      "\n",
      "Epoch  145 , training err  2.382 %, val err  2.98 %, patience  11 \n",
      "\n",
      "Epoch  146 , training err  2.424 %, val err  3.0 %, patience  10 \n",
      "\n",
      "Epoch  147 , training err  2.412 %, val err  2.92 %, patience  9 \n",
      "\n",
      "Epoch  148 , training err  2.374 %, val err  2.84 %, patience  8 \n",
      "\n",
      "Epoch  149 , training err  2.356 %, val err  2.96 %, patience  7 \n",
      "\n",
      "Epoch  150 , training err  2.356 %, val err  2.96 %, patience  6 \n",
      "\n",
      "Epoch  151 , training err  2.358 %, val err  2.9 %, patience  5 \n",
      "\n",
      "Epoch  152 , training err  2.342 %, val err  2.86 %, patience  4 \n",
      "\n",
      "Epoch  153 , training err  2.382 %, val err  2.99 %, patience  3 \n",
      "\n",
      "Epoch  154 , training err  2.33 %, val err  2.9 %, patience  2 \n",
      "\n",
      "Epoch  155 , training err  2.32 %, val err  2.92 %, patience  1 \n",
      "\n",
      "Epoch  156 , training err  2.348 %, val err  2.89 %, patience  0 \n",
      "\n",
      "Best val err  2.83 % at epoch  136  corresponding train err  2.432 %\n",
      "Hidden layer  150 . Test error  3.73 %\n",
      "Hidden layer  200 . Test error  3.67 %\n",
      "Hidden layer  300 . Test error  3.09 %\n"
     ]
    }
   ],
   "source": [
    "middle = [150, 200, 300]\n",
    "for mid in middle:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,mid,10], 0.001, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, mid));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, mid in bestWs:\n",
    "    print  'Hidden layer ', mid,  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực nghiệm với mạng nơ-ron nhiều lớp. \n",
    "Thêm mô tả về mạng nơ-ron vào layer.\n",
    "Ví dụ: layer = [[784,10], [784,30,10]]\n",
    "- Huấn luyện 2 mạng nơ-ron với số lượng nút ở mỗi mạng là [784,10] và [784,30,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  90.282 %, val err  90.17 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  90.282 %, val err  90.17 %, patience  19 \n",
      "\n",
      "Epoch  2 , training err  90.282 %, val err  90.17 %, patience  18 \n",
      "\n",
      "Epoch  3 , training err  90.282 %, val err  90.17 %, patience  17 \n",
      "\n",
      "Epoch  4 , training err  88.644 %, val err  89.36 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  88.644 %, val err  89.36 %, patience  19 \n",
      "\n",
      "Epoch  6 , training err  88.644 %, val err  89.36 %, patience  18 \n",
      "\n",
      "Epoch  7 , training err  88.644 %, val err  89.36 %, patience  17 \n",
      "\n",
      "Epoch  8 , training err  88.644 %, val err  89.36 %, patience  16 \n",
      "\n",
      "Epoch  9 , training err  88.644 %, val err  89.36 %, patience  15 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-50ba84d6b4d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mll\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbWs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mbestWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-84190e7d6ecd>\u001b[0m in \u001b[0;36mtrain_neural_net\u001b[1;34m(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0meTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0meVal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mtrain_errs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-84190e7d6ecd>\u001b[0m in \u001b[0;36mmeanBinaryError\u001b[1;34m(X, W, Y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmeanBinaryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#for debugging: def forward_prop(X, Ws):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmyY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyY\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a64aa235bf09>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(X, Ws)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer = [[784, 50, 30, 20, 10], [784, 50, 30, 20, 20, 10]]\n",
    "for ll in layer:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, ll, 0.0001, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, ll));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, ll in bestWs:\n",
    "    print ll \n",
    "    print  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
