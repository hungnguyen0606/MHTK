{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTTH08: Regularized Neural Net\n",
    "\n",
    "TODO: Ghi họ tên và MSSV của bạn (vd, Nguyễn Văn A - 1234567)\n",
    "\n",
    "---\n",
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cách làm bài và nộp bài\n",
    "\n",
    "**Làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Lưu ý: tuyệt đối không gian lận. Nếu vi phạm thì bạn sẽ bị 0 điểm cho cả phần thực hành môn học. Nên nhớ mục tiêu chính ở đây là học kiến thức.*\n",
    "\n",
    "**Nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Cell` - `Run All` để chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Cell` - `Run All` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `Ex08-RegularizedNeuralNet.ipynb` (không cần nộp file dữ liệu `mnist.pkl.gz`); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hàm đọc dữ liệu\n",
    "\n",
    "Trong bài này, bạn sẽ thử nghiệm Neural Net trên bộ dữ liệu MNIST (file `mnist.pkl.gz` đính kèm). Đây là bộ dữ liệu gồm các ảnh chữ số viết tay từ 0-9 (10 lớp); mỗi ảnh có kích thước $28\\times 28$ và là ảnh grayscale. Bộ dữ liệu đã được chia sẵn làm 3 tập: tập huấn luyện gồm 50000 ảnh, tập validation gồm 10000 ảnh, và tập kiểm tra gồm 10000 ảnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hàm lan truyền tiến qua Neural Net\n",
    "\n",
    "Trong bài này, ta sẽ sử dụng nơ-ron sigmoid ở các tẩng ẩn, và tầng softmax là tầng xuất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Computes tanh function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    ez = np.exp(Z)\n",
    "    esz = np.exp(-Z)\n",
    "    return (ez-esz)/(ez + esz)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = tanh(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hàm huấn luyện Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            #Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = tanh(Z);\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "                    \n",
    "                #Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            \n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                delta = delta.dot(Ws[-it+1].T)*(1-As[-it]*As[-it])\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "        \n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thí nghiệm\n",
    "\n",
    "Để thấy được ảnh hưởng của hệ số `l2_reg_level`, bạn sẽ dùng các hàm đã định nghĩa ở trên như sau:\n",
    "\n",
    "1. Đọc dữ liệu.\n",
    "2. Huấn luyện Neural Net trên tập huấn luyện với `layer_sizes = [784, 30, 10]`, `learning_rate = 0.1`, `mnb_size = 10`, `max_patience = 20`, và `l2_reg_level = 0, 0.0001, 0.001`. Để dễ nhìn khi chương trình `print` ra, bạn nên dùng 3 code cell cho 3 lần gọi hàm huấn luyện (ứng với 3 giá trị của `l2_reg_level`).\n",
    "3. Ở cell kế tiếp, bạn sẽ vẽ ra đồ trị có trục hoàng là số lượng epoch và trục tung là độ lỗi. Với mỗi giá trị của `l2_reg_level`, bạn sẽ vẽ ra 2 đường ứng với độ lỗi MBE trên tập huấn luyện và tập validation; như vậy, trên đồ thị sẽ có tất cả 6 đường.\n",
    "4. Cho nhận xét dựa vào đồ thị kết quả.\n",
    "5. Cuối cùng, bạn sẽ tính và in ra độ lỗi trên tập kiểm tra của mô hình có độ lỗi nhỏ nhất trên tập validation (trong số 3 mô hình ứng với 3 giá trị của `l2_reg_level`).\n",
    "\n",
    "(Kết quả chạy của mình: với `l2_reg_level = 0`, độ lỗi trên tập huấn luyện và tập validation lần lượt là 1.076% và 3.480%; với `l2_reg_level = 0.0001`, độ lỗi là 2.114% và 2.910%; với `l2_reg_level = 0.001`, độ lỗi là 6.940% và 6.130%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  5.486 %, val err  5.26 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.102 %, val err  3.54 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.108 %, val err  2.92 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  1.748 %, val err  2.65 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  1.706 %, val err  2.69 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  1.784 %, val err  2.99 %, patience  18 \n",
      "\n",
      "Epoch  6 , training err  0.896 %, val err  2.08 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  0.804 %, val err  2.33 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  0.8 %, val err  2.38 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  1.05 %, val err  2.6 %, patience  17 \n",
      "\n",
      "Epoch  10 , training err  0.65 %, val err  2.21 %, patience  16 \n",
      "\n",
      "Epoch  11 , training err  0.66 %, val err  2.14 %, patience  15 \n",
      "\n",
      "Epoch  12 , training err  0.866 %, val err  2.41 %, patience  14 \n",
      "\n",
      "Epoch  13 , training err  0.596 %, val err  2.12 %, patience  13 \n",
      "\n",
      "Epoch  14 , training err  0.818 %, val err  2.45 %, patience  12 \n",
      "\n",
      "Epoch  15 , training err  0.528 %, val err  2.12 %, patience  11 \n",
      "\n",
      "Epoch  16 , training err  0.436 %, val err  2.13 %, patience  10 \n",
      "\n",
      "Epoch  17 , training err  2.37 %, val err  3.91 %, patience  9 \n",
      "\n",
      "Epoch  18 , training err  0.486 %, val err  2.16 %, patience  8 \n",
      "\n",
      "Epoch  19 , training err  0.346 %, val err  2.0 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  0.614 %, val err  2.51 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  0.474 %, val err  2.22 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  0.514 %, val err  2.15 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  0.438 %, val err  2.03 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  0.45 %, val err  2.13 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  0.39 %, val err  2.07 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  0.378 %, val err  2.15 %, patience  13 \n",
      "\n",
      "Epoch  27 , training err  0.274 %, val err  2.0 %, patience  12 \n",
      "\n",
      "Epoch  28 , training err  0.39 %, val err  2.28 %, patience  11 \n",
      "\n",
      "Epoch  29 , training err  0.348 %, val err  2.25 %, patience  10 \n",
      "\n",
      "Epoch  30 , training err  1.526 %, val err  3.16 %, patience  9 \n",
      "\n",
      "Epoch  31 , training err  0.418 %, val err  2.2 %, patience  8 \n",
      "\n",
      "Epoch  32 , training err  0.394 %, val err  2.13 %, patience  7 \n",
      "\n",
      "Epoch  33 , training err  0.342 %, val err  2.19 %, patience  6 \n",
      "\n",
      "Epoch  34 , training err  0.38 %, val err  2.2 %, patience  5 \n",
      "\n",
      "Epoch  35 , training err  0.482 %, val err  2.2 %, patience  4 \n",
      "\n",
      "Epoch  36 , training err  0.394 %, val err  1.93 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  0.346 %, val err  2.04 %, patience  19 \n",
      "\n",
      "Epoch  38 , training err  0.246 %, val err  1.93 %, patience  18 \n",
      "\n",
      "Epoch  39 , training err  0.216 %, val err  1.9 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  0.58 %, val err  2.36 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  0.376 %, val err  2.16 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  0.514 %, val err  2.19 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  0.366 %, val err  2.09 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  0.248 %, val err  1.97 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  0.316 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  46 , training err  0.232 %, val err  1.97 %, patience  13 \n",
      "\n",
      "Epoch  47 , training err  0.274 %, val err  1.82 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  0.358 %, val err  2.04 %, patience  19 \n",
      "\n",
      "Epoch  49 , training err  0.272 %, val err  2.05 %, patience  18 \n",
      "\n",
      "Epoch  50 , training err  0.224 %, val err  1.82 %, patience  17 \n",
      "\n",
      "Epoch  51 , training err  0.262 %, val err  1.95 %, patience  16 \n",
      "\n",
      "Epoch  52 , training err  0.268 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  53 , training err  0.38 %, val err  2.11 %, patience  14 \n",
      "\n",
      "Epoch  54 , training err  0.346 %, val err  2.03 %, patience  13 \n",
      "\n",
      "Epoch  55 , training err  0.306 %, val err  2.13 %, patience  12 \n",
      "\n",
      "Epoch  56 , training err  0.28 %, val err  1.97 %, patience  11 \n",
      "\n",
      "Epoch  57 , training err  0.21 %, val err  1.87 %, patience  10 \n",
      "\n",
      "Epoch  58 , training err  0.488 %, val err  2.38 %, patience  9 \n",
      "\n",
      "Epoch  59 , training err  0.228 %, val err  1.92 %, patience  8 \n",
      "\n",
      "Epoch  60 , training err  0.672 %, val err  2.24 %, patience  7 \n",
      "\n",
      "Epoch  61 , training err  0.372 %, val err  2.03 %, patience  6 \n",
      "\n",
      "Epoch  62 , training err  0.594 %, val err  2.29 %, patience  5 \n",
      "\n",
      "Epoch  63 , training err  0.294 %, val err  1.99 %, patience  4 \n",
      "\n",
      "Epoch  64 , training err  0.674 %, val err  2.14 %, patience  3 \n",
      "\n",
      "Epoch  65 , training err  0.5 %, val err  1.92 %, patience  2 \n",
      "\n",
      "Epoch  66 , training err  0.28 %, val err  1.88 %, patience  1 \n",
      "\n",
      "Epoch  67 , training err  0.17 %, val err  1.83 %, patience  0 \n",
      "\n",
      "Best val err  1.82 % at epoch  47  corresponding train err  0.274 %\n",
      "Epoch  0 , training err  4.904 %, val err  4.9 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.056 %, val err  3.49 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.63 %, val err  3.38 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  1.414 %, val err  2.42 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  1.69 %, val err  2.76 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  1.176 %, val err  2.43 %, patience  18 \n",
      "\n",
      "Epoch  6 , training err  1.86 %, val err  3.27 %, patience  17 \n",
      "\n",
      "Epoch  7 , training err  0.726 %, val err  2.39 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  0.848 %, val err  2.36 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  0.554 %, val err  2.16 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  0.608 %, val err  2.31 %, patience  19 \n",
      "\n",
      "Epoch  11 , training err  0.626 %, val err  2.24 %, patience  18 \n",
      "\n",
      "Epoch  12 , training err  0.5 %, val err  2.09 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  0.658 %, val err  2.32 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  0.64 %, val err  2.5 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  0.554 %, val err  2.16 %, patience  17 \n",
      "\n",
      "Epoch  16 , training err  0.44 %, val err  2.05 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  0.59 %, val err  2.22 %, patience  19 \n",
      "\n",
      "Epoch  18 , training err  0.446 %, val err  2.12 %, patience  18 \n",
      "\n",
      "Epoch  19 , training err  0.458 %, val err  1.91 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  0.608 %, val err  2.28 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  0.36 %, val err  2.23 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  0.536 %, val err  2.31 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  0.54 %, val err  2.34 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  0.54 %, val err  2.42 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  0.296 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  0.394 %, val err  2.04 %, patience  13 \n",
      "\n",
      "Epoch  27 , training err  0.546 %, val err  2.44 %, patience  12 \n",
      "\n",
      "Epoch  28 , training err  0.542 %, val err  2.4 %, patience  11 \n",
      "\n",
      "Epoch  29 , training err  0.73 %, val err  2.38 %, patience  10 \n",
      "\n",
      "Epoch  30 , training err  0.218 %, val err  1.98 %, patience  9 \n",
      "\n",
      "Epoch  31 , training err  0.376 %, val err  2.09 %, patience  8 \n",
      "\n",
      "Epoch  32 , training err  0.398 %, val err  2.19 %, patience  7 \n",
      "\n",
      "Epoch  33 , training err  0.3 %, val err  2.17 %, patience  6 \n",
      "\n",
      "Epoch  34 , training err  0.204 %, val err  1.84 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  0.356 %, val err  2.0 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  0.312 %, val err  1.98 %, patience  18 \n",
      "\n",
      "Epoch  37 , training err  0.224 %, val err  1.95 %, patience  17 \n",
      "\n",
      "Epoch  38 , training err  0.566 %, val err  2.28 %, patience  16 \n",
      "\n",
      "Epoch  39 , training err  0.388 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  40 , training err  0.642 %, val err  2.52 %, patience  14 \n",
      "\n",
      "Epoch  41 , training err  0.554 %, val err  2.09 %, patience  13 \n",
      "\n",
      "Epoch  42 , training err  0.44 %, val err  2.0 %, patience  12 \n",
      "\n",
      "Epoch  43 , training err  0.306 %, val err  2.0 %, patience  11 \n",
      "\n",
      "Epoch  44 , training err  0.358 %, val err  2.13 %, patience  10 \n",
      "\n",
      "Epoch  45 , training err  0.34 %, val err  1.96 %, patience  9 \n",
      "\n",
      "Epoch  46 , training err  0.28 %, val err  2.0 %, patience  8 \n",
      "\n",
      "Epoch  47 , training err  0.352 %, val err  1.97 %, patience  7 \n",
      "\n",
      "Epoch  48 , training err  0.242 %, val err  1.93 %, patience  6 \n",
      "\n",
      "Epoch  49 , training err  0.392 %, val err  2.04 %, patience  5 \n",
      "\n",
      "Epoch  50 , training err  0.264 %, val err  2.24 %, patience  4 \n",
      "\n",
      "Epoch  51 , training err  0.432 %, val err  2.27 %, patience  3 \n",
      "\n",
      "Epoch  52 , training err  0.27 %, val err  1.94 %, patience  2 \n",
      "\n",
      "Epoch  53 , training err  0.394 %, val err  2.12 %, patience  1 \n",
      "\n",
      "Epoch  54 , training err  0.214 %, val err  1.8 %, patience  20 \n",
      "\n",
      "Epoch  55 , training err  0.352 %, val err  2.02 %, patience  19 \n",
      "\n",
      "Epoch  56 , training err  0.222 %, val err  1.94 %, patience  18 \n",
      "\n",
      "Epoch  57 , training err  0.252 %, val err  1.96 %, patience  17 \n",
      "\n",
      "Epoch  58 , training err  0.484 %, val err  2.23 %, patience  16 \n",
      "\n",
      "Epoch  59 , training err  0.228 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  60 , training err  0.87 %, val err  2.58 %, patience  14 \n",
      "\n",
      "Epoch  61 , training err  0.328 %, val err  1.99 %, patience  13 \n",
      "\n",
      "Epoch  62 , training err  0.216 %, val err  1.82 %, patience  12 \n",
      "\n",
      "Epoch  63 , training err  0.314 %, val err  1.98 %, patience  11 \n",
      "\n",
      "Epoch  64 , training err  0.41 %, val err  2.11 %, patience  10 \n",
      "\n",
      "Epoch  65 , training err  0.422 %, val err  2.16 %, patience  9 \n",
      "\n",
      "Epoch  66 , training err  0.3 %, val err  2.03 %, patience  8 \n",
      "\n",
      "Epoch  67 , training err  0.334 %, val err  2.01 %, patience  7 \n",
      "\n",
      "Epoch  68 , training err  0.276 %, val err  1.97 %, patience  6 \n",
      "\n",
      "Epoch  69 , training err  0.164 %, val err  1.78 %, patience  20 \n",
      "\n",
      "Epoch  70 , training err  0.322 %, val err  2.18 %, patience  19 \n",
      "\n",
      "Epoch  71 , training err  0.47 %, val err  2.1 %, patience  18 \n",
      "\n",
      "Epoch  72 , training err  0.284 %, val err  2.01 %, patience  17 \n",
      "\n",
      "Epoch  73 , training err  0.368 %, val err  1.99 %, patience  16 \n",
      "\n",
      "Epoch  74 , training err  0.338 %, val err  1.9 %, patience  15 \n",
      "\n",
      "Epoch  75 , training err  0.25 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  76 , training err  0.266 %, val err  1.87 %, patience  13 \n",
      "\n",
      "Epoch  77 , training err  0.242 %, val err  1.94 %, patience  12 \n",
      "\n",
      "Epoch  78 , training err  0.812 %, val err  2.45 %, patience  11 \n",
      "\n",
      "Epoch  79 , training err  0.522 %, val err  2.19 %, patience  10 \n",
      "\n",
      "Epoch  80 , training err  0.276 %, val err  1.98 %, patience  9 \n",
      "\n",
      "Epoch  81 , training err  0.194 %, val err  1.8 %, patience  8 \n",
      "\n",
      "Epoch  82 , training err  0.248 %, val err  2.09 %, patience  7 \n",
      "\n",
      "Epoch  83 , training err  0.408 %, val err  2.0 %, patience  6 \n",
      "\n",
      "Epoch  84 , training err  0.358 %, val err  2.0 %, patience  5 \n",
      "\n",
      "Epoch  85 , training err  0.41 %, val err  2.07 %, patience  4 \n",
      "\n",
      "Epoch  86 , training err  0.394 %, val err  2.35 %, patience  3 \n",
      "\n",
      "Epoch  87 , training err  0.178 %, val err  1.8 %, patience  2 \n",
      "\n",
      "Epoch  88 , training err  0.224 %, val err  1.95 %, patience  1 \n",
      "\n",
      "Epoch  89 , training err  0.134 %, val err  1.83 %, patience  0 \n",
      "\n",
      "Best val err  1.78 % at epoch  69  corresponding train err  0.164 %\n",
      "Epoch  0 , training err  5.44 %, val err  5.22 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  2.676 %, val err  3.18 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.658 %, val err  3.22 %, patience  19 \n",
      "\n",
      "Epoch  3 , training err  2.81 %, val err  3.62 %, patience  18 \n",
      "\n",
      "Epoch  4 , training err  1.42 %, val err  2.56 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  0.948 %, val err  2.24 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  0.804 %, val err  2.3 %, patience  19 \n",
      "\n",
      "Epoch  7 , training err  0.778 %, val err  2.23 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  0.704 %, val err  2.2 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  0.698 %, val err  2.35 %, patience  19 \n",
      "\n",
      "Epoch  10 , training err  1.048 %, val err  2.64 %, patience  18 \n",
      "\n",
      "Epoch  11 , training err  1.016 %, val err  2.43 %, patience  17 \n",
      "\n",
      "Epoch  12 , training err  0.424 %, val err  2.11 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  0.67 %, val err  2.28 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  0.432 %, val err  2.16 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  0.53 %, val err  2.1 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  0.608 %, val err  2.22 %, patience  19 \n",
      "\n",
      "Epoch  17 , training err  0.534 %, val err  2.11 %, patience  18 \n",
      "\n",
      "Epoch  18 , training err  0.496 %, val err  2.22 %, patience  17 \n",
      "\n",
      "Epoch  19 , training err  0.46 %, val err  2.15 %, patience  16 \n",
      "\n",
      "Epoch  20 , training err  0.598 %, val err  2.29 %, patience  15 \n",
      "\n",
      "Epoch  21 , training err  0.738 %, val err  2.41 %, patience  14 \n",
      "\n",
      "Epoch  22 , training err  0.446 %, val err  2.38 %, patience  13 \n",
      "\n",
      "Epoch  23 , training err  0.486 %, val err  2.08 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  0.63 %, val err  2.32 %, patience  19 \n",
      "\n",
      "Epoch  25 , training err  0.496 %, val err  2.12 %, patience  18 \n",
      "\n",
      "Epoch  26 , training err  0.814 %, val err  2.28 %, patience  17 \n",
      "\n",
      "Epoch  27 , training err  0.406 %, val err  2.02 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  0.44 %, val err  2.3 %, patience  19 \n",
      "\n",
      "Epoch  29 , training err  0.658 %, val err  2.32 %, patience  18 \n",
      "\n",
      "Epoch  30 , training err  0.522 %, val err  2.18 %, patience  17 \n",
      "\n",
      "Epoch  31 , training err  0.696 %, val err  2.24 %, patience  16 \n",
      "\n",
      "Epoch  32 , training err  0.494 %, val err  2.17 %, patience  15 \n",
      "\n",
      "Epoch  33 , training err  0.742 %, val err  2.36 %, patience  14 \n",
      "\n",
      "Epoch  34 , training err  0.54 %, val err  2.34 %, patience  13 \n",
      "\n",
      "Epoch  35 , training err  0.67 %, val err  2.6 %, patience  12 \n",
      "\n",
      "Epoch  36 , training err  0.408 %, val err  2.19 %, patience  11 \n",
      "\n",
      "Epoch  37 , training err  0.462 %, val err  2.06 %, patience  10 \n",
      "\n",
      "Epoch  38 , training err  0.506 %, val err  2.33 %, patience  9 \n",
      "\n",
      "Epoch  39 , training err  0.354 %, val err  2.06 %, patience  8 \n",
      "\n",
      "Epoch  40 , training err  0.374 %, val err  2.13 %, patience  7 \n",
      "\n",
      "Epoch  41 , training err  0.972 %, val err  2.65 %, patience  6 \n",
      "\n",
      "Epoch  42 , training err  0.348 %, val err  2.28 %, patience  5 \n",
      "\n",
      "Epoch  43 , training err  0.648 %, val err  2.43 %, patience  4 \n",
      "\n",
      "Epoch  44 , training err  0.402 %, val err  2.09 %, patience  3 \n",
      "\n",
      "Epoch  45 , training err  0.56 %, val err  2.17 %, patience  2 \n",
      "\n",
      "Epoch  46 , training err  0.532 %, val err  2.18 %, patience  1 \n",
      "\n",
      "Epoch  47 , training err  0.524 %, val err  2.11 %, patience  0 \n",
      "\n",
      "Best val err  2.02 % at epoch  27  corresponding train err  0.406 %\n",
      "Hidden layer  150 . Test error  1.85 %\n",
      "Hidden layer  200 . Test error  1.8 %\n",
      "Hidden layer  300 . Test error  2.09 %\n"
     ]
    }
   ],
   "source": [
    "middle = [150, 200, 300]\n",
    "for mid in middle:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,mid,10], 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, mid));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, mid in bestWs:\n",
    "    print  'Hidden layer ', mid,  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ta nhận thấy độ lỗi trên tập train và validation ứng với l2_reg_level = 0.001 đều cao, và cao hơn so với các giá trị l2 khác. Điều này cho thấy mô hình này bị underfiting do giá trị l2 khá cao dẫn đến việc các giá trị Ws tương ứng không được \"dao động thoải mái\" và có xu hướng gần giá trị 0 để tối ưu hàm chi phí J do đó khó đạt được tới mô hình tối ưu.\n",
    "- Ngược lại với giá trị l2 = 0, thì mô hình này có độ lỗi trên tập train xuống rất thấp nhưng đồng thời độ lỗi trên tập validation ban đầu có xu hướng xuống nhưng sau lại tăng lên. Điều này cho thấy mô hình này bị overfiting do giá trị l2 quá thấp dẫn tới việc các giá trị Ws 'thoải mái di chuyển' và cố gắng fit tập train (gồm cả nhiễu) dẫn tới việc mô hình không được tổng quát.\n",
    "- Còn với giá trị l2 = 0.001 thì ta thấy độ lỗi trên tập train và tập validation đều có xu hướng đi xuống và có giá trị thấp. Mô hình này tương đối tốt nếu so với 2 mô hình còn lại do giá trị l2 được chọn khá cân bằng (không quá cao hay thấp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  4.444 %, val err  4.57 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.544 %, val err  3.87 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  3.066 %, val err  3.79 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  2.642 %, val err  3.54 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  2.272 %, val err  3.71 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  1.956 %, val err  3.24 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  2.208 %, val err  3.46 %, patience  19 \n",
      "\n",
      "Epoch  7 , training err  1.808 %, val err  2.98 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  1.596 %, val err  3.19 %, patience  19 \n",
      "\n",
      "Epoch  9 , training err  1.772 %, val err  3.46 %, patience  18 \n",
      "\n",
      "Epoch  10 , training err  1.872 %, val err  3.4 %, patience  17 \n",
      "\n",
      "Epoch  11 , training err  1.6 %, val err  3.12 %, patience  16 \n",
      "\n",
      "Epoch  12 , training err  1.35 %, val err  2.88 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  1.57 %, val err  3.3 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  1.186 %, val err  2.81 %, patience  20 \n",
      "\n",
      "Epoch  15 , training err  1.686 %, val err  3.39 %, patience  19 \n",
      "\n",
      "Epoch  16 , training err  1.196 %, val err  2.78 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  1.426 %, val err  3.14 %, patience  19 \n",
      "\n",
      "Epoch  18 , training err  1.622 %, val err  3.21 %, patience  18 \n",
      "\n",
      "Epoch  19 , training err  1.358 %, val err  2.96 %, patience  17 \n",
      "\n",
      "Epoch  20 , training err  1.27 %, val err  3.08 %, patience  16 \n",
      "\n",
      "Epoch  21 , training err  1.318 %, val err  2.8 %, patience  15 \n",
      "\n",
      "Epoch  22 , training err  1.938 %, val err  3.79 %, patience  14 \n",
      "\n",
      "Epoch  23 , training err  1.716 %, val err  3.15 %, patience  13 \n",
      "\n",
      "Epoch  24 , training err  1.0 %, val err  2.71 %, patience  20 \n",
      "\n",
      "Epoch  25 , training err  1.3 %, val err  3.08 %, patience  19 \n",
      "\n",
      "Epoch  26 , training err  1.244 %, val err  2.99 %, patience  18 \n",
      "\n",
      "Epoch  27 , training err  1.226 %, val err  2.96 %, patience  17 \n",
      "\n",
      "Epoch  28 , training err  1.722 %, val err  3.61 %, patience  16 \n",
      "\n",
      "Epoch  29 , training err  1.574 %, val err  3.45 %, patience  15 \n",
      "\n",
      "Epoch  30 , training err  1.26 %, val err  3.05 %, patience  14 \n",
      "\n",
      "Epoch  31 , training err  1.418 %, val err  3.1 %, patience  13 \n",
      "\n",
      "Epoch  32 , training err  1.22 %, val err  2.85 %, patience  12 \n",
      "\n",
      "Epoch  33 , training err  1.528 %, val err  3.29 %, patience  11 \n",
      "\n",
      "Epoch  34 , training err  1.124 %, val err  2.86 %, patience  10 \n",
      "\n",
      "Epoch  35 , training err  1.256 %, val err  2.86 %, patience  9 \n",
      "\n",
      "Epoch  36 , training err  1.384 %, val err  2.98 %, patience  8 \n",
      "\n",
      "Epoch  37 , training err  1.256 %, val err  2.99 %, patience  7 \n",
      "\n",
      "Epoch  38 , training err  1.13 %, val err  2.91 %, patience  6 \n",
      "\n",
      "Epoch  39 , training err  1.162 %, val err  2.78 %, patience  5 \n",
      "\n",
      "Epoch  40 , training err  1.488 %, val err  2.89 %, patience  4 \n",
      "\n",
      "Epoch  41 , training err  1.018 %, val err  2.85 %, patience  3 \n",
      "\n",
      "Epoch  42 , training err  0.812 %, val err  2.63 %, patience  20 \n",
      "\n",
      "Epoch  43 , training err  1.494 %, val err  3.26 %, patience  19 \n",
      "\n",
      "Epoch  44 , training err  1.008 %, val err  2.87 %, patience  18 \n",
      "\n",
      "Epoch  45 , training err  1.472 %, val err  3.29 %, patience  17 \n",
      "\n",
      "Epoch  46 , training err  1.476 %, val err  3.24 %, patience  16 \n",
      "\n",
      "Epoch  47 , training err  1.14 %, val err  2.59 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  1.35 %, val err  3.25 %, patience  19 \n",
      "\n",
      "Epoch  49 , training err  0.984 %, val err  2.84 %, patience  18 \n",
      "\n",
      "Epoch  50 , training err  1.142 %, val err  2.98 %, patience  17 \n",
      "\n",
      "Epoch  51 , training err  1.158 %, val err  3.07 %, patience  16 \n",
      "\n",
      "Epoch  52 , training err  1.096 %, val err  3.1 %, patience  15 \n",
      "\n",
      "Epoch  53 , training err  1.606 %, val err  3.34 %, patience  14 \n",
      "\n",
      "Epoch  54 , training err  1.428 %, val err  3.17 %, patience  13 \n",
      "\n",
      "Epoch  55 , training err  1.092 %, val err  2.68 %, patience  12 \n",
      "\n",
      "Epoch  56 , training err  0.95 %, val err  2.95 %, patience  11 \n",
      "\n",
      "Epoch  57 , training err  1.154 %, val err  2.88 %, patience  10 \n",
      "\n",
      "Epoch  58 , training err  0.968 %, val err  2.93 %, patience  9 \n",
      "\n",
      "Epoch  59 , training err  1.04 %, val err  3.05 %, patience  8 \n",
      "\n",
      "Epoch  60 , training err  1.13 %, val err  2.91 %, patience  7 \n",
      "\n",
      "Epoch  61 , training err  1.12 %, val err  3.02 %, patience  6 \n",
      "\n",
      "Epoch  62 , training err  1.118 %, val err  2.9 %, patience  5 \n",
      "\n",
      "Epoch  63 , training err  1.356 %, val err  3.1 %, patience  4 \n",
      "\n",
      "Epoch  64 , training err  1.198 %, val err  3.09 %, patience  3 \n",
      "\n",
      "Epoch  65 , training err  0.866 %, val err  2.75 %, patience  2 \n",
      "\n",
      "Epoch  66 , training err  0.742 %, val err  2.62 %, patience  1 \n",
      "\n",
      "Epoch  67 , training err  1.506 %, val err  3.1 %, patience  0 \n",
      "\n",
      "Best val err  2.59 % at epoch  47  corresponding train err  1.14 %\n",
      "Epoch  0 , training err  6.136 %, val err  5.79 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.83 %, val err  4.04 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  3.166 %, val err  3.64 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  2.66 %, val err  3.51 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  2.698 %, val err  3.65 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  2.572 %, val err  3.49 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  1.878 %, val err  2.92 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  1.676 %, val err  3.09 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  2.532 %, val err  3.79 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  2.998 %, val err  4.25 %, patience  17 \n",
      "\n",
      "Epoch  10 , training err  2.0 %, val err  3.05 %, patience  16 \n",
      "\n",
      "Epoch  11 , training err  1.692 %, val err  3.31 %, patience  15 \n",
      "\n",
      "Epoch  12 , training err  3.118 %, val err  4.65 %, patience  14 \n",
      "\n",
      "Epoch  13 , training err  1.63 %, val err  2.88 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  4.1 %, val err  4.93 %, patience  19 \n",
      "\n",
      "Epoch  15 , training err  1.484 %, val err  3.14 %, patience  18 \n",
      "\n",
      "Epoch  16 , training err  1.432 %, val err  2.98 %, patience  17 \n",
      "\n",
      "Epoch  17 , training err  2.602 %, val err  4.17 %, patience  16 \n",
      "\n",
      "Epoch  18 , training err  1.516 %, val err  3.08 %, patience  15 \n",
      "\n",
      "Epoch  19 , training err  2.3 %, val err  3.47 %, patience  14 \n",
      "\n",
      "Epoch  20 , training err  1.834 %, val err  3.33 %, patience  13 \n",
      "\n",
      "Epoch  21 , training err  1.802 %, val err  3.56 %, patience  12 \n",
      "\n",
      "Epoch  22 , training err  1.924 %, val err  3.24 %, patience  11 \n",
      "\n",
      "Epoch  23 , training err  2.632 %, val err  3.94 %, patience  10 \n",
      "\n",
      "Epoch  24 , training err  2.118 %, val err  3.46 %, patience  9 \n",
      "\n",
      "Epoch  25 , training err  1.73 %, val err  3.27 %, patience  8 \n",
      "\n",
      "Epoch  26 , training err  1.088 %, val err  2.58 %, patience  20 \n",
      "\n",
      "Epoch  27 , training err  1.696 %, val err  3.2 %, patience  19 \n",
      "\n",
      "Epoch  28 , training err  1.808 %, val err  3.32 %, patience  18 \n",
      "\n",
      "Epoch  29 , training err  1.922 %, val err  3.51 %, patience  17 \n",
      "\n",
      "Epoch  30 , training err  1.722 %, val err  3.29 %, patience  16 \n",
      "\n",
      "Epoch  31 , training err  2.744 %, val err  4.17 %, patience  15 \n",
      "\n",
      "Epoch  32 , training err  2.108 %, val err  3.68 %, patience  14 \n",
      "\n",
      "Epoch  33 , training err  2.052 %, val err  3.71 %, patience  13 \n",
      "\n",
      "Epoch  34 , training err  1.928 %, val err  3.43 %, patience  12 \n",
      "\n",
      "Epoch  35 , training err  1.024 %, val err  2.63 %, patience  11 \n",
      "\n",
      "Epoch  36 , training err  1.33 %, val err  2.95 %, patience  10 \n",
      "\n",
      "Epoch  37 , training err  2.338 %, val err  3.54 %, patience  9 \n",
      "\n",
      "Epoch  38 , training err  1.344 %, val err  2.75 %, patience  8 \n",
      "\n",
      "Epoch  39 , training err  1.712 %, val err  3.32 %, patience  7 \n",
      "\n",
      "Epoch  40 , training err  1.644 %, val err  3.03 %, patience  6 \n",
      "\n",
      "Epoch  41 , training err  1.04 %, val err  2.64 %, patience  5 \n",
      "\n",
      "Epoch  42 , training err  1.194 %, val err  2.76 %, patience  4 \n",
      "\n",
      "Epoch  43 , training err  1.428 %, val err  2.94 %, patience  3 \n",
      "\n",
      "Epoch  44 , training err  1.524 %, val err  3.1 %, patience  2 \n",
      "\n",
      "Epoch  45 , training err  1.736 %, val err  3.26 %, patience  1 \n",
      "\n",
      "Epoch  46 , training err  1.214 %, val err  2.81 %, patience  0 \n",
      "\n",
      "Best val err  2.58 % at epoch  26  corresponding train err  1.088 %\n",
      "Epoch  0 , training err  5.328 %, val err  5.41 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  4.548 %, val err  4.82 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  3.878 %, val err  4.09 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  3.494 %, val err  4.15 %, patience  19 \n",
      "\n",
      "Epoch  4 , training err  2.794 %, val err  3.57 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  2.764 %, val err  3.74 %, patience  19 \n",
      "\n",
      "Epoch  6 , training err  2.694 %, val err  3.45 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  2.764 %, val err  3.58 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  2.546 %, val err  3.66 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  2.23 %, val err  3.51 %, patience  17 \n",
      "\n",
      "Epoch  10 , training err  2.432 %, val err  3.54 %, patience  16 \n",
      "\n",
      "Epoch  11 , training err  2.542 %, val err  3.95 %, patience  15 \n",
      "\n",
      "Epoch  12 , training err  2.112 %, val err  3.25 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  2.328 %, val err  3.47 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  3.142 %, val err  4.31 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  2.988 %, val err  3.99 %, patience  17 \n",
      "\n",
      "Epoch  16 , training err  2.676 %, val err  4.16 %, patience  16 \n",
      "\n",
      "Epoch  17 , training err  2.066 %, val err  3.38 %, patience  15 \n",
      "\n",
      "Epoch  18 , training err  2.104 %, val err  3.29 %, patience  14 \n",
      "\n",
      "Epoch  19 , training err  2.582 %, val err  3.95 %, patience  13 \n",
      "\n",
      "Epoch  20 , training err  2.1 %, val err  3.29 %, patience  12 \n",
      "\n",
      "Epoch  21 , training err  1.638 %, val err  3.23 %, patience  20 \n",
      "\n",
      "Epoch  22 , training err  3.018 %, val err  4.09 %, patience  19 \n",
      "\n",
      "Epoch  23 , training err  4.132 %, val err  5.0 %, patience  18 \n",
      "\n",
      "Epoch  24 , training err  1.986 %, val err  3.34 %, patience  17 \n",
      "\n",
      "Epoch  25 , training err  2.474 %, val err  3.61 %, patience  16 \n",
      "\n",
      "Epoch  26 , training err  2.202 %, val err  3.58 %, patience  15 \n",
      "\n",
      "Epoch  27 , training err  1.822 %, val err  3.29 %, patience  14 \n",
      "\n",
      "Epoch  28 , training err  2.126 %, val err  3.25 %, patience  13 \n",
      "\n",
      "Epoch  29 , training err  1.868 %, val err  3.09 %, patience  20 \n",
      "\n",
      "Epoch  30 , training err  1.628 %, val err  3.05 %, patience  20 \n",
      "\n",
      "Epoch  31 , training err  2.394 %, val err  3.69 %, patience  19 \n",
      "\n",
      "Epoch  32 , training err  1.754 %, val err  3.24 %, patience  18 \n",
      "\n",
      "Epoch  33 , training err  1.76 %, val err  3.07 %, patience  17 \n",
      "\n",
      "Epoch  34 , training err  1.664 %, val err  3.13 %, patience  16 \n",
      "\n",
      "Epoch  35 , training err  1.964 %, val err  3.46 %, patience  15 \n",
      "\n",
      "Epoch  36 , training err  1.894 %, val err  3.48 %, patience  14 \n",
      "\n",
      "Epoch  37 , training err  1.668 %, val err  3.09 %, patience  13 \n",
      "\n",
      "Epoch  38 , training err  1.854 %, val err  3.24 %, patience  12 \n",
      "\n",
      "Epoch  39 , training err  1.442 %, val err  2.77 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  1.528 %, val err  3.16 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  2.408 %, val err  3.72 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  1.484 %, val err  2.9 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  2.17 %, val err  3.64 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  1.596 %, val err  3.04 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  1.74 %, val err  3.22 %, patience  14 \n",
      "\n",
      "Epoch  46 , training err  1.77 %, val err  3.27 %, patience  13 \n",
      "\n",
      "Epoch  47 , training err  2.254 %, val err  3.43 %, patience  12 \n",
      "\n",
      "Epoch  48 , training err  1.246 %, val err  2.74 %, patience  20 \n",
      "\n",
      "Epoch  49 , training err  1.72 %, val err  2.83 %, patience  19 \n",
      "\n",
      "Epoch  50 , training err  1.968 %, val err  3.24 %, patience  18 \n",
      "\n",
      "Epoch  51 , training err  2.024 %, val err  3.49 %, patience  17 \n",
      "\n",
      "Epoch  52 , training err  1.758 %, val err  3.18 %, patience  16 \n",
      "\n",
      "Epoch  53 , training err  2.208 %, val err  3.26 %, patience  15 \n",
      "\n",
      "Epoch  54 , training err  3.748 %, val err  4.36 %, patience  14 \n",
      "\n",
      "Epoch  55 , training err  1.914 %, val err  3.23 %, patience  13 \n",
      "\n",
      "Epoch  56 , training err  1.45 %, val err  2.93 %, patience  12 \n",
      "\n",
      "Epoch  57 , training err  1.858 %, val err  3.39 %, patience  11 \n",
      "\n",
      "Epoch  58 , training err  2.706 %, val err  3.9 %, patience  10 \n",
      "\n",
      "Epoch  59 , training err  1.442 %, val err  2.98 %, patience  9 \n",
      "\n",
      "Epoch  60 , training err  1.918 %, val err  3.59 %, patience  8 \n",
      "\n",
      "Epoch  61 , training err  1.748 %, val err  3.2 %, patience  7 \n",
      "\n",
      "Epoch  62 , training err  2.304 %, val err  3.56 %, patience  6 \n",
      "\n",
      "Epoch  63 , training err  1.708 %, val err  3.1 %, patience  5 \n",
      "\n",
      "Epoch  64 , training err  1.612 %, val err  3.04 %, patience  4 \n",
      "\n",
      "Epoch  65 , training err  2.162 %, val err  3.22 %, patience  3 \n",
      "\n",
      "Epoch  66 , training err  1.54 %, val err  3.12 %, patience  2 \n",
      "\n",
      "Epoch  67 , training err  1.372 %, val err  3.04 %, patience  1 \n",
      "\n",
      "Epoch  68 , training err  1.998 %, val err  3.43 %, patience  0 \n",
      "\n",
      "Best val err  2.74 % at epoch  48  corresponding train err  1.246 %\n",
      "1\n",
      ". Test error  72.84 %\n",
      "2\n",
      ". Test error  57.43 %\n",
      "5\n",
      ". Test error  12.04 %\n",
      "15\n",
      ". Test error  5.07 %\n",
      "20\n",
      ". Test error  4.64 %\n",
      "30\n",
      ". Test error  3.7 %\n",
      "40\n",
      ". Test error  2.82 %\n",
      "50\n",
      ". Test error  2.47 %\n",
      "100\n",
      ". Test error  2.34 %\n",
      "[784, 50, 30, 10]\n",
      ". Test error  3.04 %\n",
      "[784, 50, 30, 20, 10]\n",
      ". Test error  2.7 %\n",
      "[784, 50, 30, 20, 20, 10]\n",
      ". Test error  2.89 %\n"
     ]
    }
   ],
   "source": [
    "layer = [[784, 50, 30, 10], [784, 50, 30, 20, 10], [784, 50, 30, 20, 20, 10]]\n",
    "for ll in layer:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, ll, 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, ll));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, ll in bestWs:\n",
    "    print ll \n",
    "    print  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000L, 785L)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
