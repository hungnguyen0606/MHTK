{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nguyễn Phan Mạnh Hùng - 1312727\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "# You can also import other things ...\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.plk.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) : tuple\n",
    "        X_train : numpy array, shape (N=50000, d+1=785)\n",
    "            Input vectors of the training set.\n",
    "        Y_train: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        X_val : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the validation set.\n",
    "        Y_val: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        X_test : numpy array, shape (N=10000, d+1=785)\n",
    "            Input vectors of the test set.\n",
    "        Y_test: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train, Y_train = train_data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    X_val, Y_val = val_data\n",
    "    X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "    \n",
    "    X_test, Y_test = test_data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Computes tanh function for each element of numpy array Z.\n",
    "    \"\"\"\n",
    "    ez = np.exp(Z)\n",
    "    esz = np.exp(-Z)\n",
    "    return (ez-esz)/(ez + esz)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Computes softmax function for each row of numpy array Z.\n",
    "    \"\"\"\n",
    "    A = np.exp(Z)\n",
    "    A /= np.sum(A, axis=1, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def forward_prop(X, Ws):\n",
    "    \"\"\"\n",
    "    Forward propagates X through layers of neural nets to get the final outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones (corresponding to x_0).\n",
    "    Ws : list of numpy arrays\n",
    "        The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array, shape (N, K=10)\n",
    "        The maxtrix of Neural Net's output vectors; each row is an output vector (containing each \n",
    "        class's probability given the corresponding input vector).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    A = X;\n",
    "    for i in range(len(Ws)):\n",
    "        W = Ws[i];\n",
    "        Z = A.dot(W)\n",
    "        if i == len(Ws)-1:\n",
    "            A = softmax(Z);\n",
    "        else:\n",
    "            A = tanh(Z);\n",
    "            A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "    return A;\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addOne(A):\n",
    "    return np.hstack((np.ones((A.shape[0],1)),A));\n",
    "def meanBinaryError(X, W, Y):\n",
    "    #for debugging: def forward_prop(X, Ws):\n",
    "    A = forward_prop(X, W);\n",
    "    myY = np.nonzero(np.max(A, axis = 1, keepdims = True) == A)[1]\n",
    "    error = 1 - np.sum(myY == Y) * 1.0 / Y.shape[0]\n",
    "    return error\n",
    "def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, \n",
    "                     l2_reg_level):\n",
    "    \"\"\"\n",
    "    Trains Neural Net on the dataset (X_train, Y_train).\n",
    "    Cost function: Mean Negative Log Likelihood + L2 regularization.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) with early stopping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the training set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_train : numpy array, shape (N,)\n",
    "        The vector of outputs of the training set.\n",
    "    X_val : numpy array, shape (N_val, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector) of the validation set; \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y_val : numpy array, shape (N_val,)\n",
    "        The vector of outputs of the validation set.  \n",
    "    layer_sizes : list of ints\n",
    "        The list of each layer' size (not count the +1 neurons).\n",
    "        E.g. layer_sizes = [784, 30, 10] means: the 1st layer (input layer) has 784 neurons,\n",
    "        the 2nd layer (hidden layer) has 30 neurons, the 3rd layer (output layer) has 10 neurons.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    mnb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    max_patience : int\n",
    "        The parameter of early stopping. You'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, you'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, you'll reset `patience` to `max_patience`; otherwise, `patience` -= 1. \n",
    "        When `patience` = 0, you'll terminate SGD.\n",
    "    l2_reg_level : float\n",
    "        The level (the coefficient) of L2 regularization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (best_Ws, train_errs, val_errs) : tuple\n",
    "        best_Ws : list of numpy arrays\n",
    "            The list of each layer's W; W of layer l will have the shape of (d^(l-1)+1, d^(l)) where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1, and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "            It's the parameters having smallest MBE on the validation set.\n",
    "        train_errs: list of floats\n",
    "            List of MBEs on the training set after each epoch.\n",
    "        val_errs: list of floats\n",
    "            List of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each epoch, you need to print out: \n",
    "    - The MBE on the training set and validation set.\n",
    "    - The value of `patience`.\n",
    "    E.g., 'Epoch ..., training err ..., val err ..., patience ...'.\n",
    "    \n",
    "    After the training, you need to print out:\n",
    "    - The best MBE on the validation set.\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set.\n",
    "    E.g., \"Best val err ..., at epoch ..., corresponding train err ...\".\n",
    "    \"\"\"\n",
    "    # Init weights\n",
    "    np.random.seed(0) # Note: this will fix the randomization so that we'll get the same random numbers each run; \n",
    "                      # it make comparisons (e.g. between different values of `l2_reg_level`) more accurate. \n",
    "    Ws = [np.random.randn(layer_sizes[l]+1, layer_sizes[l+1]) / np.sqrt(layer_sizes[l]+1) for l in range(len(layer_sizes)-1)]\n",
    "    # TODO\n",
    "    bestMBEval = 111; #infinity ~ > 1\n",
    "    corEpoch = -1;\n",
    "    corMBEtrain = 111;\n",
    "    train_errs = [];\n",
    "    val_errs = [];\n",
    "    bestWs = [];\n",
    "    \n",
    "    N = X_train.shape[0];\n",
    "    one_hot_Y = np.eye(layer_sizes[-1]);\n",
    "    rand_idxs = range(N);\n",
    "    max_epoch = 1000000000; #infinity \n",
    "    epoch = -1;\n",
    "    while epoch < max_epoch: #we can also use \"While True:\". However, we should use this condition instead to control when the loop terminates. \n",
    "        epoch += 1\n",
    "        np.random.shuffle(rand_idxs);\n",
    "        for start_idx in range(0,N, mnb_size):\n",
    "            mnb_X = X_train[rand_idxs[start_idx:start_idx+mnb_size]];\n",
    "            mnb_Y = one_hot_Y[Y_train[rand_idxs[start_idx:start_idx+mnb_size]]];\n",
    "            #forward propagation\n",
    "            As = [mnb_X];\n",
    "            #Zs =[];\n",
    "            A = mnb_X;\n",
    "            for i in range(len(Ws)):\n",
    "                W = Ws[i];\n",
    "                Z = A.dot(W)\n",
    "                if i == len(Ws)-1:\n",
    "                    A = softmax(Z);\n",
    "                else:\n",
    "                    A = tanh(Z);\n",
    "                    A = np.hstack((np.ones((A.shape[0],1)),A));\n",
    "                    \n",
    "                #Zs.append(Z); #for debuging\n",
    "                As.append(A);\n",
    "            #backward propagation\n",
    "            delta = As[-1] - mnb_Y;\n",
    "            grad = As[-2].T.dot(delta)*1.0/mnb_size + 2.0*l2_reg_level*Ws[-1]; #/mnb_size\n",
    "            Ws[-1] -= learning_rate*grad;\n",
    "            \n",
    "            for it in range(2, len(layer_sizes)):\n",
    "                delta = delta.dot(Ws[-it+1].T)*(1-As[-it]*As[-it])\n",
    "                grad = (As[-it-1].T.dot(delta)*1.0/mnb_size)[:,1:] + 2.0*l2_reg_level*Ws[-it]; #/mnb_size\n",
    "        \n",
    "                Ws[-it] -= learning_rate*grad;\n",
    "                delta = delta[:,1:];\n",
    "        \n",
    "        eTrain = meanBinaryError(X_train, Ws, Y_train);\n",
    "        eVal = meanBinaryError(X_val, Ws, Y_val);\n",
    "        train_errs.append(eTrain);\n",
    "        val_errs.append(eVal);\n",
    "        \n",
    "        if (eVal < bestMBEval):\n",
    "            bestMBEval = eVal;\n",
    "            corMBEtrain = eTrain;\n",
    "            corEpoch = epoch\n",
    "            patience = max_patience\n",
    "            bestWs = copy.deepcopy(Ws);\n",
    "        else:\n",
    "            patience = patience - 1;        \n",
    "        print 'Epoch ', epoch, ', training err ', eTrain*100, '%, val err ', eVal*100, '%, patience ', patience, '\\n'\n",
    "        if patience == 0:\n",
    "            break;\n",
    "            \n",
    "       \n",
    "    print 'Best val err ', bestMBEval*100, '% at epoch ', corEpoch, ' corresponding train err ',corMBEtrain*100, '%';\n",
    "    return (bestWs,train_errs,val_errs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = read_mnist('mnist.pkl.gz');\n",
    "l2_reg = [0, 0.0001, 0.001];\n",
    "bestWs = [];\n",
    "train_errs = [];\n",
    "val_errs = [];\n",
    "#def train_neural_net(X_train, Y_train, X_val, Y_val, layer_sizes, learning_rate, mnb_size, max_patience, l2_reg_level):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực nghiệm mạng nơ-ron 1 lớp ẩn\n",
    "Thêm các kích thước lớp ẩn vào middle.\n",
    "\n",
    "Ví dụ middle = [1,2,3]\n",
    "- Huấn luyện 3 mạng có số nút của lớp ẩn lần lượt là 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  5.486 %, val err  5.26 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.102 %, val err  3.54 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.108 %, val err  2.92 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  1.748 %, val err  2.65 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  1.706 %, val err  2.69 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  1.784 %, val err  2.99 %, patience  18 \n",
      "\n",
      "Epoch  6 , training err  0.896 %, val err  2.08 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  0.804 %, val err  2.33 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  0.8 %, val err  2.38 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  1.05 %, val err  2.6 %, patience  17 \n",
      "\n",
      "Epoch  10 , training err  0.65 %, val err  2.21 %, patience  16 \n",
      "\n",
      "Epoch  11 , training err  0.66 %, val err  2.14 %, patience  15 \n",
      "\n",
      "Epoch  12 , training err  0.866 %, val err  2.41 %, patience  14 \n",
      "\n",
      "Epoch  13 , training err  0.596 %, val err  2.12 %, patience  13 \n",
      "\n",
      "Epoch  14 , training err  0.818 %, val err  2.45 %, patience  12 \n",
      "\n",
      "Epoch  15 , training err  0.528 %, val err  2.12 %, patience  11 \n",
      "\n",
      "Epoch  16 , training err  0.436 %, val err  2.13 %, patience  10 \n",
      "\n",
      "Epoch  17 , training err  2.37 %, val err  3.91 %, patience  9 \n",
      "\n",
      "Epoch  18 , training err  0.486 %, val err  2.16 %, patience  8 \n",
      "\n",
      "Epoch  19 , training err  0.346 %, val err  2.0 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  0.614 %, val err  2.51 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  0.474 %, val err  2.22 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  0.514 %, val err  2.15 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  0.438 %, val err  2.03 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  0.45 %, val err  2.13 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  0.39 %, val err  2.07 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  0.378 %, val err  2.15 %, patience  13 \n",
      "\n",
      "Epoch  27 , training err  0.274 %, val err  2.0 %, patience  12 \n",
      "\n",
      "Epoch  28 , training err  0.39 %, val err  2.28 %, patience  11 \n",
      "\n",
      "Epoch  29 , training err  0.348 %, val err  2.25 %, patience  10 \n",
      "\n",
      "Epoch  30 , training err  1.526 %, val err  3.16 %, patience  9 \n",
      "\n",
      "Epoch  31 , training err  0.418 %, val err  2.2 %, patience  8 \n",
      "\n",
      "Epoch  32 , training err  0.394 %, val err  2.13 %, patience  7 \n",
      "\n",
      "Epoch  33 , training err  0.342 %, val err  2.19 %, patience  6 \n",
      "\n",
      "Epoch  34 , training err  0.38 %, val err  2.2 %, patience  5 \n",
      "\n",
      "Epoch  35 , training err  0.482 %, val err  2.2 %, patience  4 \n",
      "\n",
      "Epoch  36 , training err  0.394 %, val err  1.93 %, patience  20 \n",
      "\n",
      "Epoch  37 , training err  0.346 %, val err  2.04 %, patience  19 \n",
      "\n",
      "Epoch  38 , training err  0.246 %, val err  1.93 %, patience  18 \n",
      "\n",
      "Epoch  39 , training err  0.216 %, val err  1.9 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  0.58 %, val err  2.36 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  0.376 %, val err  2.16 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  0.514 %, val err  2.19 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  0.366 %, val err  2.09 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  0.248 %, val err  1.97 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  0.316 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  46 , training err  0.232 %, val err  1.97 %, patience  13 \n",
      "\n",
      "Epoch  47 , training err  0.274 %, val err  1.82 %, patience  20 \n",
      "\n",
      "Epoch  48 , training err  0.358 %, val err  2.04 %, patience  19 \n",
      "\n",
      "Epoch  49 , training err  0.272 %, val err  2.05 %, patience  18 \n",
      "\n",
      "Epoch  50 , training err  0.224 %, val err  1.82 %, patience  17 \n",
      "\n",
      "Epoch  51 , training err  0.262 %, val err  1.95 %, patience  16 \n",
      "\n",
      "Epoch  52 , training err  0.268 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  53 , training err  0.38 %, val err  2.11 %, patience  14 \n",
      "\n",
      "Epoch  54 , training err  0.346 %, val err  2.03 %, patience  13 \n",
      "\n",
      "Epoch  55 , training err  0.306 %, val err  2.13 %, patience  12 \n",
      "\n",
      "Epoch  56 , training err  0.28 %, val err  1.97 %, patience  11 \n",
      "\n",
      "Epoch  57 , training err  0.21 %, val err  1.87 %, patience  10 \n",
      "\n",
      "Epoch  58 , training err  0.488 %, val err  2.38 %, patience  9 \n",
      "\n",
      "Epoch  59 , training err  0.228 %, val err  1.92 %, patience  8 \n",
      "\n",
      "Epoch  60 , training err  0.672 %, val err  2.24 %, patience  7 \n",
      "\n",
      "Epoch  61 , training err  0.372 %, val err  2.03 %, patience  6 \n",
      "\n",
      "Epoch  62 , training err  0.594 %, val err  2.29 %, patience  5 \n",
      "\n",
      "Epoch  63 , training err  0.294 %, val err  1.99 %, patience  4 \n",
      "\n",
      "Epoch  64 , training err  0.674 %, val err  2.14 %, patience  3 \n",
      "\n",
      "Epoch  65 , training err  0.5 %, val err  1.92 %, patience  2 \n",
      "\n",
      "Epoch  66 , training err  0.28 %, val err  1.88 %, patience  1 \n",
      "\n",
      "Epoch  67 , training err  0.17 %, val err  1.83 %, patience  0 \n",
      "\n",
      "Best val err  1.82 % at epoch  47  corresponding train err  0.274 %\n",
      "Epoch  0 , training err  4.904 %, val err  4.9 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.056 %, val err  3.49 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.63 %, val err  3.38 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  1.414 %, val err  2.42 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  1.69 %, val err  2.76 %, patience  19 \n",
      "\n",
      "Epoch  5 , training err  1.176 %, val err  2.43 %, patience  18 \n",
      "\n",
      "Epoch  6 , training err  1.86 %, val err  3.27 %, patience  17 \n",
      "\n",
      "Epoch  7 , training err  0.726 %, val err  2.39 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  0.848 %, val err  2.36 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  0.554 %, val err  2.16 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  0.608 %, val err  2.31 %, patience  19 \n",
      "\n",
      "Epoch  11 , training err  0.626 %, val err  2.24 %, patience  18 \n",
      "\n",
      "Epoch  12 , training err  0.5 %, val err  2.09 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  0.658 %, val err  2.32 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  0.64 %, val err  2.5 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  0.554 %, val err  2.16 %, patience  17 \n",
      "\n",
      "Epoch  16 , training err  0.44 %, val err  2.05 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  0.59 %, val err  2.22 %, patience  19 \n",
      "\n",
      "Epoch  18 , training err  0.446 %, val err  2.12 %, patience  18 \n",
      "\n",
      "Epoch  19 , training err  0.458 %, val err  1.91 %, patience  20 \n",
      "\n",
      "Epoch  20 , training err  0.608 %, val err  2.28 %, patience  19 \n",
      "\n",
      "Epoch  21 , training err  0.36 %, val err  2.23 %, patience  18 \n",
      "\n",
      "Epoch  22 , training err  0.536 %, val err  2.31 %, patience  17 \n",
      "\n",
      "Epoch  23 , training err  0.54 %, val err  2.34 %, patience  16 \n",
      "\n",
      "Epoch  24 , training err  0.54 %, val err  2.42 %, patience  15 \n",
      "\n",
      "Epoch  25 , training err  0.296 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  26 , training err  0.394 %, val err  2.04 %, patience  13 \n",
      "\n",
      "Epoch  27 , training err  0.546 %, val err  2.44 %, patience  12 \n",
      "\n",
      "Epoch  28 , training err  0.542 %, val err  2.4 %, patience  11 \n",
      "\n",
      "Epoch  29 , training err  0.73 %, val err  2.38 %, patience  10 \n",
      "\n",
      "Epoch  30 , training err  0.218 %, val err  1.98 %, patience  9 \n",
      "\n",
      "Epoch  31 , training err  0.376 %, val err  2.09 %, patience  8 \n",
      "\n",
      "Epoch  32 , training err  0.398 %, val err  2.19 %, patience  7 \n",
      "\n",
      "Epoch  33 , training err  0.3 %, val err  2.17 %, patience  6 \n",
      "\n",
      "Epoch  34 , training err  0.204 %, val err  1.84 %, patience  20 \n",
      "\n",
      "Epoch  35 , training err  0.356 %, val err  2.0 %, patience  19 \n",
      "\n",
      "Epoch  36 , training err  0.312 %, val err  1.98 %, patience  18 \n",
      "\n",
      "Epoch  37 , training err  0.224 %, val err  1.95 %, patience  17 \n",
      "\n",
      "Epoch  38 , training err  0.566 %, val err  2.28 %, patience  16 \n",
      "\n",
      "Epoch  39 , training err  0.388 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  40 , training err  0.642 %, val err  2.52 %, patience  14 \n",
      "\n",
      "Epoch  41 , training err  0.554 %, val err  2.09 %, patience  13 \n",
      "\n",
      "Epoch  42 , training err  0.44 %, val err  2.0 %, patience  12 \n",
      "\n",
      "Epoch  43 , training err  0.306 %, val err  2.0 %, patience  11 \n",
      "\n",
      "Epoch  44 , training err  0.358 %, val err  2.13 %, patience  10 \n",
      "\n",
      "Epoch  45 , training err  0.34 %, val err  1.96 %, patience  9 \n",
      "\n",
      "Epoch  46 , training err  0.28 %, val err  2.0 %, patience  8 \n",
      "\n",
      "Epoch  47 , training err  0.352 %, val err  1.97 %, patience  7 \n",
      "\n",
      "Epoch  48 , training err  0.242 %, val err  1.93 %, patience  6 \n",
      "\n",
      "Epoch  49 , training err  0.392 %, val err  2.04 %, patience  5 \n",
      "\n",
      "Epoch  50 , training err  0.264 %, val err  2.24 %, patience  4 \n",
      "\n",
      "Epoch  51 , training err  0.432 %, val err  2.27 %, patience  3 \n",
      "\n",
      "Epoch  52 , training err  0.27 %, val err  1.94 %, patience  2 \n",
      "\n",
      "Epoch  53 , training err  0.394 %, val err  2.12 %, patience  1 \n",
      "\n",
      "Epoch  54 , training err  0.214 %, val err  1.8 %, patience  20 \n",
      "\n",
      "Epoch  55 , training err  0.352 %, val err  2.02 %, patience  19 \n",
      "\n",
      "Epoch  56 , training err  0.222 %, val err  1.94 %, patience  18 \n",
      "\n",
      "Epoch  57 , training err  0.252 %, val err  1.96 %, patience  17 \n",
      "\n",
      "Epoch  58 , training err  0.484 %, val err  2.23 %, patience  16 \n",
      "\n",
      "Epoch  59 , training err  0.228 %, val err  1.95 %, patience  15 \n",
      "\n",
      "Epoch  60 , training err  0.87 %, val err  2.58 %, patience  14 \n",
      "\n",
      "Epoch  61 , training err  0.328 %, val err  1.99 %, patience  13 \n",
      "\n",
      "Epoch  62 , training err  0.216 %, val err  1.82 %, patience  12 \n",
      "\n",
      "Epoch  63 , training err  0.314 %, val err  1.98 %, patience  11 \n",
      "\n",
      "Epoch  64 , training err  0.41 %, val err  2.11 %, patience  10 \n",
      "\n",
      "Epoch  65 , training err  0.422 %, val err  2.16 %, patience  9 \n",
      "\n",
      "Epoch  66 , training err  0.3 %, val err  2.03 %, patience  8 \n",
      "\n",
      "Epoch  67 , training err  0.334 %, val err  2.01 %, patience  7 \n",
      "\n",
      "Epoch  68 , training err  0.276 %, val err  1.97 %, patience  6 \n",
      "\n",
      "Epoch  69 , training err  0.164 %, val err  1.78 %, patience  20 \n",
      "\n",
      "Epoch  70 , training err  0.322 %, val err  2.18 %, patience  19 \n",
      "\n",
      "Epoch  71 , training err  0.47 %, val err  2.1 %, patience  18 \n",
      "\n",
      "Epoch  72 , training err  0.284 %, val err  2.01 %, patience  17 \n",
      "\n",
      "Epoch  73 , training err  0.368 %, val err  1.99 %, patience  16 \n",
      "\n",
      "Epoch  74 , training err  0.338 %, val err  1.9 %, patience  15 \n",
      "\n",
      "Epoch  75 , training err  0.25 %, val err  1.99 %, patience  14 \n",
      "\n",
      "Epoch  76 , training err  0.266 %, val err  1.87 %, patience  13 \n",
      "\n",
      "Epoch  77 , training err  0.242 %, val err  1.94 %, patience  12 \n",
      "\n",
      "Epoch  78 , training err  0.812 %, val err  2.45 %, patience  11 \n",
      "\n",
      "Epoch  79 , training err  0.522 %, val err  2.19 %, patience  10 \n",
      "\n",
      "Epoch  80 , training err  0.276 %, val err  1.98 %, patience  9 \n",
      "\n",
      "Epoch  81 , training err  0.194 %, val err  1.8 %, patience  8 \n",
      "\n",
      "Epoch  82 , training err  0.248 %, val err  2.09 %, patience  7 \n",
      "\n",
      "Epoch  83 , training err  0.408 %, val err  2.0 %, patience  6 \n",
      "\n",
      "Epoch  84 , training err  0.358 %, val err  2.0 %, patience  5 \n",
      "\n",
      "Epoch  85 , training err  0.41 %, val err  2.07 %, patience  4 \n",
      "\n",
      "Epoch  86 , training err  0.394 %, val err  2.35 %, patience  3 \n",
      "\n",
      "Epoch  87 , training err  0.178 %, val err  1.8 %, patience  2 \n",
      "\n",
      "Epoch  88 , training err  0.224 %, val err  1.95 %, patience  1 \n",
      "\n",
      "Epoch  89 , training err  0.134 %, val err  1.83 %, patience  0 \n",
      "\n",
      "Best val err  1.78 % at epoch  69  corresponding train err  0.164 %\n",
      "Epoch  0 , training err  5.44 %, val err  5.22 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  2.676 %, val err  3.18 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  2.658 %, val err  3.22 %, patience  19 \n",
      "\n",
      "Epoch  3 , training err  2.81 %, val err  3.62 %, patience  18 \n",
      "\n",
      "Epoch  4 , training err  1.42 %, val err  2.56 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  0.948 %, val err  2.24 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  0.804 %, val err  2.3 %, patience  19 \n",
      "\n",
      "Epoch  7 , training err  0.778 %, val err  2.23 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  0.704 %, val err  2.2 %, patience  20 \n",
      "\n",
      "Epoch  9 , training err  0.698 %, val err  2.35 %, patience  19 \n",
      "\n",
      "Epoch  10 , training err  1.048 %, val err  2.64 %, patience  18 \n",
      "\n",
      "Epoch  11 , training err  1.016 %, val err  2.43 %, patience  17 \n",
      "\n",
      "Epoch  12 , training err  0.424 %, val err  2.11 %, patience  20 \n",
      "\n",
      "Epoch  13 , training err  0.67 %, val err  2.28 %, patience  19 \n",
      "\n",
      "Epoch  14 , training err  0.432 %, val err  2.16 %, patience  18 \n",
      "\n",
      "Epoch  15 , training err  0.53 %, val err  2.1 %, patience  20 \n",
      "\n",
      "Epoch  16 , training err  0.608 %, val err  2.22 %, patience  19 \n",
      "\n",
      "Epoch  17 , training err  0.534 %, val err  2.11 %, patience  18 \n",
      "\n",
      "Epoch  18 , training err  0.496 %, val err  2.22 %, patience  17 \n",
      "\n",
      "Epoch  19 , training err  0.46 %, val err  2.15 %, patience  16 \n",
      "\n",
      "Epoch  20 , training err  0.598 %, val err  2.29 %, patience  15 \n",
      "\n",
      "Epoch  21 , training err  0.738 %, val err  2.41 %, patience  14 \n",
      "\n",
      "Epoch  22 , training err  0.446 %, val err  2.38 %, patience  13 \n",
      "\n",
      "Epoch  23 , training err  0.486 %, val err  2.08 %, patience  20 \n",
      "\n",
      "Epoch  24 , training err  0.63 %, val err  2.32 %, patience  19 \n",
      "\n",
      "Epoch  25 , training err  0.496 %, val err  2.12 %, patience  18 \n",
      "\n",
      "Epoch  26 , training err  0.814 %, val err  2.28 %, patience  17 \n",
      "\n",
      "Epoch  27 , training err  0.406 %, val err  2.02 %, patience  20 \n",
      "\n",
      "Epoch  28 , training err  0.44 %, val err  2.3 %, patience  19 \n",
      "\n",
      "Epoch  29 , training err  0.658 %, val err  2.32 %, patience  18 \n",
      "\n",
      "Epoch  30 , training err  0.522 %, val err  2.18 %, patience  17 \n",
      "\n",
      "Epoch  31 , training err  0.696 %, val err  2.24 %, patience  16 \n",
      "\n",
      "Epoch  32 , training err  0.494 %, val err  2.17 %, patience  15 \n",
      "\n",
      "Epoch  33 , training err  0.742 %, val err  2.36 %, patience  14 \n",
      "\n",
      "Epoch  34 , training err  0.54 %, val err  2.34 %, patience  13 \n",
      "\n",
      "Epoch  35 , training err  0.67 %, val err  2.6 %, patience  12 \n",
      "\n",
      "Epoch  36 , training err  0.408 %, val err  2.19 %, patience  11 \n",
      "\n",
      "Epoch  37 , training err  0.462 %, val err  2.06 %, patience  10 \n",
      "\n",
      "Epoch  38 , training err  0.506 %, val err  2.33 %, patience  9 \n",
      "\n",
      "Epoch  39 , training err  0.354 %, val err  2.06 %, patience  8 \n",
      "\n",
      "Epoch  40 , training err  0.374 %, val err  2.13 %, patience  7 \n",
      "\n",
      "Epoch  41 , training err  0.972 %, val err  2.65 %, patience  6 \n",
      "\n",
      "Epoch  42 , training err  0.348 %, val err  2.28 %, patience  5 \n",
      "\n",
      "Epoch  43 , training err  0.648 %, val err  2.43 %, patience  4 \n",
      "\n",
      "Epoch  44 , training err  0.402 %, val err  2.09 %, patience  3 \n",
      "\n",
      "Epoch  45 , training err  0.56 %, val err  2.17 %, patience  2 \n",
      "\n",
      "Epoch  46 , training err  0.532 %, val err  2.18 %, patience  1 \n",
      "\n",
      "Epoch  47 , training err  0.524 %, val err  2.11 %, patience  0 \n",
      "\n",
      "Best val err  2.02 % at epoch  27  corresponding train err  0.406 %\n",
      "Hidden layer  150 . Test error  1.85 %\n",
      "Hidden layer  200 . Test error  1.8 %\n",
      "Hidden layer  300 . Test error  2.09 %\n"
     ]
    }
   ],
   "source": [
    "middle = [150, 200, 300]\n",
    "for mid in middle:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, [784,mid,10], 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, mid));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, mid in bestWs:\n",
    "    print  'Hidden layer ', mid,  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thực nghiệm với mạng nơ-ron nhiều lớp. \n",
    "Thêm mô tả về mạng nơ-ron vào layer.\n",
    "Ví dụ: layer = [[784,10], [784,30,10]]\n",
    "- Huấn luyện 2 mạng nơ-ron với số lượng nút ở mỗi mạng là [784,10] và [784,30,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , training err  6.446 %, val err  6.3 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.788 %, val err  4.34 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  3.428 %, val err  4.05 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  2.758 %, val err  3.67 %, patience  20 \n",
      "\n",
      "Epoch  4 , training err  2.396 %, val err  3.34 %, patience  20 \n",
      "\n",
      "Epoch  5 , training err  2.572 %, val err  3.65 %, patience  19 \n",
      "\n",
      "Epoch  6 , training err  2.42 %, val err  3.38 %, patience  18 \n",
      "\n",
      "Epoch  7 , training err  2.112 %, val err  3.23 %, patience  20 \n",
      "\n",
      "Epoch  8 , training err  2.048 %, val err  3.33 %, patience  19 \n",
      "\n",
      "Epoch  9 , training err  1.602 %, val err  3.16 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  1.69 %, val err  3.19 %, patience  19 \n",
      "\n",
      "Epoch  11 , training err  2.03 %, val err  3.57 %, patience  18 \n",
      "\n",
      "Epoch  12 , training err  2.194 %, val err  3.6 %, patience  17 \n",
      "\n",
      "Epoch  13 , training err  1.674 %, val err  2.98 %, patience  20 \n",
      "\n",
      "Epoch  14 , training err  1.726 %, val err  3.19 %, patience  19 \n",
      "\n",
      "Epoch  15 , training err  1.608 %, val err  3.19 %, patience  18 \n",
      "\n",
      "Epoch  16 , training err  1.746 %, val err  3.27 %, patience  17 \n",
      "\n",
      "Epoch  17 , training err  1.53 %, val err  2.95 %, patience  20 \n",
      "\n",
      "Epoch  18 , training err  1.522 %, val err  3.23 %, patience  19 \n",
      "\n",
      "Epoch  19 , training err  1.976 %, val err  3.53 %, patience  18 \n",
      "\n",
      "Epoch  20 , training err  1.634 %, val err  3.46 %, patience  17 \n",
      "\n",
      "Epoch  21 , training err  1.884 %, val err  3.47 %, patience  16 \n",
      "\n",
      "Epoch  22 , training err  1.708 %, val err  3.46 %, patience  15 \n",
      "\n",
      "Epoch  23 , training err  1.508 %, val err  3.13 %, patience  14 \n",
      "\n",
      "Epoch  24 , training err  1.132 %, val err  3.0 %, patience  13 \n",
      "\n",
      "Epoch  25 , training err  1.688 %, val err  3.47 %, patience  12 \n",
      "\n",
      "Epoch  26 , training err  1.614 %, val err  3.42 %, patience  11 \n",
      "\n",
      "Epoch  27 , training err  1.428 %, val err  3.08 %, patience  10 \n",
      "\n",
      "Epoch  28 , training err  1.452 %, val err  3.07 %, patience  9 \n",
      "\n",
      "Epoch  29 , training err  1.894 %, val err  3.68 %, patience  8 \n",
      "\n",
      "Epoch  30 , training err  1.446 %, val err  3.2 %, patience  7 \n",
      "\n",
      "Epoch  31 , training err  1.552 %, val err  3.04 %, patience  6 \n",
      "\n",
      "Epoch  32 , training err  1.306 %, val err  3.04 %, patience  5 \n",
      "\n",
      "Epoch  33 , training err  1.842 %, val err  3.41 %, patience  4 \n",
      "\n",
      "Epoch  34 , training err  1.288 %, val err  3.09 %, patience  3 \n",
      "\n",
      "Epoch  35 , training err  1.34 %, val err  3.14 %, patience  2 \n",
      "\n",
      "Epoch  36 , training err  2.25 %, val err  3.85 %, patience  1 \n",
      "\n",
      "Epoch  37 , training err  1.452 %, val err  3.2 %, patience  0 \n",
      "\n",
      "Best val err  2.95 % at epoch  17  corresponding train err  1.53 %\n",
      "Epoch  0 , training err  5.114 %, val err  5.27 %, patience  20 \n",
      "\n",
      "Epoch  1 , training err  3.72 %, val err  4.1 %, patience  20 \n",
      "\n",
      "Epoch  2 , training err  3.33 %, val err  3.79 %, patience  20 \n",
      "\n",
      "Epoch  3 , training err  3.578 %, val err  4.37 %, patience  19 \n",
      "\n",
      "Epoch  4 , training err  3.038 %, val err  3.84 %, patience  18 \n",
      "\n",
      "Epoch  5 , training err  2.688 %, val err  3.75 %, patience  20 \n",
      "\n",
      "Epoch  6 , training err  2.938 %, val err  3.55 %, patience  20 \n",
      "\n",
      "Epoch  7 , training err  3.146 %, val err  4.14 %, patience  19 \n",
      "\n",
      "Epoch  8 , training err  2.968 %, val err  4.21 %, patience  18 \n",
      "\n",
      "Epoch  9 , training err  2.104 %, val err  3.46 %, patience  20 \n",
      "\n",
      "Epoch  10 , training err  1.762 %, val err  3.11 %, patience  20 \n",
      "\n",
      "Epoch  11 , training err  2.228 %, val err  3.74 %, patience  19 \n",
      "\n",
      "Epoch  12 , training err  1.776 %, val err  3.14 %, patience  18 \n",
      "\n",
      "Epoch  13 , training err  1.914 %, val err  3.26 %, patience  17 \n",
      "\n",
      "Epoch  14 , training err  2.228 %, val err  3.62 %, patience  16 \n",
      "\n",
      "Epoch  15 , training err  1.738 %, val err  3.19 %, patience  15 \n",
      "\n",
      "Epoch  16 , training err  1.588 %, val err  3.09 %, patience  20 \n",
      "\n",
      "Epoch  17 , training err  2.252 %, val err  3.46 %, patience  19 \n",
      "\n",
      "Epoch  18 , training err  1.672 %, val err  3.22 %, patience  18 \n",
      "\n",
      "Epoch  19 , training err  2.032 %, val err  3.55 %, patience  17 \n",
      "\n",
      "Epoch  20 , training err  2.16 %, val err  3.67 %, patience  16 \n",
      "\n",
      "Epoch  21 , training err  1.932 %, val err  3.26 %, patience  15 \n",
      "\n",
      "Epoch  22 , training err  1.878 %, val err  3.33 %, patience  14 \n",
      "\n",
      "Epoch  23 , training err  1.988 %, val err  3.28 %, patience  13 \n",
      "\n",
      "Epoch  24 , training err  2.654 %, val err  3.8 %, patience  12 \n",
      "\n",
      "Epoch  25 , training err  1.236 %, val err  2.77 %, patience  20 \n",
      "\n",
      "Epoch  26 , training err  1.942 %, val err  3.38 %, patience  19 \n",
      "\n",
      "Epoch  27 , training err  3.302 %, val err  4.47 %, patience  18 \n",
      "\n",
      "Epoch  28 , training err  2.684 %, val err  4.05 %, patience  17 \n",
      "\n",
      "Epoch  29 , training err  1.774 %, val err  3.29 %, patience  16 \n",
      "\n",
      "Epoch  30 , training err  1.532 %, val err  3.2 %, patience  15 \n",
      "\n",
      "Epoch  31 , training err  1.476 %, val err  2.99 %, patience  14 \n",
      "\n",
      "Epoch  32 , training err  2.44 %, val err  3.79 %, patience  13 \n",
      "\n",
      "Epoch  33 , training err  1.786 %, val err  3.34 %, patience  12 \n",
      "\n",
      "Epoch  34 , training err  2.7 %, val err  4.07 %, patience  11 \n",
      "\n",
      "Epoch  35 , training err  1.634 %, val err  3.46 %, patience  10 \n",
      "\n",
      "Epoch  36 , training err  2.69 %, val err  3.87 %, patience  9 \n",
      "\n",
      "Epoch  37 , training err  1.686 %, val err  3.23 %, patience  8 \n",
      "\n",
      "Epoch  38 , training err  2.028 %, val err  3.3 %, patience  7 \n",
      "\n",
      "Epoch  39 , training err  1.294 %, val err  2.75 %, patience  20 \n",
      "\n",
      "Epoch  40 , training err  1.33 %, val err  2.82 %, patience  19 \n",
      "\n",
      "Epoch  41 , training err  1.694 %, val err  3.11 %, patience  18 \n",
      "\n",
      "Epoch  42 , training err  2.062 %, val err  3.46 %, patience  17 \n",
      "\n",
      "Epoch  43 , training err  1.932 %, val err  3.45 %, patience  16 \n",
      "\n",
      "Epoch  44 , training err  1.74 %, val err  3.17 %, patience  15 \n",
      "\n",
      "Epoch  45 , training err  1.738 %, val err  3.3 %, patience  14 \n",
      "\n",
      "Epoch  46 , training err  1.998 %, val err  3.43 %, patience  13 \n",
      "\n",
      "Epoch  47 , training err  1.858 %, val err  3.18 %, patience  12 \n",
      "\n",
      "Epoch  48 , training err  1.798 %, val err  3.41 %, patience  11 \n",
      "\n",
      "Epoch  49 , training err  1.548 %, val err  2.97 %, patience  10 \n",
      "\n",
      "Epoch  50 , training err  1.514 %, val err  3.28 %, patience  9 \n",
      "\n",
      "Epoch  51 , training err  1.944 %, val err  3.3 %, patience  8 \n",
      "\n",
      "Epoch  52 , training err  1.722 %, val err  3.1 %, patience  7 \n",
      "\n",
      "Epoch  53 , training err  1.716 %, val err  3.45 %, patience  6 \n",
      "\n",
      "Epoch  54 , training err  1.998 %, val err  3.41 %, patience  5 \n",
      "\n",
      "Epoch  55 , training err  1.48 %, val err  2.94 %, patience  4 \n",
      "\n",
      "Epoch  56 , training err  1.328 %, val err  3.2 %, patience  3 \n",
      "\n",
      "Epoch  57 , training err  1.694 %, val err  3.3 %, patience  2 \n",
      "\n",
      "Epoch  58 , training err  1.742 %, val err  3.23 %, patience  1 \n",
      "\n",
      "Epoch  59 , training err  1.588 %, val err  2.98 %, patience  0 \n",
      "\n",
      "Best val err  2.75 % at epoch  39  corresponding train err  1.294 %\n",
      "150\n",
      ". Test error  1.85 %\n",
      "200\n",
      ". Test error  1.8 %\n",
      "300\n",
      ". Test error  2.09 %\n",
      "[784, 50, 30, 30, 10]\n",
      ". Test error  3.25 %\n",
      "[784, 50, 30, 30, 30, 10]\n",
      ". Test error  3.02 %\n"
     ]
    }
   ],
   "source": [
    "layer = [[784, 50, 30, 20, 10], [784, 50, 30, 20, 20, 10]]\n",
    "for ll in layer:\n",
    "    bWs,tE,vE = train_neural_net(X_train, Y_train, X_val, Y_val, ll, 0.1, 10, 20, 0.0001);\n",
    "    bestWs.append((bWs, ll));\n",
    "    train_errs.append(tE);\n",
    "    val_errs.append(vE);\n",
    "\n",
    "for WS, ll in bestWs:\n",
    "    print ll \n",
    "    print  '. Test error ', meanBinaryError(X_test, WS, Y_test)*100, '%';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
